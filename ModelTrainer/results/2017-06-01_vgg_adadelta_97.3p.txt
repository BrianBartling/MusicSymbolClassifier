C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py --delete_and_recreate_dataset_directory False --model_name vgg
Using TensorFlow backend.
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 16)      448       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 16)      2320      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 16)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 32)       4640      
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 32)       9248      
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 64)        18496     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 64)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 128)       73856     
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 128)       512       
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 128)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 128)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 192)        221376    
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_14 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_14 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 4, 192)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-06-01 14:13:53.087295: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.087536: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.087775: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.087995: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.088804: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.088968: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.089131: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.089314: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 14:13:53.414894: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-01 14:13:53.415150: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-06-01 14:13:53.415288: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-06-01 14:13:53.415425: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 128, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'epsilon': 1e-08, 'rho': 0.95, 'decay': 0.0}
Epoch 1/200
 9/96 [=>............................] - ETA: 87s - loss: 5.2110 - acc: 0.09032017-06-01 14:14:10.691740: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3616 get requests, put_count=3566 evicted_count=1000 eviction_rate=0.280426 and unsatisfied allocation rate=0.318031
2017-06-01 14:14:10.692008: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
95/96 [============================>.] - ETA: 0s - loss: 2.2391 - acc: 0.4602Epoch 00000: val_acc improved from -inf to 0.13399, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 69s - loss: 2.2252 - acc: 0.4627 - val_loss: 3.1254 - val_acc: 0.1340
Epoch 2/200
95/96 [============================>.] - ETA: 0s - loss: 0.8682 - acc: 0.7692Epoch 00001: val_acc improved from 0.13399 to 0.24092, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.8654 - acc: 0.7706 - val_loss: 3.0773 - val_acc: 0.2409
Epoch 3/200
95/96 [============================>.] - ETA: 0s - loss: 0.6431 - acc: 0.8283Epoch 00002: val_acc improved from 0.24092 to 0.53201, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 64s - loss: 0.6409 - acc: 0.8290 - val_loss: 2.5654 - val_acc: 0.5320
Epoch 4/200
95/96 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8682Epoch 00003: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.5276 - acc: 0.8695 - val_loss: 6.6289 - val_acc: 0.2766
Epoch 5/200
95/96 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8911Epoch 00004: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.4604 - acc: 0.8902 - val_loss: 3.2798 - val_acc: 0.3518
Epoch 6/200
95/96 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.9050Epoch 00005: val_acc improved from 0.53201 to 0.81914, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 66s - loss: 0.4099 - acc: 0.9060 - val_loss: 0.8019 - val_acc: 0.8191
Epoch 7/200
95/96 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.9141Epoch 00006: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.3834 - acc: 0.9139 - val_loss: 1.7161 - val_acc: 0.6693
Epoch 8/200
95/96 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.9285Epoch 00007: val_acc improved from 0.81914 to 0.85677, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 65s - loss: 0.3413 - acc: 0.9293 - val_loss: 0.5975 - val_acc: 0.8568
Epoch 9/200
95/96 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9280Epoch 00008: val_acc did not improve
96/96 [==============================] - 66s - loss: 0.3355 - acc: 0.9288 - val_loss: 0.8211 - val_acc: 0.8244
Epoch 10/200
95/96 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9383Epoch 00009: val_acc did not improve
96/96 [==============================] - 67s - loss: 0.3215 - acc: 0.9369 - val_loss: 7.7990 - val_acc: 0.1518
Epoch 11/200
95/96 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9510Epoch 00010: val_acc did not improve
96/96 [==============================] - 66s - loss: 0.2901 - acc: 0.9515 - val_loss: 0.6563 - val_acc: 0.8317
Epoch 12/200
95/96 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9539Epoch 00011: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.2726 - acc: 0.9544 - val_loss: 1.2534 - val_acc: 0.7419
Epoch 13/200
95/96 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9525Epoch 00012: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.2703 - acc: 0.9520 - val_loss: 3.6896 - val_acc: 0.5195
Epoch 14/200
95/96 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9560Epoch 00013: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.2575 - acc: 0.9565 - val_loss: 0.6773 - val_acc: 0.8521
Epoch 15/200
95/96 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9607Epoch 00014: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.2461 - acc: 0.9611 - val_loss: 2.1710 - val_acc: 0.7129
Epoch 16/200
95/96 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9632Epoch 00015: val_acc did not improve
96/96 [==============================] - 66s - loss: 0.2399 - acc: 0.9626 - val_loss: 1.7738 - val_acc: 0.6238
Epoch 17/200
95/96 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9629Epoch 00016: val_acc improved from 0.85677 to 0.91221, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 66s - loss: 0.2467 - acc: 0.9633 - val_loss: 0.4639 - val_acc: 0.9122
Epoch 18/200
95/96 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9685Epoch 00017: val_acc improved from 0.91221 to 0.93597, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 64s - loss: 0.2212 - acc: 0.9688 - val_loss: 0.3838 - val_acc: 0.9360
Epoch 19/200
95/96 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9627Epoch 00018: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.2376 - acc: 0.9631 - val_loss: 1.1270 - val_acc: 0.7340
Epoch 20/200
95/96 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9727Epoch 00019: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.2133 - acc: 0.9719 - val_loss: 0.7074 - val_acc: 0.8653
Epoch 21/200
95/96 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9772Epoch 00020: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.2028 - acc: 0.9754 - val_loss: 6.0049 - val_acc: 0.3215
Epoch 22/200
95/96 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9771Epoch 00021: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.2034 - acc: 0.9774 - val_loss: 1.0269 - val_acc: 0.8475
Epoch 23/200
95/96 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9764Epoch 00022: val_acc did not improve
96/96 [==============================] - 66s - loss: 0.2036 - acc: 0.9756 - val_loss: 2.3495 - val_acc: 0.6706
Epoch 24/200
95/96 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9807Epoch 00023: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.1892 - acc: 0.9809 - val_loss: 2.5582 - val_acc: 0.5135
Epoch 25/200
95/96 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9815Epoch 00024: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1871 - acc: 0.9817 - val_loss: 0.3841 - val_acc: 0.9300
Epoch 26/200
95/96 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9767Epoch 00025: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.1990 - acc: 0.9770 - val_loss: 3.0173 - val_acc: 0.4865
Epoch 27/200
95/96 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9828Epoch 00026: val_acc did not improve

Epoch 00026: reducing learning rate to 0.5.
96/96 [==============================] - 64s - loss: 0.1802 - acc: 0.9830 - val_loss: 0.8674 - val_acc: 0.8502
Epoch 28/200
95/96 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9909Epoch 00027: val_acc did not improve
96/96 [==============================] - 66s - loss: 0.1574 - acc: 0.9899 - val_loss: 0.6340 - val_acc: 0.8587
Epoch 29/200
95/96 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9934Epoch 00028: val_acc improved from 0.93597 to 0.93927, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 64s - loss: 0.1486 - acc: 0.9935 - val_loss: 0.3362 - val_acc: 0.9393
Epoch 30/200
95/96 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9956Epoch 00029: val_acc improved from 0.93927 to 0.96502, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 64s - loss: 0.1428 - acc: 0.9956 - val_loss: 0.2451 - val_acc: 0.9650
Epoch 31/200
95/96 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9928Epoch 00030: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.1469 - acc: 0.9928 - val_loss: 0.2756 - val_acc: 0.9604
Epoch 32/200
95/96 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9949Epoch 00031: val_acc did not improve
96/96 [==============================] - 65s - loss: 0.1411 - acc: 0.9950 - val_loss: 0.3353 - val_acc: 0.9380
Epoch 33/200
95/96 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9946Epoch 00032: val_acc improved from 0.96502 to 0.97294, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 64s - loss: 0.1401 - acc: 0.9946 - val_loss: 0.2238 - val_acc: 0.9729
Epoch 34/200
95/96 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9951Epoch 00033: val_acc improved from 0.97294 to 0.97294, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1398 - acc: 0.9952 - val_loss: 0.2284 - val_acc: 0.9729
Epoch 35/200
95/96 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9950Epoch 00034: val_acc did not improve
96/96 [==============================] - 64s - loss: 0.1390 - acc: 0.9950 - val_loss: 0.2400 - val_acc: 0.9677
Epoch 36/200
95/96 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9949Epoch 00035: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1380 - acc: 0.9950 - val_loss: 0.3366 - val_acc: 0.9492
Epoch 37/200
95/96 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9951Epoch 00036: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1397 - acc: 0.9941 - val_loss: 3.1081 - val_acc: 0.6508
Epoch 38/200
95/96 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9962Epoch 00037: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1340 - acc: 0.9963 - val_loss: 0.2553 - val_acc: 0.9558
Epoch 39/200
95/96 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9960Epoch 00038: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1327 - acc: 0.9960 - val_loss: 0.2250 - val_acc: 0.9637
Epoch 40/200
95/96 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9980Epoch 00039: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1281 - acc: 0.9980 - val_loss: 0.2377 - val_acc: 0.9644
Epoch 41/200
95/96 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9951Epoch 00040: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1325 - acc: 0.9951 - val_loss: 0.2900 - val_acc: 0.9551
Epoch 42/200
95/96 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9951Epoch 00041: val_acc did not improve

Epoch 00041: reducing learning rate to 0.25.
96/96 [==============================] - 63s - loss: 0.1309 - acc: 0.9951 - val_loss: 0.4286 - val_acc: 0.9069
Epoch 43/200
95/96 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9975Epoch 00042: val_acc improved from 0.97294 to 0.97888, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1245 - acc: 0.9976 - val_loss: 0.1885 - val_acc: 0.9789
Epoch 44/200
95/96 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9977Epoch 00043: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1227 - acc: 0.9977 - val_loss: 0.2527 - val_acc: 0.9624
Epoch 45/200
95/96 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9987Epoch 00044: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1206 - acc: 0.9987 - val_loss: 0.2358 - val_acc: 0.9690
Epoch 46/200
95/96 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9985Epoch 00045: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1198 - acc: 0.9985 - val_loss: 0.2094 - val_acc: 0.9762
Epoch 47/200
95/96 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9993Epoch 00046: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1184 - acc: 0.9993 - val_loss: 0.1960 - val_acc: 0.9743
Epoch 48/200
95/96 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9987Epoch 00047: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1188 - acc: 0.9987 - val_loss: 0.2026 - val_acc: 0.9756
Epoch 49/200
95/96 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9992Epoch 00048: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1167 - acc: 0.9992 - val_loss: 0.2169 - val_acc: 0.9683
Epoch 50/200
95/96 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9987Epoch 00049: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1218 - acc: 0.9977 - val_loss: 0.2458 - val_acc: 0.9597
Epoch 51/200
95/96 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9994Epoch 00050: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1158 - acc: 0.9994 - val_loss: 0.2325 - val_acc: 0.9736
Epoch 52/200
95/96 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9992Epoch 00051: val_acc did not improve

Epoch 00051: reducing learning rate to 0.125.
96/96 [==============================] - 63s - loss: 0.1157 - acc: 0.9992 - val_loss: 0.2266 - val_acc: 0.9677
Epoch 53/200
95/96 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9993Epoch 00052: val_acc improved from 0.97888 to 0.97954, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1140 - acc: 0.9993 - val_loss: 0.1876 - val_acc: 0.9795
Epoch 54/200
95/96 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9993Epoch 00053: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1138 - acc: 0.9993 - val_loss: 0.1933 - val_acc: 0.9776
Epoch 55/200
95/96 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9994Epoch 00054: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1132 - acc: 0.9994 - val_loss: 0.2121 - val_acc: 0.9762
Epoch 56/200
95/96 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9991Epoch 00055: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1133 - acc: 0.9991 - val_loss: 0.2192 - val_acc: 0.9762
Epoch 57/200
95/96 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9993Epoch 00056: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1127 - acc: 0.9993 - val_loss: 0.2023 - val_acc: 0.9782
Epoch 58/200
95/96 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9992Epoch 00057: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1121 - acc: 0.9992 - val_loss: 0.2341 - val_acc: 0.9716
Epoch 59/200
95/96 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9993Epoch 00058: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1124 - acc: 0.9993 - val_loss: 0.1920 - val_acc: 0.9762
Epoch 60/200
95/96 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9991Epoch 00059: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1123 - acc: 0.9991 - val_loss: 0.1990 - val_acc: 0.9756
Epoch 61/200
95/96 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9993Epoch 00060: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1113 - acc: 0.9993 - val_loss: 0.1927 - val_acc: 0.9782
Epoch 62/200
95/96 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9995Epoch 00061: val_acc did not improve

Epoch 00061: reducing learning rate to 0.0625.
96/96 [==============================] - 62s - loss: 0.1202 - acc: 0.9974 - val_loss: 0.2965 - val_acc: 0.9551
Epoch 63/200
95/96 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9995Epoch 00062: val_acc improved from 0.97954 to 0.98020, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1102 - acc: 0.9995 - val_loss: 0.1837 - val_acc: 0.9802
Epoch 64/200
95/96 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9995Epoch 00063: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1103 - acc: 0.9995 - val_loss: 0.2000 - val_acc: 0.9769
Epoch 65/200
95/96 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9997Epoch 00064: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1095 - acc: 0.9997 - val_loss: 0.2054 - val_acc: 0.9723
Epoch 66/200
95/96 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9994Epoch 00065: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1101 - acc: 0.9994 - val_loss: 0.1967 - val_acc: 0.9762
Epoch 67/200
95/96 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9998Epoch 00066: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1093 - acc: 0.9998 - val_loss: 0.1948 - val_acc: 0.9762
Epoch 68/200
95/96 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9995Epoch 00067: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1090 - acc: 0.9995 - val_loss: 0.2054 - val_acc: 0.9749
Epoch 69/200
95/96 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9994Epoch 00068: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1089 - acc: 0.9994 - val_loss: 0.2017 - val_acc: 0.9756
Epoch 70/200
95/96 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9993Epoch 00069: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1085 - acc: 0.9993 - val_loss: 0.1764 - val_acc: 0.9789
Epoch 71/200
95/96 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9993Epoch 00070: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1086 - acc: 0.9993 - val_loss: 0.2081 - val_acc: 0.9762
Epoch 72/200
95/96 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9993Epoch 00071: val_acc did not improve

Epoch 00071: reducing learning rate to 0.03125.
96/96 [==============================] - 63s - loss: 0.1086 - acc: 0.9993 - val_loss: 0.1976 - val_acc: 0.9762
Epoch 73/200
95/96 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9996Epoch 00072: val_acc improved from 0.98020 to 0.98152, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1081 - acc: 0.9996 - val_loss: 0.1848 - val_acc: 0.9815
Epoch 74/200
95/96 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9992Epoch 00073: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1089 - acc: 0.9992 - val_loss: 0.1852 - val_acc: 0.9756
Epoch 75/200
95/96 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9993Epoch 00074: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1084 - acc: 0.9993 - val_loss: 0.2026 - val_acc: 0.9743
Epoch 76/200
95/96 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9993Epoch 00075: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1080 - acc: 0.9993 - val_loss: 0.2041 - val_acc: 0.9776
Epoch 77/200
95/96 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9995Epoch 00076: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1080 - acc: 0.9995 - val_loss: 0.1932 - val_acc: 0.9762
Epoch 78/200
95/96 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9996Epoch 00077: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1076 - acc: 0.9996 - val_loss: 0.1932 - val_acc: 0.9795
Epoch 79/200
95/96 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9994Epoch 00078: val_acc improved from 0.98152 to 0.98218, saving model to 2017-06-01_vgg.h5
96/96 [==============================] - 63s - loss: 0.1080 - acc: 0.9994 - val_loss: 0.1648 - val_acc: 0.9822
Epoch 80/200
95/96 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9996Epoch 00079: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1074 - acc: 0.9996 - val_loss: 0.1830 - val_acc: 0.9782
Epoch 81/200
95/96 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9996Epoch 00080: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1075 - acc: 0.9996 - val_loss: 0.1977 - val_acc: 0.9782
Epoch 82/200
95/96 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9997Epoch 00081: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1069 - acc: 0.9997 - val_loss: 0.1966 - val_acc: 0.9762
Epoch 83/200
95/96 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9996Epoch 00082: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1072 - acc: 0.9996 - val_loss: 0.1908 - val_acc: 0.9776
Epoch 84/200
95/96 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9996Epoch 00083: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1068 - acc: 0.9996 - val_loss: 0.1986 - val_acc: 0.9762
Epoch 85/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9998Epoch 00084: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1077 - acc: 0.9987 - val_loss: 0.1911 - val_acc: 0.9782
Epoch 86/200
95/96 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9995Epoch 00085: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1072 - acc: 0.9995 - val_loss: 0.2019 - val_acc: 0.9762
Epoch 87/200
95/96 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9993Epoch 00086: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1080 - acc: 0.9983 - val_loss: 0.1923 - val_acc: 0.9776
Epoch 88/200
95/96 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9998Epoch 00087: val_acc did not improve

Epoch 00087: reducing learning rate to 0.015625.
96/96 [==============================] - 63s - loss: 0.1069 - acc: 0.9998 - val_loss: 0.1776 - val_acc: 0.9782
Epoch 89/200
95/96 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9995Epoch 00088: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1067 - acc: 0.9995 - val_loss: 0.1970 - val_acc: 0.9776
Epoch 90/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9998Epoch 00089: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1064 - acc: 0.9998 - val_loss: 0.1862 - val_acc: 0.9782
Epoch 91/200
95/96 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9994Epoch 00090: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1067 - acc: 0.9994 - val_loss: 0.1936 - val_acc: 0.9789
Epoch 92/200
95/96 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9998Epoch 00091: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1062 - acc: 0.9998 - val_loss: 0.1773 - val_acc: 0.9802
Epoch 93/200
95/96 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9998Epoch 00092: val_acc did not improve
96/96 [==============================] - 63s - loss: 0.1069 - acc: 0.9998 - val_loss: 0.1947 - val_acc: 0.9762
Epoch 94/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9996Epoch 00093: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1064 - acc: 0.9996 - val_loss: 0.1923 - val_acc: 0.9776
Epoch 95/200
95/96 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9997Epoch 00094: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1062 - acc: 0.9997 - val_loss: 0.1941 - val_acc: 0.9769
Epoch 96/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9995Epoch 00095: val_acc did not improve

Epoch 00095: reducing learning rate to 0.0078125.
96/96 [==============================] - 63s - loss: 0.1065 - acc: 0.9995 - val_loss: 0.1962 - val_acc: 0.9756
Epoch 97/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9995Epoch 00096: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1065 - acc: 0.9995 - val_loss: 0.1917 - val_acc: 0.9769
Epoch 98/200
95/96 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9995Epoch 00097: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1065 - acc: 0.9995 - val_loss: 0.2002 - val_acc: 0.9789
Epoch 99/200
95/96 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9995Epoch 00098: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1062 - acc: 0.9995 - val_loss: 0.2182 - val_acc: 0.9703
Epoch 100/200
95/96 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9995Epoch 00099: val_acc did not improve
96/96 [==============================] - 62s - loss: 0.1062 - acc: 0.9995 - val_loss: 0.1906 - val_acc: 0.9769
Epoch 00099: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

      12-8-Time       1.00      1.00      1.00        40
       2-2-Time       1.00      1.00      1.00        39
       2-4-Time       0.95      0.97      0.96        40
       3-4-Time       1.00      0.95      0.97        40
       3-8-Time       1.00      1.00      1.00        40
       4-4-Time       1.00      1.00      1.00        40
       6-8-Time       1.00      1.00      1.00        40
       9-8-Time       1.00      1.00      1.00        40
        Barline       0.98      1.00      0.99        40
         C-Clef       1.00      1.00      1.00        40
    Common-Time       1.00      0.97      0.99        40
       Cut-Time       1.00      1.00      1.00        40
            Dot       0.97      0.97      0.97        40
   Double-Sharp       1.00      1.00      1.00        40
    Eighth-Note       0.96      0.97      0.97        80
    Eighth-Rest       1.00      0.97      0.99        40
         F-Clef       1.00      1.00      1.00        40
           Flat       1.00      1.00      1.00        39
         G-Clef       1.00      1.00      1.00        40
      Half-Note       1.00      0.99      0.99        79
        Natural       0.97      0.95      0.96        40
   Quarter-Note       0.99      1.00      0.99        80
   Quarter-Rest       0.88      0.95      0.92        40
          Sharp       0.97      0.95      0.96        40
 Sixteenth-Note       0.92      0.96      0.94        80
 Sixteenth-Rest       0.93      0.93      0.93        40
Sixty-Four-Note       0.97      0.94      0.95        79
Sixty-Four-Rest       0.97      0.95      0.96        40
Thirty-Two-Note       0.92      0.91      0.92        79
Thirty-Two-Rest       0.92      0.90      0.91        40
Whole-Half-Rest       0.95      0.97      0.96        40
     Whole-Note       0.98      1.00      0.99        40

    avg / total       0.97      0.97      0.97      1515

Total Loss: 0.21341
Total Accuracy: 97.35974%
Total Error: 2.64026%
Execution time: 6405.8s
