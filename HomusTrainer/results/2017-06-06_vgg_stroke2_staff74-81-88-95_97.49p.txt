**********************
Windows PowerShell transcript start
Start time: 20170606122628
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Machine: DONKEY (Microsoft Windows NT 10.0.14393.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\TrainOnDonkey.ps1'
Process ID: 13572
PSVersion: 5.1.14393.1198
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.14393.1198
BuildVersion: 10.0.14393.1198
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\output_1.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 60800 images with 15200 symbols in 1 different stroke thicknesses ([2]) and with staff-lines with 4 different offsets from the top ([74, 81, 88, 95])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\data\images
60800/60800Deleting split directories...
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 128, 224, 16)      448
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 224, 16)      64
_________________________________________________________________
activation_1 (Activation)    (None, 128, 224, 16)      0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 128, 224, 16)      2320
_________________________________________________________________
batch_normalization_2 (Batch (None, 128, 224, 16)      64
_________________________________________________________________
activation_2 (Activation)    (None, 128, 224, 16)      0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 112, 16)       0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 64, 112, 32)       4640
_________________________________________________________________
batch_normalization_3 (Batch (None, 64, 112, 32)       128
_________________________________________________________________
activation_3 (Activation)    (None, 64, 112, 32)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 112, 32)       9248
_________________________________________________________________
batch_normalization_4 (Batch (None, 64, 112, 32)       128
_________________________________________________________________
activation_4 (Activation)    (None, 64, 112, 32)       0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 56, 32)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 32, 56, 64)        18496
_________________________________________________________________
batch_normalization_5 (Batch (None, 32, 56, 64)        256
_________________________________________________________________
activation_5 (Activation)    (None, 32, 56, 64)        0
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 32, 56, 64)        36928
_________________________________________________________________
batch_normalization_6 (Batch (None, 32, 56, 64)        256
_________________________________________________________________
activation_6 (Activation)    (None, 32, 56, 64)        0
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 32, 56, 64)        36928
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 56, 64)        256
_________________________________________________________________
activation_7 (Activation)    (None, 32, 56, 64)        0
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 28, 64)        0
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 16, 28, 128)       73856
_________________________________________________________________
batch_normalization_8 (Batch (None, 16, 28, 128)       512
_________________________________________________________________
activation_8 (Activation)    (None, 16, 28, 128)       0
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 16, 28, 128)       147584
_________________________________________________________________
batch_normalization_9 (Batch (None, 16, 28, 128)       512
_________________________________________________________________
activation_9 (Activation)    (None, 16, 28, 128)       0
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 16, 28, 128)       147584
_________________________________________________________________
batch_normalization_10 (Batc (None, 16, 28, 128)       512
_________________________________________________________________
activation_10 (Activation)   (None, 16, 28, 128)       0
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 8, 14, 128)        0
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 8, 14, 192)        221376
_________________________________________________________________
batch_normalization_11 (Batc (None, 8, 14, 192)        768
_________________________________________________________________
activation_11 (Activation)   (None, 8, 14, 192)        0
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 8, 14, 192)        331968
_________________________________________________________________
batch_normalization_12 (Batc (None, 8, 14, 192)        768
_________________________________________________________________
activation_12 (Activation)   (None, 8, 14, 192)        0
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 8, 14, 192)        331968
_________________________________________________________________
batch_normalization_13 (Batc (None, 8, 14, 192)        768
_________________________________________________________________
activation_13 (Activation)   (None, 8, 14, 192)        0
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 8, 14, 192)        331968
_________________________________________________________________
batch_normalization_14 (Batc (None, 8, 14, 192)        768
_________________________________________________________________
activation_14 (Activation)   (None, 8, 14, 192)        0
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 4, 7, 192)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064
_________________________________________________________________
output_node (Activation)     (None, 32)                0
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-06-06 12:31:03.614460: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.614549: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.615231: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.617314: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.618578: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.621117: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.621631: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.621965: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't
 compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-06 12:31:03.949783: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with prope
rties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-06 12:31:03.949896: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0
2017-06-06 12:31:03.951688: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y
2017-06-06 12:31:03.952050: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow devic
e (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 64, Early stopping after 20 epochs without improvement
Data-Shape: (128, 224, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-08}
Epoch 1/200
  9/191 [>.............................] - ETA: 128s - loss: 5.7857 - acc: 0.05732017-06-06 12:31:18.580227: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\window
s\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3613 get requests, put_count=3531 evicted_count=1000 eviction_rate=0.283206 and unsatisfied
 allocation rate=0.327152
2017-06-06 12:31:18.580308: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_lim
it_ from 100 to 110
190/191 [============================>.] - ETA: 0s - loss: 1.9226 - acc: 0.5189Epoch 00000: val_acc improved from -inf to 0.43762, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 74s - loss: 1.9188 - acc: 0.5188 - val_loss: 2.2638 - val_acc: 0.4376
Epoch 2/200
190/191 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.8049Epoch 00001: val_acc did not improve
191/191 [==============================] - 68s - loss: 0.7194 - acc: 0.8043 - val_loss: 3.3497 - val_acc: 0.3815
Epoch 3/200
190/191 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8567Epoch 00002: val_acc improved from 0.43762 to 0.63168, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 76s - loss: 0.5578 - acc: 0.8564 - val_loss: 1.9483 - val_acc: 0.6317
Epoch 4/200
190/191 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8876Epoch 00003: val_acc improved from 0.63168 to 0.81386, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 67s - loss: 0.4621 - acc: 0.8876 - val_loss: 0.7934 - val_acc: 0.8139
Epoch 5/200
190/191 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.8975Epoch 00004: val_acc improved from 0.81386 to 0.83102, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 67s - loss: 0.4315 - acc: 0.8975 - val_loss: 0.6607 - val_acc: 0.8310
Epoch 6/200
190/191 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.9121Epoch 00005: val_acc did not improve
191/191 [==============================] - 69s - loss: 0.3792 - acc: 0.9115 - val_loss: 1.1205 - val_acc: 0.6937
Epoch 7/200
190/191 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.9276Epoch 00006: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.3477 - acc: 0.9264 - val_loss: 3.7189 - val_acc: 0.4851
Epoch 8/200
190/191 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9372Epoch 00007: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.3177 - acc: 0.9365 - val_loss: 4.4136 - val_acc: 0.3710
Epoch 9/200
190/191 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9446Epoch 00008: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2981 - acc: 0.9443 - val_loss: 5.0179 - val_acc: 0.3861
Epoch 10/200
190/191 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9450Epoch 00009: val_acc did not improve
191/191 [==============================] - 68s - loss: 0.2950 - acc: 0.9453 - val_loss: 3.7099 - val_acc: 0.3901
Epoch 11/200
190/191 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9543Epoch 00010: val_acc improved from 0.83102 to 0.92343, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 64s - loss: 0.2688 - acc: 0.9545 - val_loss: 0.4299 - val_acc: 0.9234
Epoch 12/200
190/191 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9568Epoch 00011: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2593 - acc: 0.9565 - val_loss: 2.1714 - val_acc: 0.7076
Epoch 13/200
190/191 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9596Epoch 00012: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2520 - acc: 0.9598 - val_loss: 1.8611 - val_acc: 0.6832
Epoch 14/200
190/191 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9590Epoch 00013: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2502 - acc: 0.9576 - val_loss: 0.8826 - val_acc: 0.8297
Epoch 15/200
190/191 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9687Epoch 00014: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2231 - acc: 0.9688 - val_loss: 0.6539 - val_acc: 0.8416
Epoch 16/200
190/191 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9665Epoch 00015: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2296 - acc: 0.9657 - val_loss: 0.9530 - val_acc: 0.8521
Epoch 17/200
190/191 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9737Epoch 00016: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.2120 - acc: 0.9728 - val_loss: 1.0814 - val_acc: 0.8343
Epoch 18/200
190/191 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9763Epoch 00017: val_acc improved from 0.92343 to 0.92739, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 64s - loss: 0.2017 - acc: 0.9764 - val_loss: 0.3809 - val_acc: 0.9274
Epoch 19/200
190/191 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9727Epoch 00018: val_acc improved from 0.92739 to 0.95974, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 64s - loss: 0.2072 - acc: 0.9728 - val_loss: 0.2792 - val_acc: 0.9597
Epoch 20/200
190/191 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9739Epoch 00019: val_acc did not improve
191/191 [==============================] - 67s - loss: 0.2017 - acc: 0.9735 - val_loss: 5.6474 - val_acc: 0.3611
Epoch 21/200
190/191 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9786Epoch 00020: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1935 - acc: 0.9787 - val_loss: 0.4160 - val_acc: 0.9307
Epoch 22/200
190/191 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9791Epoch 00021: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1894 - acc: 0.9787 - val_loss: 0.4232 - val_acc: 0.9162
Epoch 23/200
190/191 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9786Epoch 00022: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1915 - acc: 0.9787 - val_loss: 7.9164 - val_acc: 0.3597
Epoch 24/200
190/191 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9805Epoch 00023: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1836 - acc: 0.9806 - val_loss: 0.5018 - val_acc: 0.9023
Epoch 25/200
190/191 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9783Epoch 00024: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1885 - acc: 0.9784 - val_loss: 1.3475 - val_acc: 0.7386
Epoch 26/200
190/191 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9839Epoch 00025: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1733 - acc: 0.9840 - val_loss: 0.3456 - val_acc: 0.9340
Epoch 27/200
190/191 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9841Epoch 00026: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1706 - acc: 0.9842 - val_loss: 0.3023 - val_acc: 0.9492
Epoch 28/200
190/191 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9825Epoch 00027: val_acc did not improve

Epoch 00027: reducing learning rate to 0.5.
191/191 [==============================] - 64s - loss: 0.1727 - acc: 0.9826 - val_loss: 0.3303 - val_acc: 0.9327
Epoch 29/200
190/191 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9927Epoch 00028: val_acc did not improve
191/191 [==============================] - 66s - loss: 0.1430 - acc: 0.9927 - val_loss: 0.3090 - val_acc: 0.9472
Epoch 30/200
190/191 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9948Epoch 00029: val_acc improved from 0.95974 to 0.97294, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 68s - loss: 0.1372 - acc: 0.9948 - val_loss: 0.2141 - val_acc: 0.9729
Epoch 31/200
190/191 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9928Epoch 00030: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1365 - acc: 0.9928 - val_loss: 0.2194 - val_acc: 0.9683
Epoch 32/200
190/191 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9957Epoch 00031: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1295 - acc: 0.9957 - val_loss: 0.2873 - val_acc: 0.9538
Epoch 33/200
190/191 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9952Epoch 00032: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.1300 - acc: 0.9953 - val_loss: 0.2136 - val_acc: 0.9716
Epoch 34/200
190/191 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9967Epoch 00033: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1258 - acc: 0.9962 - val_loss: 0.2424 - val_acc: 0.9617
Epoch 35/200
190/191 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9970Epoch 00034: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1216 - acc: 0.9970 - val_loss: 0.2540 - val_acc: 0.9657
Epoch 36/200
190/191 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9966Epoch 00035: val_acc did not improve
191/191 [==============================] - 66s - loss: 0.1217 - acc: 0.9966 - val_loss: 0.2147 - val_acc: 0.9710
Epoch 37/200
190/191 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9957Epoch 00036: val_acc did not improve
191/191 [==============================] - 72s - loss: 0.1220 - acc: 0.9957 - val_loss: 0.2274 - val_acc: 0.9637
Epoch 38/200
190/191 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9970Epoch 00037: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1167 - acc: 0.9971 - val_loss: 0.2359 - val_acc: 0.9624
Epoch 39/200
190/191 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9974Epoch 00038: val_acc did not improve

Epoch 00038: reducing learning rate to 0.25.
191/191 [==============================] - 67s - loss: 0.1163 - acc: 0.9974 - val_loss: 0.2376 - val_acc: 0.9657
Epoch 40/200
190/191 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9986Epoch 00039: val_acc improved from 0.97294 to 0.97360, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 66s - loss: 0.1117 - acc: 0.9986 - val_loss: 0.1947 - val_acc: 0.9736
Epoch 41/200
190/191 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9987Epoch 00040: val_acc improved from 0.97360 to 0.97492, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 67s - loss: 0.1097 - acc: 0.9987 - val_loss: 0.1985 - val_acc: 0.9749
Epoch 42/200
190/191 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9989Epoch 00041: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1088 - acc: 0.9989 - val_loss: 0.2012 - val_acc: 0.9696
Epoch 43/200
190/191 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9986Epoch 00042: val_acc did not improve
191/191 [==============================] - 66s - loss: 0.1082 - acc: 0.9986 - val_loss: 0.1889 - val_acc: 0.9743
Epoch 44/200
190/191 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9988Epoch 00043: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1059 - acc: 0.9989 - val_loss: 0.2276 - val_acc: 0.9696
Epoch 45/200
190/191 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9989Epoch 00044: val_acc did not improve
191/191 [==============================] - 68s - loss: 0.1055 - acc: 0.9989 - val_loss: 0.2011 - val_acc: 0.9743
Epoch 46/200
190/191 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9987Epoch 00045: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1044 - acc: 0.9987 - val_loss: 0.2343 - val_acc: 0.9677
Epoch 47/200
190/191 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9993Epoch 00046: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1031 - acc: 0.9993 - val_loss: 0.1944 - val_acc: 0.9736
Epoch 48/200
190/191 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9988Epoch 00047: val_acc improved from 0.97492 to 0.97822, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 64s - loss: 0.1013 - acc: 0.9988 - val_loss: 0.2003 - val_acc: 0.9782
Epoch 49/200
190/191 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9985Epoch 00048: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1015 - acc: 0.9985 - val_loss: 0.2004 - val_acc: 0.9749
Epoch 50/200
190/191 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9988Epoch 00049: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.1005 - acc: 0.9988 - val_loss: 0.1882 - val_acc: 0.9756
Epoch 51/200
190/191 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9989Epoch 00050: val_acc improved from 0.97822 to 0.98218, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 63s - loss: 0.0987 - acc: 0.9989 - val_loss: 0.1711 - val_acc: 0.9822
Epoch 52/200
190/191 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9988Epoch 00051: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0983 - acc: 0.9989 - val_loss: 0.2202 - val_acc: 0.9683
Epoch 53/200
190/191 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9988Epoch 00052: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0980 - acc: 0.9988 - val_loss: 0.1994 - val_acc: 0.9736
Epoch 54/200
190/191 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9991Epoch 00053: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0963 - acc: 0.9991 - val_loss: 0.1721 - val_acc: 0.9762
Epoch 55/200
190/191 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9993Epoch 00054: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0951 - acc: 0.9993 - val_loss: 0.1692 - val_acc: 0.9795
Epoch 56/200
190/191 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9991Epoch 00055: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0941 - acc: 0.9991 - val_loss: 0.1827 - val_acc: 0.9710
Epoch 57/200
190/191 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9993Epoch 00056: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0929 - acc: 0.9993 - val_loss: 0.1838 - val_acc: 0.9762
Epoch 58/200
190/191 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9991Epoch 00057: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0924 - acc: 0.9991 - val_loss: 0.1962 - val_acc: 0.9736
Epoch 59/200
190/191 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9995Epoch 00058: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0909 - acc: 0.9995 - val_loss: 0.2282 - val_acc: 0.9663
Epoch 60/200
190/191 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9993Epoch 00059: val_acc did not improve

Epoch 00059: reducing learning rate to 0.125.
191/191 [==============================] - 63s - loss: 0.0902 - acc: 0.9993 - val_loss: 0.1714 - val_acc: 0.9736
Epoch 61/200
190/191 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9992Epoch 00060: val_acc improved from 0.98218 to 0.98482, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 63s - loss: 0.0898 - acc: 0.9992 - val_loss: 0.1503 - val_acc: 0.9848
Epoch 62/200
190/191 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9991Epoch 00061: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0890 - acc: 0.9991 - val_loss: 0.1704 - val_acc: 0.9756
Epoch 63/200
190/191 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9994Epoch 00062: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0882 - acc: 0.9994 - val_loss: 0.1629 - val_acc: 0.9782
Epoch 64/200
190/191 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9993Epoch 00063: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0887 - acc: 0.9988 - val_loss: 0.1789 - val_acc: 0.9723
Epoch 65/200
190/191 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9994Epoch 00064: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0874 - acc: 0.9994 - val_loss: 0.2016 - val_acc: 0.9729
Epoch 66/200
190/191 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9990Epoch 00065: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0888 - acc: 0.9990 - val_loss: 0.1788 - val_acc: 0.9743
Epoch 67/200
190/191 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9998Epoch 00066: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0862 - acc: 0.9998 - val_loss: 0.1801 - val_acc: 0.9749
Epoch 68/200
190/191 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9994Epoch 00067: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0865 - acc: 0.9994 - val_loss: 0.1654 - val_acc: 0.9723
Epoch 69/200
190/191 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9996Epoch 00068: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0858 - acc: 0.9991 - val_loss: 0.1729 - val_acc: 0.9736
Epoch 70/200
190/191 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9994Epoch 00069: val_acc did not improve

Epoch 00069: reducing learning rate to 0.0625.
191/191 [==============================] - 63s - loss: 0.0850 - acc: 0.9994 - val_loss: 0.1660 - val_acc: 0.9736
Epoch 71/200
190/191 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9997Epoch 00070: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0841 - acc: 0.9997 - val_loss: 0.1596 - val_acc: 0.9802
Epoch 72/200
190/191 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9993Epoch 00071: val_acc did not improve
191/191 [==============================] - 63s - loss: 0.0849 - acc: 0.9993 - val_loss: 0.1720 - val_acc: 0.9736
Epoch 73/200
190/191 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9995Epoch 00072: val_acc improved from 0.98482 to 0.98548, saving model to 2017-06-06_vgg.h5
191/191 [==============================] - 64s - loss: 0.0842 - acc: 0.9995 - val_loss: 0.1440 - val_acc: 0.9855
Epoch 74/200
190/191 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9993Epoch 00073: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0842 - acc: 0.9993 - val_loss: 0.1702 - val_acc: 0.9756
Epoch 75/200
190/191 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9997Epoch 00074: val_acc did not improve
191/191 [==============================] - 66s - loss: 0.0832 - acc: 0.9997 - val_loss: 0.1736 - val_acc: 0.9776
Epoch 76/200
190/191 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9997Epoch 00075: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0829 - acc: 0.9997 - val_loss: 0.1839 - val_acc: 0.9756
Epoch 77/200
190/191 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9998Epoch 00076: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0828 - acc: 0.9998 - val_loss: 0.1640 - val_acc: 0.9776
Epoch 78/200
190/191 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9996Epoch 00077: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0826 - acc: 0.9996 - val_loss: 0.1747 - val_acc: 0.9782
Epoch 79/200
190/191 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9995Epoch 00078: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0832 - acc: 0.9995 - val_loss: 0.1639 - val_acc: 0.9795
Epoch 80/200
190/191 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9995Epoch 00079: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0828 - acc: 0.9995 - val_loss: 0.1841 - val_acc: 0.9789
Epoch 81/200
190/191 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9995Epoch 00080: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0823 - acc: 0.9995 - val_loss: 0.1682 - val_acc: 0.9802
Epoch 82/200
190/191 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9993Epoch 00081: val_acc did not improve

Epoch 00081: reducing learning rate to 0.03125.
191/191 [==============================] - 64s - loss: 0.0821 - acc: 0.9993 - val_loss: 0.1713 - val_acc: 0.9749
Epoch 83/200
190/191 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9997Epoch 00082: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0818 - acc: 0.9997 - val_loss: 0.1842 - val_acc: 0.9723
Epoch 84/200
190/191 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9997Epoch 00083: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0811 - acc: 0.9997 - val_loss: 0.1867 - val_acc: 0.9749
Epoch 85/200
190/191 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9997Epoch 00084: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0813 - acc: 0.9997 - val_loss: 0.1424 - val_acc: 0.9822
Epoch 86/200
190/191 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9996Epoch 00085: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0811 - acc: 0.9996 - val_loss: 0.1734 - val_acc: 0.9769
Epoch 87/200
190/191 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9997Epoch 00086: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0809 - acc: 0.9997 - val_loss: 0.1726 - val_acc: 0.9795
Epoch 88/200
190/191 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9996Epoch 00087: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0808 - acc: 0.9996 - val_loss: 0.1714 - val_acc: 0.9769
Epoch 89/200
190/191 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9997Epoch 00088: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0808 - acc: 0.9997 - val_loss: 0.1535 - val_acc: 0.9782
Epoch 90/200
190/191 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9993Epoch 00089: val_acc did not improve

Epoch 00089: reducing learning rate to 0.015625.
191/191 [==============================] - 64s - loss: 0.0812 - acc: 0.9993 - val_loss: 0.1948 - val_acc: 0.9729
Epoch 91/200
190/191 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9997Epoch 00090: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0803 - acc: 0.9997 - val_loss: 0.1634 - val_acc: 0.9782
Epoch 92/200
190/191 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9998Epoch 00091: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0801 - acc: 0.9998 - val_loss: 0.1729 - val_acc: 0.9769
Epoch 93/200
190/191 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9996Epoch 00092: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0806 - acc: 0.9996 - val_loss: 0.1671 - val_acc: 0.9789
Epoch 94/200
190/191 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9995Epoch 00093: val_acc did not improve
191/191 [==============================] - 64s - loss: 0.0808 - acc: 0.9995 - val_loss: 0.1606 - val_acc: 0.9815
Epoch 00093: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

       3-8-Time       0.98      1.00      0.99        40
       2-2-Time       1.00      1.00      1.00        39
       3-4-Time       0.97      0.97      0.97        40
 Sixteenth-Rest       1.00      0.97      0.99        40
       2-4-Time       1.00      1.00      1.00        40
Thirty-Two-Note       1.00      1.00      1.00        40
   Double-Sharp       0.98      1.00      0.99        40
        Barline       1.00      0.95      0.97        40
       4-4-Time       0.98      1.00      0.99        40
   Quarter-Note       1.00      1.00      1.00        40
       6-8-Time       1.00      1.00      1.00        40
            Dot       1.00      0.97      0.99        40
         F-Clef       0.97      0.97      0.97        40
     Whole-Note       1.00      1.00      1.00        40
 Sixteenth-Note       0.96      0.97      0.97        80
          Sharp       0.97      0.97      0.97        40
      12-8-Time       1.00      1.00      1.00        40
    Eighth-Note       1.00      1.00      1.00        39
         C-Clef       1.00      1.00      1.00        40
Sixty-Four-Rest       1.00      1.00      1.00        79
Thirty-Two-Rest       1.00      0.97      0.99        40
    Common-Time       1.00      1.00      1.00        80
       Cut-Time       0.93      0.93      0.93        40
           Flat       1.00      1.00      1.00        40
    Eighth-Rest       0.93      0.94      0.93        80
         G-Clef       0.95      0.93      0.94        40
      Half-Note       0.94      0.94      0.94        79
Sixty-Four-Note       0.97      0.97      0.97        40
Whole-Half-Rest       0.91      0.91      0.91        79
       9-8-Time       0.93      0.93      0.93        40
   Quarter-Rest       0.97      0.97      0.97        40
        Natural       0.98      1.00      0.99        40

    avg / total       0.98      0.97      0.97      1515

Total Loss: 0.18724
Total Accuracy: 97.49175%
Total Error: 2.50825%
Execution time: 6137.9s
**********************
Windows PowerShell transcript end
End time: 20170606141339
**********************
