**********************
Windows PowerShell transcript start
Start time: 20170801224400
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Machine: DONKEY (Microsoft Windows NT 10.0.15063.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\TrainModel.ps1'
Process ID: 13252
PSVersion: 5.1.15063.483
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.15063.483
BuildVersion: 10.0.15063.483
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\2017-07-31_res_net_3_small_96x96_no_fixed_canvas_Adadelta_mb16_homus_rebelo1_rebelo2_printed_audiveris_muscima_pp_fornes_openomr.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\data\images
15200/15200
Extracting Rebelo Symbol Dataset 1...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset1
Extracting Rebelo Symbol Dataset 2...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset2
Extracting Printed Music Symbol dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\PrintedMusicSymbolsDataset
Extracting Fornes Music Symbol dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Fornes-Music-Symbols
Extracting Audiveris OMR Dataset...
Extracting Symbols from Audiveris OMR Dataset...
Extracting MUSCIMA++ Dataset...
Extracting Symbols from Muscima++ Dataset...
Loading crop-objects from xml-files 140/140
Loaded 91255 crop-objects
Filtering 135 broken symbols
Filtering 7977 symbols from 7 ignored classes
Generating images from crop-object masks 37076/37076
Processing compound objects ...
Generating images from crop-object masks 18382/18382
Extracting OpenOMR dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\OpenOmrDataset
Resizing all images with the LANCZOS interpolation to 96x96px (width x height).
Resizing images 91054 of 91054Deleting split directories...
Splitting data into training, validation and test sets...
Copying 2 training files of 1-8-Time...
Copying 0 validation files of 1-8-Time...
Copying 0 test files of 1-8-Time...
Copying 323 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 319 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 353 training files of 2-4-Time...
Copying 44 validation files of 2-4-Time...
Copying 44 test files of 2-4-Time...
Copying 10 training files of 2-8-Time...
Copying 1 validation files of 2-8-Time...
Copying 1 test files of 2-8-Time...
Copying 387 training files of 3-4-Time...
Copying 48 validation files of 3-4-Time...
Copying 48 test files of 3-4-Time...
Copying 344 training files of 3-8-Time...
Copying 42 validation files of 3-8-Time...
Copying 42 test files of 3-8-Time...
Copying 6 training files of 4-2-Time...
Copying 0 validation files of 4-2-Time...
Copying 0 test files of 4-2-Time...
Copying 342 training files of 4-4-Time...
Copying 42 validation files of 4-4-Time...
Copying 42 test files of 4-4-Time...
Copying 1 training files of 4-8-Time...
Copying 0 validation files of 4-8-Time...
Copying 0 test files of 4-8-Time...
Copying 12 training files of 5-4-Time...
Copying 1 validation files of 5-4-Time...
Copying 1 test files of 5-4-Time...
Copying 11 training files of 5-8-Time...
Copying 1 validation files of 5-8-Time...
Copying 1 test files of 5-8-Time...
Copying 4 training files of 6-4-Time...
Copying 0 validation files of 6-4-Time...
Copying 0 test files of 6-4-Time...
Copying 337 training files of 6-8-Time...
Copying 42 validation files of 6-8-Time...
Copying 42 test files of 6-8-Time...
Copying 9 training files of 7-4-Time...
Copying 0 validation files of 7-4-Time...
Copying 0 test files of 7-4-Time...
Copying 6 training files of 8-8-Time...
Copying 0 validation files of 8-8-Time...
Copying 0 test files of 8-8-Time...
Copying 326 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 497 training files of Accent...
Copying 62 validation files of Accent...
Copying 62 test files of Accent...
Copying 17 training files of Arpeggio...
Copying 2 validation files of Arpeggio...
Copying 2 test files of Arpeggio...
Copying 5438 training files of Barline...
Copying 679 validation files of Barline...
Copying 679 test files of Barline...
Copying 6537 training files of Beam...
Copying 816 validation files of Beam...
Copying 816 test files of Beam...
Copying 289 training files of Brace...
Copying 35 validation files of Brace...
Copying 35 test files of Brace...
Copying 11 training files of Breath-Mark...
Copying 1 validation files of Breath-Mark...
Copying 1 test files of Breath-Mark...
Copying 22 training files of Breve...
Copying 2 validation files of Breve...
Copying 2 test files of Breve...
Copying 1353 training files of C-Clef...
Copying 168 validation files of C-Clef...
Copying 168 test files of C-Clef...
Copying 96 training files of Chord...
Copying 11 validation files of Chord...
Copying 11 test files of Chord...
Copying 2 training files of Coda...
Copying 0 validation files of Coda...
Copying 0 test files of Coda...
Copying 2 training files of Coda-Square...
Copying 0 validation files of Coda-Square...
Copying 0 test files of Coda-Square...
Copying 550 training files of Common-Time...
Copying 68 validation files of Common-Time...
Copying 68 test files of Common-Time...
Copying 510 training files of Cut-Time...
Copying 63 validation files of Cut-Time...
Copying 63 test files of Cut-Time...
Copying 3816 training files of Dot...
Copying 477 validation files of Dot...
Copying 477 test files of Dot...
Copying 8 training files of Dotted-Horizontal-Spanner...
Copying 0 validation files of Dotted-Horizontal-Spanner...
Copying 0 test files of Dotted-Horizontal-Spanner...
Copying 1 training files of Double-Flat...
Copying 0 validation files of Double-Flat...
Copying 0 test files of Double-Flat...
Copying 751 training files of Double-Sharp...
Copying 93 validation files of Double-Sharp...
Copying 93 test files of Double-Sharp...
Copying 120 training files of Double-Whole-Rest...
Copying 15 validation files of Double-Whole-Rest...
Copying 15 test files of Double-Whole-Rest...
Copying 14 training files of Eighth-Grace-Note...
Copying 1 validation files of Eighth-Grace-Note...
Copying 1 test files of Eighth-Grace-Note...
Copying 2511 training files of Eighth-Note...
Copying 313 validation files of Eighth-Note...
Copying 313 test files of Eighth-Note...
Copying 1805 training files of Eighth-Rest...
Copying 225 validation files of Eighth-Rest...
Copying 225 test files of Eighth-Rest...
Copying 1261 training files of F-Clef...
Copying 157 validation files of F-Clef...
Copying 157 test files of F-Clef...
Copying 111 training files of Fermata...
Copying 13 validation files of Fermata...
Copying 13 test files of Fermata...
Copying 2496 training files of Flat...
Copying 311 validation files of Flat...
Copying 311 test files of Flat...
Copying 1984 training files of G-Clef...
Copying 247 validation files of G-Clef...
Copying 247 test files of G-Clef...
Copying 97 training files of Glissando...
Copying 12 validation files of Glissando...
Copying 12 test files of Glissando...
Copying 179 training files of Hairpin-Crescendo...
Copying 22 validation files of Hairpin-Crescendo...
Copying 22 test files of Hairpin-Crescendo...
Copying 213 training files of Hairpin-Decrescendo...
Copying 26 validation files of Hairpin-Decrescendo...
Copying 26 test files of Hairpin-Decrescendo...
Copying 1192 training files of Half-Note...
Copying 148 validation files of Half-Note...
Copying 148 test files of Half-Note...
Copying 10 training files of Horizontal-Spanner...
Copying 1 validation files of Horizontal-Spanner...
Copying 1 test files of Horizontal-Spanner...
Copying 64 training files of Marcato...
Copying 7 validation files of Marcato...
Copying 7 test files of Marcato...
Copying 70 training files of Mordent...
Copying 8 validation files of Mordent...
Copying 8 test files of Mordent...
Copying 78 training files of Multiple-Eighth-Notes...
Copying 9 validation files of Multiple-Eighth-Notes...
Copying 9 test files of Multiple-Eighth-Notes...
Copying 27 training files of Multiple-Half-Notes...
Copying 3 validation files of Multiple-Half-Notes...
Copying 3 test files of Multiple-Half-Notes...
Copying 195 training files of Multiple-Quarter-Notes...
Copying 24 validation files of Multiple-Quarter-Notes...
Copying 24 test files of Multiple-Quarter-Notes...
Copying 69 training files of Multiple-Sixteenth-Notes...
Copying 8 validation files of Multiple-Sixteenth-Notes...
Copying 8 test files of Multiple-Sixteenth-Notes...
Copying 2511 training files of Natural...
Copying 313 validation files of Natural...
Copying 313 test files of Natural...
Copying 3 training files of Onehundred-Twenty-Eight-Note...
Copying 0 validation files of Onehundred-Twenty-Eight-Note...
Copying 0 test files of Onehundred-Twenty-Eight-Note...
Copying 3 training files of Onehundred-Twenty-Eight-Rest...
Copying 0 validation files of Onehundred-Twenty-Eight-Rest...
Copying 0 test files of Onehundred-Twenty-Eight-Rest...
Copying 5313 training files of Other...
Copying 664 validation files of Other...
Copying 664 test files of Other...
Copying 14356 training files of Quarter-Note...
Copying 1794 validation files of Quarter-Note...
Copying 1794 test files of Quarter-Note...
Copying 1417 training files of Quarter-Rest...
Copying 177 validation files of Quarter-Rest...
Copying 177 test files of Quarter-Rest...
Copying 14 training files of Repeat-Measure...
Copying 1 validation files of Repeat-Measure...
Copying 1 test files of Repeat-Measure...
Copying 2 training files of Segno...
Copying 0 validation files of Segno...
Copying 0 test files of Segno...
Copying 3424 training files of Sharp...
Copying 428 validation files of Sharp...
Copying 428 test files of Sharp...
Copying 1001 training files of Sixteenth-Note...
Copying 124 validation files of Sixteenth-Note...
Copying 124 test files of Sixteenth-Note...
Copying 936 training files of Sixteenth-Rest...
Copying 117 validation files of Sixteenth-Rest...
Copying 117 test files of Sixteenth-Rest...
Copying 706 training files of Sixty-Four-Note...
Copying 88 validation files of Sixty-Four-Note...
Copying 88 test files of Sixty-Four-Note...
Copying 433 training files of Sixty-Four-Rest...
Copying 54 validation files of Sixty-Four-Rest...
Copying 54 test files of Sixty-Four-Rest...
Copying 97 training files of Staccatissimo...
Copying 11 validation files of Staccatissimo...
Copying 11 test files of Staccatissimo...
Copying 69 training files of Stopped...
Copying 8 validation files of Stopped...
Copying 8 test files of Stopped...
Copying 193 training files of Tenuto...
Copying 24 validation files of Tenuto...
Copying 24 test files of Tenuto...
Copying 762 training files of Thirty-Two-Note...
Copying 95 validation files of Thirty-Two-Note...
Copying 95 test files of Thirty-Two-Note...
Copying 508 training files of Thirty-Two-Rest...
Copying 63 validation files of Thirty-Two-Rest...
Copying 63 test files of Thirty-Two-Rest...
Copying 3018 training files of Tie-Slur...
Copying 377 validation files of Tie-Slur...
Copying 377 test files of Tie-Slur...
Copying 145 training files of Trill...
Copying 17 validation files of Trill...
Copying 17 test files of Trill...
Copying 19 training files of Trill-Wobble...
Copying 2 validation files of Trill-Wobble...
Copying 2 test files of Trill-Wobble...
Copying 59 training files of Tuplet...
Copying 7 validation files of Tuplet...
Copying 7 test files of Tuplet...
Copying 65 training files of Turn...
Copying 8 validation files of Turn...
Copying 8 test files of Turn...
Copying 10 training files of Volta...
Copying 1 validation files of Volta...
Copying 1 test files of Volta...
Copying 783 training files of Whole-Half-Rest...
Copying 97 validation files of Whole-Half-Rest...
Copying 97 test files of Whole-Half-Rest...
Copying 1877 training files of Whole-Note...
Copying 234 validation files of Whole-Note...
Copying 234 test files of Whole-Note...
Loading configuration and data-readers...
Found 72912 images belonging to 79 classes.
Found 9071 images belonging to 79 classes.
Found 9071 images belonging to 79 classes.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 96, 96, 3)     0
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 96, 96, 16)    448         input_1[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 96, 96, 16)    64          conv2d_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 96, 96, 16)    0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 96, 96, 16)    2320        activation_1[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 96, 96, 16)    64          conv2d_2[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 96, 96, 16)    0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 96, 96, 16)    2320        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 96, 96, 16)    64          conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 96, 96, 16)    0           batch_normalization_3[0][0]
                                                                   activation_1[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 96, 96, 16)    0           add_1[0][0]
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 48, 48, 16)    0           activation_3[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 48, 48, 32)    4640        max_pooling2d_1[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 48, 48, 32)    128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 48, 48, 32)    0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 48, 48, 32)    9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 48, 48, 32)    128         conv2d_5[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 48, 48, 32)    4640        max_pooling2d_1[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 48, 48, 32)    0           batch_normalization_5[0][0]
                                                                   conv2d_6[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 48, 48, 32)    0           add_2[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 48, 48, 32)    9248        activation_5[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 48, 48, 32)    128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 48, 48, 32)    0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 48, 48, 32)    9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 48, 48, 32)    128         conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 48, 48, 32)    0           batch_normalization_7[0][0]
                                                                   activation_5[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 48, 48, 32)    0           add_3[0][0]
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 24, 24, 32)    0           activation_7[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 24, 24, 64)    18496       max_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 24, 24, 64)    256         conv2d_9[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 24, 24, 64)    0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 24, 24, 64)    36928       activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 24, 24, 64)    256         conv2d_10[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 24, 24, 64)    18496       max_pooling2d_2[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 24, 24, 64)    0           batch_normalization_9[0][0]
                                                                   conv2d_11[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 24, 24, 64)    0           add_4[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 24, 24, 64)    36928       activation_9[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 24, 24, 64)    256         conv2d_12[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 24, 24, 64)    0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 24, 24, 64)    36928       activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 24, 24, 64)    256         conv2d_13[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 24, 24, 64)    0           batch_normalization_11[0][0]
                                                                   activation_9[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 24, 24, 64)    0           add_5[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 24, 24, 64)    36928       activation_11[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 24, 24, 64)    256         conv2d_14[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 24, 24, 64)    0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 24, 24, 64)    36928       activation_12[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 24, 24, 64)    256         conv2d_15[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 24, 24, 64)    0           batch_normalization_13[0][0]
                                                                   activation_11[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 24, 24, 64)    0           add_6[0][0]
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 12, 12, 64)    0           activation_13[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 12, 12, 128)   73856       max_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 12, 12, 128)   512         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 12, 12, 128)   0           batch_normalization_14[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 12, 12, 128)   147584      activation_14[0][0]
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 12, 12, 128)   512         conv2d_17[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 12, 12, 128)   73856       max_pooling2d_3[0][0]
____________________________________________________________________________________________________
add_7 (Add)                      (None, 12, 12, 128)   0           batch_normalization_15[0][0]
                                                                   conv2d_18[0][0]
____________________________________________________________________________________________________
activation_15 (Activation)       (None, 12, 12, 128)   0           add_7[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 12, 12, 128)   147584      activation_15[0][0]
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 12, 12, 128)   512         conv2d_19[0][0]
____________________________________________________________________________________________________
activation_16 (Activation)       (None, 12, 12, 128)   0           batch_normalization_16[0][0]
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 12, 12, 128)   147584      activation_16[0][0]
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 12, 12, 128)   512         conv2d_20[0][0]
____________________________________________________________________________________________________
add_8 (Add)                      (None, 12, 12, 128)   0           batch_normalization_17[0][0]
                                                                   activation_15[0][0]
____________________________________________________________________________________________________
activation_17 (Activation)       (None, 12, 12, 128)   0           add_8[0][0]
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 12, 12, 128)   147584      activation_17[0][0]
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 12, 12, 128)   512         conv2d_21[0][0]
____________________________________________________________________________________________________
activation_18 (Activation)       (None, 12, 12, 128)   0           batch_normalization_18[0][0]
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 12, 12, 128)   147584      activation_18[0][0]
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 12, 12, 128)   512         conv2d_22[0][0]
____________________________________________________________________________________________________
add_9 (Add)                      (None, 12, 12, 128)   0           batch_normalization_19[0][0]
                                                                   activation_17[0][0]
____________________________________________________________________________________________________
activation_19 (Activation)       (None, 12, 12, 128)   0           add_9[0][0]
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 6, 6, 128)     0           activation_19[0][0]
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 6, 6, 256)     295168      max_pooling2d_4[0][0]
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 6, 6, 256)     1024        conv2d_23[0][0]
____________________________________________________________________________________________________
activation_20 (Activation)       (None, 6, 6, 256)     0           batch_normalization_20[0][0]
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 6, 6, 256)     590080      activation_20[0][0]
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 6, 6, 256)     1024        conv2d_24[0][0]
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 6, 6, 256)     295168      max_pooling2d_4[0][0]
____________________________________________________________________________________________________
add_10 (Add)                     (None, 6, 6, 256)     0           batch_normalization_21[0][0]
                                                                   conv2d_25[0][0]
____________________________________________________________________________________________________
activation_21 (Activation)       (None, 6, 6, 256)     0           add_10[0][0]
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 6, 6, 256)     590080      activation_21[0][0]
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 6, 6, 256)     1024        conv2d_26[0][0]
____________________________________________________________________________________________________
activation_22 (Activation)       (None, 6, 6, 256)     0           batch_normalization_22[0][0]
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 6, 6, 256)     590080      activation_22[0][0]
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 6, 6, 256)     1024        conv2d_27[0][0]
____________________________________________________________________________________________________
add_11 (Add)                     (None, 6, 6, 256)     0           batch_normalization_23[0][0]
                                                                   activation_21[0][0]
____________________________________________________________________________________________________
activation_23 (Activation)       (None, 6, 6, 256)     0           add_11[0][0]
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 6, 6, 256)     590080      activation_23[0][0]
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 6, 6, 256)     1024        conv2d_28[0][0]
____________________________________________________________________________________________________
activation_24 (Activation)       (None, 6, 6, 256)     0           batch_normalization_24[0][0]
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 6, 6, 256)     590080      activation_24[0][0]
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 6, 6, 256)     1024        conv2d_29[0][0]
____________________________________________________________________________________________________
add_12 (Add)                     (None, 6, 6, 256)     0           batch_normalization_25[0][0]
                                                                   activation_23[0][0]
____________________________________________________________________________________________________
activation_25 (Activation)       (None, 6, 6, 256)     0           add_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 3, 3, 256)     0           activation_25[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 2304)          0           average_pooling2d_1[0][0]
____________________________________________________________________________________________________
output_class (Dense)             (None, 79)            182095      flatten_1[0][0]
====================================================================================================
Total params: 4,883,663
Trainable params: 4,877,935
Non-trainable params: 5,728
____________________________________________________________________________________________________
Model res_net_3_small loaded.
2017-08-01 22:58:51.850249: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are avai
lable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.850394: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are ava
ilable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.851072: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are ava
ilable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.852614: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are a
vailable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.853139: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are a
vailable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.853709: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are avai
lable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.855994: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are ava
ilable on your machine and could speed up CPU computations.
2017-08-01 22:58:51.856361: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are avai
lable on your machine and could speed up CPU computations.
2017-08-01 22:58:52.171247: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-08-01 22:58:52.171368: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0
2017-08-01 22:58:52.173142: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y
2017-08-01 22:58:52.173603: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti,
 pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 16, Early stopping after 20 epochs without improvement
Data-Shape: (96, 96, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'decay': 0.0, 'lr': 1.0, 'rho': 0.95, 'epsilon': 1e-08}
Performing object localization: False
Training on dataset...
Epoch 1/200
   3/4557 [..............................] - ETA: 9715s - loss: 7.0614 - acc: 0.1458 2017-08-01 22:59:11.401062: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocat
or.cc:247] PoolAllocator: After 2425 get requests, put_count=2234 evicted_count=1000 eviction_rate=0.447628 and unsatisfied allocation rate=0.532371
2017-08-01 22:59:11.401162: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
  50/4557 [..............................] - ETA: 840s - loss: 4.9100 - acc: 0.34122017-08-01 22:59:14.326284: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocator
.cc:247] PoolAllocator: After 4783 get requests, put_count=4725 evicted_count=1000 eviction_rate=0.21164 and unsatisfied allocation rate=0.226009
2017-08-01 22:59:14.326377: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
4556/4557 [============================>.] - ETA: 0s - loss: 0.9274 - acc: 0.8336Epoch 00000: val_acc improved from -inf to 0.91203, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 300s - loss: 0.9274 - acc: 0.8335 - val_loss: 0.5612 - val_acc: 0.9120
Epoch 2/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.9196Epoch 00001: val_acc improved from 0.91203 to 0.93793, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 293s - loss: 0.5199 - acc: 0.9197 - val_loss: 0.4567 - val_acc: 0.9379
Epoch 3/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.9354Epoch 00002: val_acc improved from 0.93793 to 0.94786, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4462 - acc: 0.9354 - val_loss: 0.4066 - val_acc: 0.9479
Epoch 4/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.9425Epoch 00003: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.4177 - acc: 0.9425 - val_loss: 0.4092 - val_acc: 0.9466
Epoch 5/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.9476Epoch 00004: val_acc improved from 0.94786 to 0.95513, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4106 - acc: 0.9476 - val_loss: 0.4042 - val_acc: 0.9551
Epoch 6/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.9508Epoch 00005: val_acc improved from 0.95513 to 0.95701, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 293s - loss: 0.4179 - acc: 0.9507 - val_loss: 0.4157 - val_acc: 0.9570
Epoch 7/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.9541Epoch 00006: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4254 - acc: 0.9541 - val_loss: 0.4385 - val_acc: 0.9525
Epoch 8/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.9586Epoch 00007: val_acc improved from 0.95701 to 0.96439, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4285 - acc: 0.9586 - val_loss: 0.4259 - val_acc: 0.9644
Epoch 9/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.9584Epoch 00008: val_acc improved from 0.96439 to 0.96571, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4453 - acc: 0.9584 - val_loss: 0.4460 - val_acc: 0.9657
Epoch 10/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.9587Epoch 00009: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.4632 - acc: 0.9587 - val_loss: 0.4849 - val_acc: 0.9638
Epoch 11/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.9621Epoch 00010: val_acc improved from 0.96571 to 0.96803, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4732 - acc: 0.9621 - val_loss: 0.4755 - val_acc: 0.9680
Epoch 12/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.9627Epoch 00011: val_acc improved from 0.96803 to 0.96902, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.4891 - acc: 0.9627 - val_loss: 0.4954 - val_acc: 0.9690
Epoch 13/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.9636Epoch 00012: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5004 - acc: 0.9636 - val_loss: 0.5571 - val_acc: 0.9555
Epoch 14/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.9646Epoch 00013: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5133 - acc: 0.9646 - val_loss: 0.5233 - val_acc: 0.9660
Epoch 15/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.9643Epoch 00014: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5279 - acc: 0.9643 - val_loss: 0.5378 - val_acc: 0.9674
Epoch 16/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.9664Epoch 00015: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5408 - acc: 0.9664 - val_loss: 0.6555 - val_acc: 0.9491
Epoch 17/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.9664Epoch 00016: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5538 - acc: 0.9664 - val_loss: 0.5776 - val_acc: 0.9632
Epoch 18/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.9665Epoch 00017: val_acc improved from 0.96902 to 0.97200, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.5653 - acc: 0.9665 - val_loss: 0.5596 - val_acc: 0.9720
Epoch 19/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.9680Epoch 00018: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5746 - acc: 0.9680 - val_loss: 0.5713 - val_acc: 0.9690
Epoch 20/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5841 - acc: 0.9672Epoch 00019: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5841 - acc: 0.9672 - val_loss: 0.5859 - val_acc: 0.9696
Epoch 21/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.9677Epoch 00020: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5927 - acc: 0.9677 - val_loss: 0.6206 - val_acc: 0.9641
Epoch 22/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.9691Epoch 00021: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.6010 - acc: 0.9691 - val_loss: 0.6246 - val_acc: 0.9655
Epoch 23/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.9685Epoch 00022: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.6087 - acc: 0.9685 - val_loss: 0.6310 - val_acc: 0.9652
Epoch 24/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6157 - acc: 0.9696Epoch 00023: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.6156 - acc: 0.9696 - val_loss: 0.6562 - val_acc: 0.9662
Epoch 25/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.9700Epoch 00024: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.6220 - acc: 0.9700 - val_loss: 0.6678 - val_acc: 0.9644
Epoch 26/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6320 - acc: 0.9701Epoch 00025: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.6320 - acc: 0.9701 - val_loss: 0.6487 - val_acc: 0.9699
Epoch 27/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.9711Epoch 00026: val_acc did not improve

Epoch 00026: reducing learning rate to 0.5.
4557/4557 [==============================] - 294s - loss: 0.6384 - acc: 0.9711 - val_loss: 0.7006 - val_acc: 0.9656
Epoch 28/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.9810Epoch 00027: val_acc improved from 0.97200 to 0.97630, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.5941 - acc: 0.9810 - val_loss: 0.6152 - val_acc: 0.9763
Epoch 29/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.9820Epoch 00028: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5800 - acc: 0.9820 - val_loss: 0.6166 - val_acc: 0.9732
Epoch 30/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.9822Epoch 00029: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5709 - acc: 0.9822 - val_loss: 0.6235 - val_acc: 0.9708
Epoch 31/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.9827Epoch 00030: val_acc improved from 0.97630 to 0.97718, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.5663 - acc: 0.9827 - val_loss: 0.6009 - val_acc: 0.9772
Epoch 32/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.9820Epoch 00031: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5636 - acc: 0.9820 - val_loss: 0.7321 - val_acc: 0.9617
Epoch 33/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.9823Epoch 00032: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5602 - acc: 0.9823 - val_loss: 0.6017 - val_acc: 0.9716
Epoch 34/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.9826Epoch 00033: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5603 - acc: 0.9826 - val_loss: 0.6221 - val_acc: 0.9686
Epoch 35/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.9828Epoch 00034: val_acc did not improve
4557/4557 [==============================] - 293s - loss: 0.5582 - acc: 0.9828 - val_loss: 0.5795 - val_acc: 0.9772
Epoch 36/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.9820Epoch 00035: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5549 - acc: 0.9820 - val_loss: 0.5855 - val_acc: 0.9757
Epoch 37/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.9828Epoch 00036: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5534 - acc: 0.9828 - val_loss: 0.5922 - val_acc: 0.9766
Epoch 38/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.9827Epoch 00037: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5553 - acc: 0.9827 - val_loss: 0.6067 - val_acc: 0.9710
Epoch 39/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.9827Epoch 00038: val_acc improved from 0.97718 to 0.97751, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.5548 - acc: 0.9827 - val_loss: 0.5868 - val_acc: 0.9775
Epoch 40/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.9830Epoch 00039: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5563 - acc: 0.9830 - val_loss: 0.6209 - val_acc: 0.9749
Epoch 41/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.9834Epoch 00040: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5570 - acc: 0.9834 - val_loss: 0.6111 - val_acc: 0.9724
Epoch 42/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.9835Epoch 00041: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5572 - acc: 0.9835 - val_loss: 0.5997 - val_acc: 0.9746
Epoch 43/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.9838Epoch 00042: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5573 - acc: 0.9838 - val_loss: 0.6012 - val_acc: 0.9746
Epoch 44/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5569 - acc: 0.9826Epoch 00043: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5569 - acc: 0.9827 - val_loss: 0.5971 - val_acc: 0.9745
Epoch 45/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.9835Epoch 00044: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5568 - acc: 0.9835 - val_loss: 0.6133 - val_acc: 0.9722
Epoch 46/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.9822Epoch 00045: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5594 - acc: 0.9822 - val_loss: 0.6182 - val_acc: 0.9724
Epoch 47/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.9839Epoch 00046: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5578 - acc: 0.9839 - val_loss: 0.6223 - val_acc: 0.9724
Epoch 48/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.9838Epoch 00047: val_acc did not improve

Epoch 00047: reducing learning rate to 0.25.
4557/4557 [==============================] - 294s - loss: 0.5597 - acc: 0.9838 - val_loss: 0.6466 - val_acc: 0.9705
Epoch 49/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.9899Epoch 00048: val_acc improved from 0.97751 to 0.98093, saving model to 2017-08-01_res_net_3_small.h5
4557/4557 [==============================] - 294s - loss: 0.5345 - acc: 0.9899 - val_loss: 0.5732 - val_acc: 0.9809
Epoch 50/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.9903Epoch 00049: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5223 - acc: 0.9903 - val_loss: 0.5737 - val_acc: 0.9778
Epoch 51/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.9905Epoch 00050: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5168 - acc: 0.9905 - val_loss: 0.5696 - val_acc: 0.9795
Epoch 52/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.9909Epoch 00051: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5103 - acc: 0.9909 - val_loss: 0.5613 - val_acc: 0.9798
Epoch 53/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.9912Epoch 00052: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5042 - acc: 0.9912 - val_loss: 0.5726 - val_acc: 0.9766
Epoch 54/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.9908Epoch 00053: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.5010 - acc: 0.9908 - val_loss: 0.5645 - val_acc: 0.9771
Epoch 55/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.9907Epoch 00054: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4957 - acc: 0.9907 - val_loss: 0.5671 - val_acc: 0.9783
Epoch 56/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.9904Epoch 00055: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4917 - acc: 0.9904 - val_loss: 0.5713 - val_acc: 0.9752
Epoch 57/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.9909Epoch 00056: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4868 - acc: 0.9909 - val_loss: 0.5528 - val_acc: 0.9777
Epoch 58/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.9908Epoch 00057: val_acc did not improve

Epoch 00057: reducing learning rate to 0.125.
4557/4557 [==============================] - 294s - loss: 0.4832 - acc: 0.9908 - val_loss: 0.5638 - val_acc: 0.9767
Epoch 59/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.9934Epoch 00058: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4696 - acc: 0.9934 - val_loss: 0.5409 - val_acc: 0.9784
Epoch 60/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.9944Epoch 00059: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4610 - acc: 0.9944 - val_loss: 0.5347 - val_acc: 0.9776
Epoch 61/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.9944Epoch 00060: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4547 - acc: 0.9944 - val_loss: 0.5359 - val_acc: 0.9780
Epoch 62/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.9942Epoch 00061: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4511 - acc: 0.9942 - val_loss: 0.5246 - val_acc: 0.9783
Epoch 63/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.9942Epoch 00062: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4461 - acc: 0.9942 - val_loss: 0.5187 - val_acc: 0.9781
Epoch 64/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.9945Epoch 00063: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4415 - acc: 0.9944 - val_loss: 0.5150 - val_acc: 0.9787
Epoch 65/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.9940Epoch 00064: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4380 - acc: 0.9940 - val_loss: 0.5081 - val_acc: 0.9799
Epoch 66/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.9943Epoch 00065: val_acc did not improve

Epoch 00065: reducing learning rate to 0.0625.
4557/4557 [==============================] - 294s - loss: 0.4327 - acc: 0.9943 - val_loss: 0.5233 - val_acc: 0.9770
Epoch 67/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.9956Epoch 00066: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4241 - acc: 0.9956 - val_loss: 0.5020 - val_acc: 0.9794
Epoch 68/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.9959Epoch 00067: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4209 - acc: 0.9959 - val_loss: 0.4970 - val_acc: 0.9799
Epoch 69/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.9962Epoch 00068: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4162 - acc: 0.9962 - val_loss: 0.5080 - val_acc: 0.9788
Epoch 70/200
4556/4557 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.9961Epoch 00069: val_acc did not improve
4557/4557 [==============================] - 294s - loss: 0.4139 - acc: 0.9961 - val_loss: 0.4982 - val_acc: 0.9798
Epoch 00069: early stopping
Loading best model from check-point and testing...
C:\Programming\Anaconda3-4.2.0\lib\site-packages\sklearn\metrics\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
C:\Programming\Anaconda3-4.2.0\lib\site-packages\sklearn\metrics\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
Traceback (most recent call last):
  File "C:/Users/Alex/Repositories/MusicSymbolClassifier/ModelTrainer/TrainModel.py", line 267, in <module>
    datasets=datasets)
  File "C:/Users/Alex/Repositories/MusicSymbolClassifier/ModelTrainer/TrainModel.py", line 141, in train_model
    target_names=names_of_classes_with_test_data)
  File "C:\Programming\Anaconda3-4.2.0\lib\site-packages\sklearn\metrics\classification.py", line 1418, in classification_report
    values = [target_names[i]]
IndexError: list index out of range
**********************
Windows PowerShell transcript end
End time: 20170802044231
**********************
