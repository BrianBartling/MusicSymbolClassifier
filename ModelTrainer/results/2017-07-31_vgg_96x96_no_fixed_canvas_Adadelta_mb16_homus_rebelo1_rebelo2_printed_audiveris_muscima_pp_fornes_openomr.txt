**********************
Windows PowerShell transcript start
Start time: 20170802174107
Username: MONSTI\Alex
RunAs User: MONSTI\Alex
Machine: MONSTI (Microsoft Windows NT 10.0.15063.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\TrainModel.ps1'
Process ID: 11148
PSVersion: 5.1.15063.483
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.15063.483
BuildVersion: 10.0.15063.483
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\2017-07-31_vgg_96x96_no_fixed_canvas_Adadelta_mb16_homus_rebelo1_rebelo2_printed_audiveris_muscima_pp_fornes_openomr.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\data\images
15200/15200
Resizing all images with the LANCZOS interpolation to 96x96px (width x height).
Resizing images 15200 of 15200Deleting split directories...
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Loading configuration and data-readers...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 96, 96, 16)        448
_________________________________________________________________
batch_normalization_1 (Batch (None, 96, 96, 16)        64
_________________________________________________________________
activation_1 (Activation)    (None, 96, 96, 16)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 96, 96, 16)        2320
_________________________________________________________________
batch_normalization_2 (Batch (None, 96, 96, 16)        64
_________________________________________________________________
activation_2 (Activation)    (None, 96, 96, 16)        0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 48, 16)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 48, 48, 32)        4640
_________________________________________________________________
batch_normalization_3 (Batch (None, 48, 48, 32)        128
_________________________________________________________________
activation_3 (Activation)    (None, 48, 48, 32)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 48, 48, 32)        9248
_________________________________________________________________
batch_normalization_4 (Batch (None, 48, 48, 32)        128
_________________________________________________________________
activation_4 (Activation)    (None, 48, 48, 32)        0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 24, 24, 32)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496
_________________________________________________________________
batch_normalization_5 (Batch (None, 24, 24, 64)        256
_________________________________________________________________
activation_5 (Activation)    (None, 24, 24, 64)        0
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 24, 24, 64)        36928
_________________________________________________________________
batch_normalization_6 (Batch (None, 24, 24, 64)        256
_________________________________________________________________
activation_6 (Activation)    (None, 24, 24, 64)        0
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 24, 64)        36928
_________________________________________________________________
batch_normalization_7 (Batch (None, 24, 24, 64)        256
_________________________________________________________________
activation_7 (Activation)    (None, 24, 24, 64)        0
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 12, 12, 128)       73856
_________________________________________________________________
batch_normalization_8 (Batch (None, 12, 12, 128)       512
_________________________________________________________________
activation_8 (Activation)    (None, 12, 12, 128)       0
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 12, 12, 128)       147584
_________________________________________________________________
batch_normalization_9 (Batch (None, 12, 12, 128)       512
_________________________________________________________________
activation_9 (Activation)    (None, 12, 12, 128)       0
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 12, 12, 128)       147584
_________________________________________________________________
batch_normalization_10 (Batc (None, 12, 12, 128)       512
_________________________________________________________________
activation_10 (Activation)   (None, 12, 12, 128)       0
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 6, 6, 192)         221376
_________________________________________________________________
batch_normalization_11 (Batc (None, 6, 6, 192)         768
_________________________________________________________________
activation_11 (Activation)   (None, 6, 6, 192)         0
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 6, 6, 192)         331968
_________________________________________________________________
batch_normalization_12 (Batc (None, 6, 6, 192)         768
_________________________________________________________________
activation_12 (Activation)   (None, 6, 6, 192)         0
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 6, 6, 192)         331968
_________________________________________________________________
batch_normalization_13 (Batc (None, 6, 6, 192)         768
_________________________________________________________________
activation_13 (Activation)   (None, 6, 6, 192)         0
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 6, 6, 192)         331968
_________________________________________________________________
batch_normalization_14 (Batc (None, 6, 6, 192)         768
_________________________________________________________________
activation_14 (Activation)   (None, 6, 6, 192)         0
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 3, 3, 192)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 1728)              0
_________________________________________________________________
output_class (Dense)         (None, 32)                55328
=================================================================
Total params: 1,756,400
Trainable params: 1,753,520
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-08-02 17:47:22.569995: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are av
ailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.570531: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are a
vailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.571401: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are a
vailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.571629: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are
 available on your machine and could speed up CPU computations.
2017-08-02 17:47:22.571850: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are
 available on your machine and could speed up CPU computations.
2017-08-02 17:47:22.572056: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are av
ailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.572263: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are a
vailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.572450: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are av
ailable on your machine and could speed up CPU computations.
2017-08-02 17:47:22.986946: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2017-08-02 17:47:22.987228: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0
2017-08-02 17:47:22.988343: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y
2017-08-02 17:47:22.988770: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, p
ci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 16, Early stopping after 20 epochs without improvement
Data-Shape: (96, 96, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-08}
Performing object localization: False
Training on dataset...
Epoch 1/200
  9/761 [..............................] - ETA: 451s - loss: 4.5460 - acc: 0.09722017-08-02 17:47:36.921423: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\pool_allocator
.cc:247] PoolAllocator: After 3056 get requests, put_count=3042 evicted_count=1000 eviction_rate=0.328731 and unsatisfied allocation rate=0.364529
761/761 [==============================] - 77s - loss: 1.1489 - acc: 0.6906 - val_loss: 1.1954 - val_acc: 0.7254m -inf to 0.72541, saving model to 2017-08-02_vgg.h5ze_limit_ from 100 to 110
Epoch 2/200
761/761 [==============================] - 73s - loss: 0.5823 - acc: 0.8523 - val_loss: 0.5475 - val_acc: 0.8634m 0.72541 to 0.86337, saving model to 2017-08-02_vgg.h5
Epoch 3/200
761/761 [==============================] - 73s - loss: 0.4496 - acc: 0.8937 - val_loss: 0.5069 - val_acc: 0.8917m 0.86337 to 0.89175, saving model to 2017-08-02_vgg.h5
Epoch 4/200
761/761 [==============================] - 74s - loss: 0.3851 - acc: 0.9192 - val_loss: 0.5206 - val_acc: 0.8759ove
Epoch 5/200
761/761 [==============================] - 72s - loss: 0.3504 - acc: 0.9307 - val_loss: 0.3742 - val_acc: 0.9267m 0.89175 to 0.92673, saving model to 2017-08-02_vgg.h5
Epoch 6/200
761/761 [==============================] - 72s - loss: 0.3157 - acc: 0.9433 - val_loss: 0.3909 - val_acc: 0.9327m 0.92673 to 0.93267, saving model to 2017-08-02_vgg.h5
Epoch 7/200
761/761 [==============================] - 71s - loss: 0.2927 - acc: 0.9517 - val_loss: 0.3989 - val_acc: 0.9116ove
Epoch 8/200
761/761 [==============================] - 72s - loss: 0.2762 - acc: 0.9578 - val_loss: 0.3859 - val_acc: 0.9353m 0.93267 to 0.93531, saving model to 2017-08-02_vgg.h5
Epoch 9/200
761/761 [==============================] - 71s - loss: 0.2532 - acc: 0.9663 - val_loss: 0.4019 - val_acc: 0.9347ove
Epoch 10/200
761/761 [==============================] - 75s - loss: 0.2510 - acc: 0.9655 - val_loss: 0.3529 - val_acc: 0.9393m 0.93531 to 0.93927, saving model to 2017-08-02_vgg.h5
Epoch 11/200
761/761 [==============================] - 72s - loss: 0.2405 - acc: 0.9685 - val_loss: 0.4044 - val_acc: 0.9248ove
Epoch 12/200
761/761 [==============================] - 72s - loss: 0.2335 - acc: 0.9728 - val_loss: 0.3514 - val_acc: 0.9419m 0.93927 to 0.94191, saving model to 2017-08-02_vgg.h5
Epoch 13/200
761/761 [==============================] - 72s - loss: 0.2213 - acc: 0.9736 - val_loss: 0.3106 - val_acc: 0.9465m 0.94191 to 0.94653, saving model to 2017-08-02_vgg.h5
Epoch 14/200
761/761 [==============================] - 72s - loss: 0.2197 - acc: 0.9738 - val_loss: 0.2871 - val_acc: 0.9578m 0.94653 to 0.95776, saving model to 2017-08-02_vgg.h5
Epoch 15/200
761/761 [==============================] - 71s - loss: 0.2042 - acc: 0.9797 - val_loss: 0.3237 - val_acc: 0.9459ove
Epoch 16/200
761/761 [==============================] - 73s - loss: 0.1986 - acc: 0.9810 - val_loss: 0.2801 - val_acc: 0.9571ove
Epoch 17/200
761/761 [==============================] - 71s - loss: 0.1936 - acc: 0.9810 - val_loss: 0.3403 - val_acc: 0.9505ove
Epoch 18/200
761/761 [==============================] - 72s - loss: 0.1918 - acc: 0.9827 - val_loss: 0.2971 - val_acc: 0.9591m 0.95776 to 0.95908, saving model to 2017-08-02_vgg.h5
Epoch 19/200
761/761 [==============================] - 71s - loss: 0.1817 - acc: 0.9840 - val_loss: 0.2757 - val_acc: 0.9630m 0.95908 to 0.96304, saving model to 2017-08-02_vgg.h5
Epoch 20/200
761/761 [==============================] - 72s - loss: 0.1792 - acc: 0.9841 - val_loss: 0.2885 - val_acc: 0.9604ove
Epoch 21/200
761/761 [==============================] - 72s - loss: 0.1754 - acc: 0.9837 - val_loss: 0.2981 - val_acc: 0.9538ove
Epoch 22/200
761/761 [==============================] - 72s - loss: 0.1695 - acc: 0.9867 - val_loss: 0.2753 - val_acc: 0.9584ove
Epoch 23/200
761/761 [==============================] - 72s - loss: 0.1693 - acc: 0.9850 - val_loss: 0.2765 - val_acc: 0.9630ove
Epoch 24/200
761/761 [==============================] - 73s - loss: 0.1612 - acc: 0.9873 - val_loss: 0.2484 - val_acc: 0.9578ove
Epoch 25/200
761/761 [==============================] - 71s - loss: 0.1584 - acc: 0.9881 - val_loss: 0.2802 - val_acc: 0.9545ove
Epoch 26/200
761/761 [==============================] - 72s - loss: 0.1567 - acc: 0.9874 - val_loss: 0.3218 - val_acc: 0.9518ove
Epoch 27/200
761/761 [==============================] - 642s - loss: 0.1548 - acc: 0.9873 - val_loss: 0.2607 - val_acc: 0.9564ve
Epoch 28/200
509/761 [===================>..........] - ETA: 962s - loss: 0.1514 - acc: 0.9891forrtl: error (200): program aborting due to control-C event
Image              PC                Routine            Line        Source
libifcoremd.dll    00007FF9380F43E4  Unknown               Unknown  Unknown
KERNELBASE.dll     00007FF96E14717D  Unknown               Unknown  Unknown
KERNEL32.DLL       00007FF971362774  Unknown               Unknown  Unknown
ntdll.dll          00007FF971C20D51  Unknown               Unknown  Unknown
PS>TerminatingError(): "The pipeline has been stopped."
>> TerminatingError(): "The pipeline has been stopped."
>> TerminatingError(): "The pipeline has been stopped."
>> $global:?
False
**********************
Windows PowerShell transcript end
End time: 20170802190213
**********************
