**********************
Windows PowerShell transcript start
Start time: 20170807105044
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Machine: DONKEY (Microsoft Windows NT 10.0.15063.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\TrainModel.ps1'
Process ID: 8012
PSVersion: 5.1.15063.483
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.15063.483
BuildVersion: 10.0.15063.483
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\2017-08-07_res_net_5_48x48_no_fixed_canvas_Adadelta_mb32_homus_rebelo1_rebelo2_printed_audiveris_muscima_pp_fornes_openomr.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\data\images
15200/15200
Extracting Rebelo Symbol Dataset 1...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset1
Extracting Rebelo Symbol Dataset 2...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset2
Extracting Printed Music Symbol dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\PrintedMusicSymbolsDataset
Extracting Fornes Music Symbol dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Fornes-Music-Symbols
Extracting Audiveris OMR Dataset...
Extracting Symbols from Audiveris OMR Dataset...
Extracting MUSCIMA++ Dataset...
Extracting Symbols from Muscima++ Dataset...
Loading crop-objects from xml-files 140/140
Loaded 91255 crop-objects
Filtering 135 broken symbols
Filtering 7977 symbols from 7 ignored classes
Generating images from crop-object masks 37076/37076
Processing compound objects ...
Generating images from crop-object masks 18382/18382
Extracting OpenOMR dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\OpenOmrDataset
Resizing all images with the LANCZOS interpolation to 48x48px (width x height).
Resizing images 91054 of 91054Deleting split directories...
Splitting data into training, validation and test sets...
Copying 2 training files of 1-8-Time...
Copying 0 validation files of 1-8-Time...
Copying 0 test files of 1-8-Time...
Copying 323 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 319 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 353 training files of 2-4-Time...
Copying 44 validation files of 2-4-Time...
Copying 44 test files of 2-4-Time...
Copying 10 training files of 2-8-Time...
Copying 1 validation files of 2-8-Time...
Copying 1 test files of 2-8-Time...
Copying 387 training files of 3-4-Time...
Copying 48 validation files of 3-4-Time...
Copying 48 test files of 3-4-Time...
Copying 344 training files of 3-8-Time...
Copying 42 validation files of 3-8-Time...
Copying 42 test files of 3-8-Time...
Copying 6 training files of 4-2-Time...
Copying 0 validation files of 4-2-Time...
Copying 0 test files of 4-2-Time...
Copying 342 training files of 4-4-Time...
Copying 42 validation files of 4-4-Time...
Copying 42 test files of 4-4-Time...
Copying 1 training files of 4-8-Time...
Copying 0 validation files of 4-8-Time...
Copying 0 test files of 4-8-Time...
Copying 12 training files of 5-4-Time...
Copying 1 validation files of 5-4-Time...
Copying 1 test files of 5-4-Time...
Copying 11 training files of 5-8-Time...
Copying 1 validation files of 5-8-Time...
Copying 1 test files of 5-8-Time...
Copying 4 training files of 6-4-Time...
Copying 0 validation files of 6-4-Time...
Copying 0 test files of 6-4-Time...
Copying 337 training files of 6-8-Time...
Copying 42 validation files of 6-8-Time...
Copying 42 test files of 6-8-Time...
Copying 9 training files of 7-4-Time...
Copying 0 validation files of 7-4-Time...
Copying 0 test files of 7-4-Time...
Copying 6 training files of 8-8-Time...
Copying 0 validation files of 8-8-Time...
Copying 0 test files of 8-8-Time...
Copying 326 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 497 training files of Accent...
Copying 62 validation files of Accent...
Copying 62 test files of Accent...
Copying 17 training files of Arpeggio...
Copying 2 validation files of Arpeggio...
Copying 2 test files of Arpeggio...
Copying 5438 training files of Barline...
Copying 679 validation files of Barline...
Copying 679 test files of Barline...
Copying 6537 training files of Beam...
Copying 816 validation files of Beam...
Copying 816 test files of Beam...
Copying 289 training files of Brace...
Copying 35 validation files of Brace...
Copying 35 test files of Brace...
Copying 11 training files of Breath-Mark...
Copying 1 validation files of Breath-Mark...
Copying 1 test files of Breath-Mark...
Copying 22 training files of Breve...
Copying 2 validation files of Breve...
Copying 2 test files of Breve...
Copying 1353 training files of C-Clef...
Copying 168 validation files of C-Clef...
Copying 168 test files of C-Clef...
Copying 96 training files of Chord...
Copying 11 validation files of Chord...
Copying 11 test files of Chord...
Copying 2 training files of Coda...
Copying 0 validation files of Coda...
Copying 0 test files of Coda...
Copying 2 training files of Coda-Square...
Copying 0 validation files of Coda-Square...
Copying 0 test files of Coda-Square...
Copying 550 training files of Common-Time...
Copying 68 validation files of Common-Time...
Copying 68 test files of Common-Time...
Copying 510 training files of Cut-Time...
Copying 63 validation files of Cut-Time...
Copying 63 test files of Cut-Time...
Copying 3816 training files of Dot...
Copying 477 validation files of Dot...
Copying 477 test files of Dot...
Copying 8 training files of Dotted-Horizontal-Spanner...
Copying 0 validation files of Dotted-Horizontal-Spanner...
Copying 0 test files of Dotted-Horizontal-Spanner...
Copying 1 training files of Double-Flat...
Copying 0 validation files of Double-Flat...
Copying 0 test files of Double-Flat...
Copying 751 training files of Double-Sharp...
Copying 93 validation files of Double-Sharp...
Copying 93 test files of Double-Sharp...
Copying 120 training files of Double-Whole-Rest...
Copying 15 validation files of Double-Whole-Rest...
Copying 15 test files of Double-Whole-Rest...
Copying 14 training files of Eighth-Grace-Note...
Copying 1 validation files of Eighth-Grace-Note...
Copying 1 test files of Eighth-Grace-Note...
Copying 2511 training files of Eighth-Note...
Copying 313 validation files of Eighth-Note...
Copying 313 test files of Eighth-Note...
Copying 1805 training files of Eighth-Rest...
Copying 225 validation files of Eighth-Rest...
Copying 225 test files of Eighth-Rest...
Copying 1261 training files of F-Clef...
Copying 157 validation files of F-Clef...
Copying 157 test files of F-Clef...
Copying 111 training files of Fermata...
Copying 13 validation files of Fermata...
Copying 13 test files of Fermata...
Copying 2496 training files of Flat...
Copying 311 validation files of Flat...
Copying 311 test files of Flat...
Copying 1984 training files of G-Clef...
Copying 247 validation files of G-Clef...
Copying 247 test files of G-Clef...
Copying 97 training files of Glissando...
Copying 12 validation files of Glissando...
Copying 12 test files of Glissando...
Copying 179 training files of Hairpin-Crescendo...
Copying 22 validation files of Hairpin-Crescendo...
Copying 22 test files of Hairpin-Crescendo...
Copying 213 training files of Hairpin-Decrescendo...
Copying 26 validation files of Hairpin-Decrescendo...
Copying 26 test files of Hairpin-Decrescendo...
Copying 1192 training files of Half-Note...
Copying 148 validation files of Half-Note...
Copying 148 test files of Half-Note...
Copying 10 training files of Horizontal-Spanner...
Copying 1 validation files of Horizontal-Spanner...
Copying 1 test files of Horizontal-Spanner...
Copying 64 training files of Marcato...
Copying 7 validation files of Marcato...
Copying 7 test files of Marcato...
Copying 70 training files of Mordent...
Copying 8 validation files of Mordent...
Copying 8 test files of Mordent...
Copying 78 training files of Multiple-Eighth-Notes...
Copying 9 validation files of Multiple-Eighth-Notes...
Copying 9 test files of Multiple-Eighth-Notes...
Copying 27 training files of Multiple-Half-Notes...
Copying 3 validation files of Multiple-Half-Notes...
Copying 3 test files of Multiple-Half-Notes...
Copying 195 training files of Multiple-Quarter-Notes...
Copying 24 validation files of Multiple-Quarter-Notes...
Copying 24 test files of Multiple-Quarter-Notes...
Copying 69 training files of Multiple-Sixteenth-Notes...
Copying 8 validation files of Multiple-Sixteenth-Notes...
Copying 8 test files of Multiple-Sixteenth-Notes...
Copying 2511 training files of Natural...
Copying 313 validation files of Natural...
Copying 313 test files of Natural...
Copying 3 training files of Onehundred-Twenty-Eight-Note...
Copying 0 validation files of Onehundred-Twenty-Eight-Note...
Copying 0 test files of Onehundred-Twenty-Eight-Note...
Copying 3 training files of Onehundred-Twenty-Eight-Rest...
Copying 0 validation files of Onehundred-Twenty-Eight-Rest...
Copying 0 test files of Onehundred-Twenty-Eight-Rest...
Copying 5313 training files of Other...
Copying 664 validation files of Other...
Copying 664 test files of Other...
Copying 14356 training files of Quarter-Note...
Copying 1794 validation files of Quarter-Note...
Copying 1794 test files of Quarter-Note...
Copying 1417 training files of Quarter-Rest...
Copying 177 validation files of Quarter-Rest...
Copying 177 test files of Quarter-Rest...
Copying 14 training files of Repeat-Measure...
Copying 1 validation files of Repeat-Measure...
Copying 1 test files of Repeat-Measure...
Copying 2 training files of Segno...
Copying 0 validation files of Segno...
Copying 0 test files of Segno...
Copying 3424 training files of Sharp...
Copying 428 validation files of Sharp...
Copying 428 test files of Sharp...
Copying 1001 training files of Sixteenth-Note...
Copying 124 validation files of Sixteenth-Note...
Copying 124 test files of Sixteenth-Note...
Copying 936 training files of Sixteenth-Rest...
Copying 117 validation files of Sixteenth-Rest...
Copying 117 test files of Sixteenth-Rest...
Copying 706 training files of Sixty-Four-Note...
Copying 88 validation files of Sixty-Four-Note...
Copying 88 test files of Sixty-Four-Note...
Copying 433 training files of Sixty-Four-Rest...
Copying 54 validation files of Sixty-Four-Rest...
Copying 54 test files of Sixty-Four-Rest...
Copying 97 training files of Staccatissimo...
Copying 11 validation files of Staccatissimo...
Copying 11 test files of Staccatissimo...
Copying 69 training files of Stopped...
Copying 8 validation files of Stopped...
Copying 8 test files of Stopped...
Copying 193 training files of Tenuto...
Copying 24 validation files of Tenuto...
Copying 24 test files of Tenuto...
Copying 762 training files of Thirty-Two-Note...
Copying 95 validation files of Thirty-Two-Note...
Copying 95 test files of Thirty-Two-Note...
Copying 508 training files of Thirty-Two-Rest...
Copying 63 validation files of Thirty-Two-Rest...
Copying 63 test files of Thirty-Two-Rest...
Copying 3018 training files of Tie-Slur...
Copying 377 validation files of Tie-Slur...
Copying 377 test files of Tie-Slur...
Copying 145 training files of Trill...
Copying 17 validation files of Trill...
Copying 17 test files of Trill...
Copying 19 training files of Trill-Wobble...
Copying 2 validation files of Trill-Wobble...
Copying 2 test files of Trill-Wobble...
Copying 59 training files of Tuplet...
Copying 7 validation files of Tuplet...
Copying 7 test files of Tuplet...
Copying 65 training files of Turn...
Copying 8 validation files of Turn...
Copying 8 test files of Turn...
Copying 10 training files of Volta...
Copying 1 validation files of Volta...
Copying 1 test files of Volta...
Copying 783 training files of Whole-Half-Rest...
Copying 97 validation files of Whole-Half-Rest...
Copying 97 test files of Whole-Half-Rest...
Copying 1877 training files of Whole-Note...
Copying 234 validation files of Whole-Note...
Copying 234 test files of Whole-Note...
Loading configuration and data-readers...
Found 72912 images belonging to 79 classes.
Found 9071 images belonging to 79 classes.
Found 9071 images belonging to 79 classes.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 48, 48, 3)     0
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 48, 48, 32)    896         input_1[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 48, 48, 32)    128         conv2d_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 48, 48, 32)    0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 48, 48, 32)    128         activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 48, 48, 32)    0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 48, 48, 32)    9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 48, 48, 32)    128         conv2d_2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 48, 48, 32)    0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 48, 48, 32)    9248        activation_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 48, 48, 32)    0           conv2d_3[0][0]
                                                                   activation_1[0][0]
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 24, 24, 32)    0           add_1[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 24, 24, 32)    128         max_pooling2d_1[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 24, 24, 32)    0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 24, 24, 64)    18496       activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 24, 24, 64)    256         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 24, 24, 64)    0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 24, 24, 64)    36928       activation_5[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 24, 24, 64)    18496       max_pooling2d_1[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 24, 24, 64)    0           conv2d_5[0][0]
                                                                   conv2d_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 24, 24, 64)    256         add_2[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 24, 24, 64)    0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 24, 24, 64)    36928       activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 24, 24, 64)    256         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 24, 24, 64)    0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 24, 24, 64)    36928       activation_7[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 24, 24, 64)    0           conv2d_8[0][0]
                                                                   add_2[0][0]
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 64)    0           add_3[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 12, 12, 64)    256         max_pooling2d_2[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 12, 12, 64)    0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 12, 12, 128)   73856       activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 12, 12, 128)   512         conv2d_9[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 12, 12, 128)   0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 12, 12, 128)   147584      activation_9[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 12, 12, 128)   73856       max_pooling2d_2[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 12, 12, 128)   0           conv2d_10[0][0]
                                                                   conv2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 12, 12, 128)   512         add_4[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 12, 12, 128)   0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 12, 12, 128)   147584      activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 12, 12, 128)   512         conv2d_12[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 12, 12, 128)   0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 12, 12, 128)   147584      activation_11[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 12, 12, 128)   0           conv2d_13[0][0]
                                                                   add_4[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 12, 12, 128)   512         add_5[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 12, 12, 128)   0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 12, 12, 128)   147584      activation_12[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 12, 12, 128)   512         conv2d_14[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 12, 12, 128)   0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 12, 12, 128)   147584      activation_13[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 12, 12, 128)   0           conv2d_15[0][0]
                                                                   add_5[0][0]
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 6, 6, 128)     0           add_6[0][0]
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 6, 6, 128)     512         max_pooling2d_3[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 6, 6, 128)     0           batch_normalization_14[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 6, 6, 256)     295168      activation_14[0][0]
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 6, 6, 256)     1024        conv2d_16[0][0]
____________________________________________________________________________________________________
activation_15 (Activation)       (None, 6, 6, 256)     0           batch_normalization_15[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 6, 6, 256)     590080      activation_15[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 6, 6, 256)     295168      max_pooling2d_3[0][0]
____________________________________________________________________________________________________
add_7 (Add)                      (None, 6, 6, 256)     0           conv2d_17[0][0]
                                                                   conv2d_18[0][0]
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 6, 6, 256)     1024        add_7[0][0]
____________________________________________________________________________________________________
activation_16 (Activation)       (None, 6, 6, 256)     0           batch_normalization_16[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 6, 6, 256)     590080      activation_16[0][0]
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 6, 6, 256)     1024        conv2d_19[0][0]
____________________________________________________________________________________________________
activation_17 (Activation)       (None, 6, 6, 256)     0           batch_normalization_17[0][0]
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 6, 6, 256)     590080      activation_17[0][0]
____________________________________________________________________________________________________
add_8 (Add)                      (None, 6, 6, 256)     0           conv2d_20[0][0]
                                                                   add_7[0][0]
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 6, 6, 256)     1024        add_8[0][0]
____________________________________________________________________________________________________
activation_18 (Activation)       (None, 6, 6, 256)     0           batch_normalization_18[0][0]
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 6, 6, 256)     590080      activation_18[0][0]
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 6, 6, 256)     1024        conv2d_21[0][0]
____________________________________________________________________________________________________
activation_19 (Activation)       (None, 6, 6, 256)     0           batch_normalization_19[0][0]
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 6, 6, 256)     590080      activation_19[0][0]
____________________________________________________________________________________________________
add_9 (Add)                      (None, 6, 6, 256)     0           conv2d_22[0][0]
                                                                   add_8[0][0]
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 3, 3, 256)     0           add_9[0][0]
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 3, 3, 256)     1024        max_pooling2d_4[0][0]
____________________________________________________________________________________________________
activation_20 (Activation)       (None, 3, 3, 256)     0           batch_normalization_20[0][0]
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 3, 3, 512)     1180160     activation_20[0][0]
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 3, 3, 512)     2048        conv2d_23[0][0]
____________________________________________________________________________________________________
activation_21 (Activation)       (None, 3, 3, 512)     0           batch_normalization_21[0][0]
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 3, 3, 512)     2359808     activation_21[0][0]
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 3, 3, 512)     1180160     max_pooling2d_4[0][0]
____________________________________________________________________________________________________
add_10 (Add)                     (None, 3, 3, 512)     0           conv2d_24[0][0]
                                                                   conv2d_25[0][0]
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 3, 3, 512)     2048        add_10[0][0]
____________________________________________________________________________________________________
activation_22 (Activation)       (None, 3, 3, 512)     0           batch_normalization_22[0][0]
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 3, 3, 512)     2359808     activation_22[0][0]
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 3, 3, 512)     2048        conv2d_26[0][0]
____________________________________________________________________________________________________
activation_23 (Activation)       (None, 3, 3, 512)     0           batch_normalization_23[0][0]
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 3, 3, 512)     2359808     activation_23[0][0]
____________________________________________________________________________________________________
add_11 (Add)                     (None, 3, 3, 512)     0           conv2d_27[0][0]
                                                                   add_10[0][0]
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 3, 3, 512)     2048        add_11[0][0]
____________________________________________________________________________________________________
activation_24 (Activation)       (None, 3, 3, 512)     0           batch_normalization_24[0][0]
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 3, 3, 512)     2359808     activation_24[0][0]
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 3, 3, 512)     2048        conv2d_28[0][0]
____________________________________________________________________________________________________
activation_25 (Activation)       (None, 3, 3, 512)     0           batch_normalization_25[0][0]
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 3, 3, 512)     2359808     activation_25[0][0]
____________________________________________________________________________________________________
add_12 (Add)                     (None, 3, 3, 512)     0           conv2d_29[0][0]
                                                                   add_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           add_12[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]
____________________________________________________________________________________________________
output_class (Dense)             (None, 79)            40527       flatten_1[0][0]
====================================================================================================
Total params: 18,814,415
Trainable params: 18,803,919
Non-trainable params: 10,496
____________________________________________________________________________________________________
Model res_net_5 loaded.
2017-08-07 11:03:11.268641: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are availab
le on your machine and could speed up CPU computations.
2017-08-07 11:03:11.269158: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are availa
ble on your machine and could speed up CPU computations.
2017-08-07 11:03:11.269525: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are availa
ble on your machine and could speed up CPU computations.
2017-08-07 11:03:11.269960: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are avai
lable on your machine and could speed up CPU computations.
2017-08-07 11:03:11.275306: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are avai
lable on your machine and could speed up CPU computations.
2017-08-07 11:03:11.278032: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are availab
le on your machine and could speed up CPU computations.
2017-08-07 11:03:11.279209: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are availa
ble on your machine and could speed up CPU computations.
2017-08-07 11:03:11.279539: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are availab
le on your machine and could speed up CPU computations.
2017-08-07 11:03:11.679145: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-08-07 11:03:11.679275: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0
2017-08-07 11:03:11.680871: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y
2017-08-07 11:03:11.681811: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pc
i bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 32, Early stopping after 20 epochs without improvement
Data-Shape: (48, 48, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-08}
Performing object localization: False
Training on dataset...
Epoch 1/200
   3/2279 [..............................] - ETA: 5650s - loss: 11.0417 - acc: 0.13542017-08-07 11:03:32.069659: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\pool_allocator.
cc:247] PoolAllocator: After 2421 get requests, put_count=2234 evicted_count=1000 eviction_rate=0.447628 and unsatisfied allocation rate=0.531599
  50/2279 [..............................] - ETA: 511s - loss: 13.3489 - acc: 0.19692017-08-07 11:03:36.113734: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\36\tensorflow\core\common_runtime\gpu\pool_allocator.c
c:247] PoolAllocator: After 4784 get requests, put_count=4727 evicted_count=1000 eviction_rate=0.211551 and unsatisfied allocation rate=0.225753
2279/2279 [==============================] - 211s - loss: 5.3590 - acc: 0.5663 - val_loss: 0.8106 - val_acc: 0.8818 -inf to 0.88182, saving model to 2017-08-07_res_net_5.h5_ from 256 to 281
Epoch 2/200
2279/2279 [==============================] - 203s - loss: 0.8328 - acc: 0.8765 - val_loss: 0.7335 - val_acc: 0.9024 0.88182 to 0.90244, saving model to 2017-08-07_res_net_5.h5
Epoch 3/200
2279/2279 [==============================] - 203s - loss: 0.6810 - acc: 0.9036 - val_loss: 0.5213 - val_acc: 0.9320 0.90244 to 0.93198, saving model to 2017-08-07_res_net_5.h5
Epoch 4/200
2279/2279 [==============================] - 202s - loss: 0.6006 - acc: 0.9185 - val_loss: 0.4940 - val_acc: 0.9261ve
Epoch 5/200
2279/2279 [==============================] - 203s - loss: 0.4959 - acc: 0.9301 - val_loss: 0.3969 - val_acc: 0.9498 0.93198 to 0.94984, saving model to 2017-08-07_res_net_5.h5
Epoch 6/200
2279/2279 [==============================] - 203s - loss: 0.4944 - acc: 0.9318 - val_loss: 0.3826 - val_acc: 0.9530 0.94984 to 0.95304, saving model to 2017-08-07_res_net_5.h5
Epoch 7/200
2279/2279 [==============================] - 203s - loss: 0.4328 - acc: 0.9401 - val_loss: 0.4385 - val_acc: 0.9470ve
Epoch 8/200
2279/2279 [==============================] - 203s - loss: 0.3975 - acc: 0.9455 - val_loss: 0.3844 - val_acc: 0.9485ve
Epoch 9/200
2279/2279 [==============================] - 203s - loss: 0.3727 - acc: 0.9476 - val_loss: 0.3392 - val_acc: 0.9557 0.95304 to 0.95568, saving model to 2017-08-07_res_net_5.h5
Epoch 10/200
2279/2279 [==============================] - 203s - loss: 0.3511 - acc: 0.9498 - val_loss: 0.5026 - val_acc: 0.9054ve
Epoch 11/200
2279/2279 [==============================] - 203s - loss: 0.3290 - acc: 0.9536 - val_loss: 0.3395 - val_acc: 0.9483ve
Epoch 12/200
2279/2279 [==============================] - 203s - loss: 0.3152 - acc: 0.9550 - val_loss: 0.3295 - val_acc: 0.9507ve
Epoch 13/200
2279/2279 [==============================] - 203s - loss: 0.2998 - acc: 0.9582 - val_loss: 0.3085 - val_acc: 0.9569 0.95568 to 0.95690, saving model to 2017-08-07_res_net_5.h5
Epoch 14/200
2279/2279 [==============================] - 203s - loss: 0.2884 - acc: 0.9590 - val_loss: 0.3013 - val_acc: 0.9578 0.95690 to 0.95778, saving model to 2017-08-07_res_net_5.h5
Epoch 15/200
2279/2279 [==============================] - 203s - loss: 0.2808 - acc: 0.9604 - val_loss: 0.2740 - val_acc: 0.9635 0.95778 to 0.96351, saving model to 2017-08-07_res_net_5.h5
Epoch 16/200
2279/2279 [==============================] - 203s - loss: 0.2698 - acc: 0.9636 - val_loss: 0.2738 - val_acc: 0.9614ve
Epoch 17/200
2279/2279 [==============================] - 203s - loss: 0.2637 - acc: 0.9632 - val_loss: 0.2958 - val_acc: 0.9574ve
Epoch 18/200
2279/2279 [==============================] - 203s - loss: 0.2592 - acc: 0.9646 - val_loss: 0.2670 - val_acc: 0.9659 0.96351 to 0.96594, saving model to 2017-08-07_res_net_5.h5
Epoch 19/200
2279/2279 [==============================] - 203s - loss: 0.2498 - acc: 0.9673 - val_loss: 0.2673 - val_acc: 0.9642ve
Epoch 20/200
2279/2279 [==============================] - 203s - loss: 0.2478 - acc: 0.9667 - val_loss: 0.3289 - val_acc: 0.9479ve
Epoch 21/200
2279/2279 [==============================] - 203s - loss: 0.2432 - acc: 0.9667 - val_loss: 0.2696 - val_acc: 0.9605ve
Epoch 22/200
2279/2279 [==============================] - 203s - loss: 0.2390 - acc: 0.9682 - val_loss: 0.2462 - val_acc: 0.9685 0.96594 to 0.96847, saving model to 2017-08-07_res_net_5.h5
Epoch 23/200
2279/2279 [==============================] - 203s - loss: 0.2365 - acc: 0.9690 - val_loss: 0.2547 - val_acc: 0.9683ve
Epoch 24/200
2279/2279 [==============================] - 203s - loss: 0.2326 - acc: 0.9691 - val_loss: 0.2694 - val_acc: 0.9627ve
Epoch 25/200
2279/2279 [==============================] - 203s - loss: 0.2310 - acc: 0.9701 - val_loss: 0.2753 - val_acc: 0.9578ve
Epoch 26/200
2279/2279 [==============================] - 203s - loss: 0.2300 - acc: 0.9704 - val_loss: 0.2549 - val_acc: 0.9668ve
Epoch 27/200
2279/2279 [==============================] - 203s - loss: 0.2265 - acc: 0.9705 - val_loss: 0.2527 - val_acc: 0.9666ve
Epoch 28/200
2279/2279 [==============================] - 203s - loss: 0.2243 - acc: 0.9714 - val_loss: 0.2521 - val_acc: 0.9671ve
Epoch 29/200
2279/2279 [==============================] - 203s - loss: 0.2221 - acc: 0.9724 - val_loss: 0.2588 - val_acc: 0.9654ve
Epoch 30/200
2279/2279 [==============================] - 199s - loss: 0.2220 - acc: 0.9726 - val_loss: 0.2410 - val_acc: 0.9687 0.96847 to 0.96869, saving model to 2017-08-07_res_net_5.h5
Epoch 31/200
2279/2279 [==============================] - 197s - loss: 0.2209 - acc: 0.9729 - val_loss: 0.2481 - val_acc: 0.9676ve
Epoch 32/200
2279/2279 [==============================] - 197s - loss: 0.2184 - acc: 0.9736 - val_loss: 0.2440 - val_acc: 0.9692 0.96869 to 0.96924, saving model to 2017-08-07_res_net_5.h5
Epoch 33/200
2279/2279 [==============================] - 197s - loss: 0.2186 - acc: 0.9732 - val_loss: 0.2436 - val_acc: 0.9685ve
Epoch 34/200
2279/2279 [==============================] - 197s - loss: 0.2205 - acc: 0.9732 - val_loss: 0.2775 - val_acc: 0.9615ve
Epoch 35/200
2279/2279 [==============================] - 197s - loss: 0.2167 - acc: 0.9743 - val_loss: 0.2611 - val_acc: 0.9624ve
Epoch 36/200
2279/2279 [==============================] - 197s - loss: 0.2164 - acc: 0.9741 - val_loss: 0.2738 - val_acc: 0.9637ve
Epoch 37/200
2279/2279 [==============================] - 197s - loss: 0.2177 - acc: 0.9739 - val_loss: 0.2584 - val_acc: 0.9670ve
Epoch 38/200
2279/2279 [==============================] - 197s - loss: 0.2158 - acc: 0.9747 - val_loss: 0.4294 - val_acc: 0.9103ve
Epoch 39/200
2279/2279 [==============================] - 197s - loss: 0.2161 - acc: 0.9746 - val_loss: 0.2427 - val_acc: 0.9671ve
Epoch 40/200
2279/2279 [==============================] - 197s - loss: 0.2139 - acc: 0.9754 - val_loss: 0.2477 - val_acc: 0.9718 0.96924 to 0.97178, saving model to 2017-08-07_res_net_5.h5
Epoch 41/200
2279/2279 [==============================] - 197s - loss: 0.2130 - acc: 0.9769 - val_loss: 0.2477 - val_acc: 0.9701ve
Epoch 42/200
2279/2279 [==============================] - 197s - loss: 0.2157 - acc: 0.9756 - val_loss: 0.2486 - val_acc: 0.9710ve
Epoch 43/200
2279/2279 [==============================] - 197s - loss: 0.2140 - acc: 0.9767 - val_loss: 0.2980 - val_acc: 0.9552ve
Epoch 44/200
2279/2279 [==============================] - 197s - loss: 0.2169 - acc: 0.9755 - val_loss: 0.2798 - val_acc: 0.9624ve
Epoch 45/200
2279/2279 [==============================] - 197s - loss: 0.2156 - acc: 0.9771 - val_loss: 0.2653 - val_acc: 0.9691ve
Epoch 46/200
2279/2279 [==============================] - 197s - loss: 0.2151 - acc: 0.9770 - val_loss: 0.2440 - val_acc: 0.9716ve
Epoch 47/200
2279/2279 [==============================] - 197s - loss: 0.2125 - acc: 0.9782 - val_loss: 0.2799 - val_acc: 0.9616ve
Epoch 48/200
2279/2279 [==============================] - 197s - loss: 0.2131 - acc: 0.9773 - val_loss: 0.2345 - val_acc: 0.9737 0.97178 to 0.97365, saving model to 2017-08-07_res_net_5.h5
Epoch 49/200
2279/2279 [==============================] - 197s - loss: 0.2173 - acc: 0.9766 - val_loss: 0.2508 - val_acc: 0.9690ve
Epoch 50/200
2279/2279 [==============================] - 197s - loss: 0.2141 - acc: 0.9772 - val_loss: 0.2690 - val_acc: 0.9664ve
Epoch 51/200
2279/2279 [==============================] - 198s - loss: 0.2158 - acc: 0.9776 - val_loss: 0.2523 - val_acc: 0.9740 0.97365 to 0.97398, saving model to 2017-08-07_res_net_5.h5
Epoch 52/200
2279/2279 [==============================] - 197s - loss: 0.2146 - acc: 0.9779 - val_loss: 0.2507 - val_acc: 0.9712ve
Epoch 53/200
2279/2279 [==============================] - 197s - loss: 0.2135 - acc: 0.9784 - val_loss: 0.2693 - val_acc: 0.9644ve
Epoch 54/200
2279/2279 [==============================] - 197s - loss: 0.2147 - acc: 0.9782 - val_loss: 0.2529 - val_acc: 0.9697ve
Epoch 55/200
2279/2279 [==============================] - 197s - loss: 0.2169 - acc: 0.9782 - val_loss: 0.2636 - val_acc: 0.9694ve
Epoch 56/200
2279/2279 [==============================] - 197s - loss: 0.2163 - acc: 0.9785 - val_loss: 0.2640 - val_acc: 0.9705ve
Epoch 57/200
2279/2279 [==============================] - 197s - loss: 0.2175 - acc: 0.9783 - val_loss: 0.2799 - val_acc: 0.9656ve
Epoch 58/200
2279/2279 [==============================] - 197s - loss: 0.2167 - acc: 0.9786 - val_loss: 0.2548 - val_acc: 0.9721ve
Epoch 59/200
2279/2279 [==============================] - 197s - loss: 0.2176 - acc: 0.9783 - val_loss: 0.2564 - val_acc: 0.9710ve
Epoch 60/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9788Epoch 00059: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.2183 - acc: 0.9788 - val_loss: 0.2512 - val_acc: 0.9739
Epoch 61/200
2279/2279 [==============================] - 197s - loss: 0.1877 - acc: 0.9877 - val_loss: 0.2679 - val_acc: 0.9667ve
Epoch 62/200
2279/2279 [==============================] - 198s - loss: 0.1775 - acc: 0.9891 - val_loss: 0.2365 - val_acc: 0.9721ve
Epoch 63/200
2279/2279 [==============================] - 198s - loss: 0.1731 - acc: 0.9892 - val_loss: 0.2317 - val_acc: 0.9765 0.97398 to 0.97652, saving model to 2017-08-07_res_net_5.h5
Epoch 64/200
2279/2279 [==============================] - 198s - loss: 0.1691 - acc: 0.9893 - val_loss: 0.2253 - val_acc: 0.9774 0.97652 to 0.97740, saving model to 2017-08-07_res_net_5.h5
Epoch 65/200
2279/2279 [==============================] - 197s - loss: 0.1679 - acc: 0.9894 - val_loss: 0.2317 - val_acc: 0.9760ve
Epoch 66/200
2279/2279 [==============================] - 197s - loss: 0.1666 - acc: 0.9891 - val_loss: 0.2251 - val_acc: 0.9770ve
Epoch 67/200
2279/2279 [==============================] - 198s - loss: 0.1629 - acc: 0.9895 - val_loss: 0.2114 - val_acc: 0.9777 0.97740 to 0.97773, saving model to 2017-08-07_res_net_5.h5
Epoch 68/200
2279/2279 [==============================] - 197s - loss: 0.1619 - acc: 0.9892 - val_loss: 0.2291 - val_acc: 0.9746ve
Epoch 69/200
2279/2279 [==============================] - 197s - loss: 0.1620 - acc: 0.9893 - val_loss: 0.2152 - val_acc: 0.9774ve
Epoch 70/200
2279/2279 [==============================] - 197s - loss: 0.1613 - acc: 0.9898 - val_loss: 0.2103 - val_acc: 0.9773ve
Epoch 71/200
2279/2279 [==============================] - 197s - loss: 0.1599 - acc: 0.9892 - val_loss: 0.2309 - val_acc: 0.9714ve
Epoch 72/200
2279/2279 [==============================] - 198s - loss: 0.1605 - acc: 0.9892 - val_loss: 0.2273 - val_acc: 0.9752ve
Epoch 73/200
2279/2279 [==============================] - 198s - loss: 0.1610 - acc: 0.9894 - val_loss: 0.2144 - val_acc: 0.9787 0.97773 to 0.97872, saving model to 2017-08-07_res_net_5.h5
Epoch 74/200
2279/2279 [==============================] - 198s - loss: 0.1604 - acc: 0.9889 - val_loss: 0.2080 - val_acc: 0.9797 0.97872 to 0.97972, saving model to 2017-08-07_res_net_5.h5
Epoch 75/200
2279/2279 [==============================] - 197s - loss: 0.1583 - acc: 0.9896 - val_loss: 0.2259 - val_acc: 0.9739ve
Epoch 76/200
2279/2279 [==============================] - 198s - loss: 0.1589 - acc: 0.9892 - val_loss: 0.2144 - val_acc: 0.9808 0.97972 to 0.98082, saving model to 2017-08-07_res_net_5.h5
Epoch 77/200
2279/2279 [==============================] - 197s - loss: 0.1569 - acc: 0.9898 - val_loss: 0.2192 - val_acc: 0.9768ve
Epoch 78/200
2279/2279 [==============================] - 198s - loss: 0.1575 - acc: 0.9897 - val_loss: 0.2028 - val_acc: 0.9813 0.98082 to 0.98126, saving model to 2017-08-07_res_net_5.h5
Epoch 79/200
2279/2279 [==============================] - 197s - loss: 0.1594 - acc: 0.9889 - val_loss: 0.2401 - val_acc: 0.9687ve
Epoch 80/200
2279/2279 [==============================] - 197s - loss: 0.1563 - acc: 0.9897 - val_loss: 0.2366 - val_acc: 0.9722ve
Epoch 81/200
2279/2279 [==============================] - 197s - loss: 0.1573 - acc: 0.9889 - val_loss: 0.2358 - val_acc: 0.9745ve
Epoch 82/200
2279/2279 [==============================] - 198s - loss: 0.1573 - acc: 0.9889 - val_loss: 0.2155 - val_acc: 0.9766ve
Epoch 83/200
2279/2279 [==============================] - 197s - loss: 0.1565 - acc: 0.9892 - val_loss: 0.2269 - val_acc: 0.9765ve
Epoch 84/200
2279/2279 [==============================] - 197s - loss: 0.1558 - acc: 0.9891 - val_loss: 0.2267 - val_acc: 0.9750ve
Epoch 85/200
2279/2279 [==============================] - 197s - loss: 0.1553 - acc: 0.9894 - val_loss: 0.2229 - val_acc: 0.9762ve
Epoch 86/200
2279/2279 [==============================] - 197s - loss: 0.1564 - acc: 0.9891 - val_loss: 0.2250 - val_acc: 0.9776ve
Epoch 87/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9889Epoch 00086: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.1554 - acc: 0.9889 - val_loss: 0.2283 - val_acc: 0.9766
Epoch 88/200
2279/2279 [==============================] - 197s - loss: 0.1417 - acc: 0.9936 - val_loss: 0.2094 - val_acc: 0.9794ve
Epoch 89/200
2279/2279 [==============================] - 198s - loss: 0.1369 - acc: 0.9938 - val_loss: 0.1968 - val_acc: 0.9814 0.98126 to 0.98137, saving model to 2017-08-07_res_net_5.h5
Epoch 90/200
2279/2279 [==============================] - 197s - loss: 0.1341 - acc: 0.9945 - val_loss: 0.2144 - val_acc: 0.9766ve
Epoch 91/200
2279/2279 [==============================] - 197s - loss: 0.1312 - acc: 0.9944 - val_loss: 0.1969 - val_acc: 0.9800ve
Epoch 92/200
2279/2279 [==============================] - 197s - loss: 0.1297 - acc: 0.9944 - val_loss: 0.1992 - val_acc: 0.9787ve
Epoch 93/200
2279/2279 [==============================] - 197s - loss: 0.1265 - acc: 0.9948 - val_loss: 0.2044 - val_acc: 0.9791ve
Epoch 94/200
2279/2279 [==============================] - 197s - loss: 0.1255 - acc: 0.9950 - val_loss: 0.1972 - val_acc: 0.9804ve
Epoch 95/200
2279/2279 [==============================] - 198s - loss: 0.1246 - acc: 0.9949 - val_loss: 0.2014 - val_acc: 0.9797ve
Epoch 96/200
2279/2279 [==============================] - 197s - loss: 0.1233 - acc: 0.9945 - val_loss: 0.2038 - val_acc: 0.9785ve
Epoch 97/200
2279/2279 [==============================] - 198s - loss: 0.1226 - acc: 0.9945 - val_loss: 0.1909 - val_acc: 0.9816 0.98137 to 0.98159, saving model to 2017-08-07_res_net_5.h5
Epoch 98/200
2279/2279 [==============================] - 197s - loss: 0.1220 - acc: 0.9946 - val_loss: 0.2009 - val_acc: 0.9787ve
Epoch 99/200
2279/2279 [==============================] - 197s - loss: 0.1201 - acc: 0.9949 - val_loss: 0.1972 - val_acc: 0.9782ve
Epoch 100/200
2279/2279 [==============================] - 197s - loss: 0.1199 - acc: 0.9944 - val_loss: 0.2148 - val_acc: 0.9765ve
Epoch 101/200
2279/2279 [==============================] - 197s - loss: 0.1191 - acc: 0.9941 - val_loss: 0.1997 - val_acc: 0.9766ve
Epoch 102/200
2279/2279 [==============================] - 197s - loss: 0.1166 - acc: 0.9947 - val_loss: 0.2030 - val_acc: 0.9782ve
Epoch 103/200
2279/2279 [==============================] - 197s - loss: 0.1165 - acc: 0.9946 - val_loss: 0.2080 - val_acc: 0.9746ve
Epoch 104/200
2279/2279 [==============================] - 197s - loss: 0.1155 - acc: 0.9945 - val_loss: 0.1921 - val_acc: 0.9798ve
Epoch 105/200
2279/2279 [==============================] - 197s - loss: 0.1150 - acc: 0.9949 - val_loss: 0.2014 - val_acc: 0.9753ve
Epoch 106/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9949Epoch 00105: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.1145 - acc: 0.9949 - val_loss: 0.2056 - val_acc: 0.9768
Epoch 107/200
2279/2279 [==============================] - 197s - loss: 0.1078 - acc: 0.9963 - val_loss: 0.1859 - val_acc: 0.9803ve
Epoch 108/200
2279/2279 [==============================] - 197s - loss: 0.1057 - acc: 0.9966 - val_loss: 0.1991 - val_acc: 0.9797ve
Epoch 109/200
2279/2279 [==============================] - 198s - loss: 0.1047 - acc: 0.9964 - val_loss: 0.1748 - val_acc: 0.9817 0.98159 to 0.98170, saving model to 2017-08-07_res_net_5.h5
Epoch 110/200
2279/2279 [==============================] - 197s - loss: 0.1026 - acc: 0.9967 - val_loss: 0.1903 - val_acc: 0.9786ve
Epoch 111/200
2279/2279 [==============================] - 197s - loss: 0.1010 - acc: 0.9970 - val_loss: 0.1862 - val_acc: 0.9804ve
Epoch 112/200
2279/2279 [==============================] - 197s - loss: 0.1003 - acc: 0.9968 - val_loss: 0.1879 - val_acc: 0.9804ve
Epoch 113/200
2279/2279 [==============================] - 197s - loss: 0.0982 - acc: 0.9971 - val_loss: 0.1901 - val_acc: 0.9795ve
Epoch 114/200
2279/2279 [==============================] - 197s - loss: 0.0985 - acc: 0.9966 - val_loss: 0.1810 - val_acc: 0.9802ve
Epoch 115/200
2279/2279 [==============================] - 197s - loss: 0.0964 - acc: 0.9969 - val_loss: 0.1834 - val_acc: 0.9799ve
Epoch 116/200
2279/2279 [==============================] - 197s - loss: 0.0950 - acc: 0.9969 - val_loss: 0.1827 - val_acc: 0.9813ve
Epoch 117/200
2279/2279 [==============================] - 197s - loss: 0.0940 - acc: 0.9970 - val_loss: 0.1876 - val_acc: 0.9792ve
Epoch 118/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9969Epoch 00117: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.0933 - acc: 0.9969 - val_loss: 0.1808 - val_acc: 0.9804
Epoch 119/200
2279/2279 [==============================] - 197s - loss: 0.0903 - acc: 0.9977 - val_loss: 0.1736 - val_acc: 0.9811ve
Epoch 120/200
2279/2279 [==============================] - 198s - loss: 0.0891 - acc: 0.9978 - val_loss: 0.1819 - val_acc: 0.9823 0.98170 to 0.98225, saving model to 2017-08-07_res_net_5.h5
Epoch 121/200
2279/2279 [==============================] - 197s - loss: 0.0891 - acc: 0.9976 - val_loss: 0.1793 - val_acc: 0.9815ve
Epoch 122/200
2279/2279 [==============================] - 197s - loss: 0.0875 - acc: 0.9979 - val_loss: 0.1745 - val_acc: 0.9818ve
Epoch 123/200
2279/2279 [==============================] - 197s - loss: 0.0867 - acc: 0.9979 - val_loss: 0.1920 - val_acc: 0.9796ve
Epoch 124/200
2279/2279 [==============================] - 197s - loss: 0.0854 - acc: 0.9982 - val_loss: 0.1736 - val_acc: 0.9817ve
Epoch 125/200
2279/2279 [==============================] - 198s - loss: 0.0855 - acc: 0.9975 - val_loss: 0.1707 - val_acc: 0.9826 0.98225 to 0.98258, saving model to 2017-08-07_res_net_5.h5
Epoch 126/200
2279/2279 [==============================] - 197s - loss: 0.0844 - acc: 0.9978 - val_loss: 0.1781 - val_acc: 0.9816ve
Epoch 127/200
2279/2279 [==============================] - 197s - loss: 0.0838 - acc: 0.9979 - val_loss: 0.1922 - val_acc: 0.9811ve
Epoch 128/200
2279/2279 [==============================] - 198s - loss: 0.0825 - acc: 0.9982 - val_loss: 0.1623 - val_acc: 0.9839 0.98258 to 0.98390, saving model to 2017-08-07_res_net_5.h5
Epoch 129/200
2279/2279 [==============================] - 197s - loss: 0.0812 - acc: 0.9982 - val_loss: 0.1865 - val_acc: 0.9809ve
Epoch 130/200
2279/2279 [==============================] - 197s - loss: 0.0818 - acc: 0.9979 - val_loss: 0.1792 - val_acc: 0.9824ve
Epoch 131/200
2279/2279 [==============================] - 197s - loss: 0.0808 - acc: 0.9978 - val_loss: 0.1730 - val_acc: 0.9817ve
Epoch 132/200
2279/2279 [==============================] - 197s - loss: 0.0803 - acc: 0.9977 - val_loss: 0.1747 - val_acc: 0.9816ve
Epoch 133/200
2279/2279 [==============================] - 197s - loss: 0.0793 - acc: 0.9980 - val_loss: 0.1685 - val_acc: 0.9828ve
Epoch 134/200
2279/2279 [==============================] - 197s - loss: 0.0785 - acc: 0.9980 - val_loss: 0.1788 - val_acc: 0.9820ve
Epoch 135/200
2279/2279 [==============================] - 197s - loss: 0.0777 - acc: 0.9978 - val_loss: 0.1800 - val_acc: 0.9816ve
Epoch 136/200
2279/2279 [==============================] - 197s - loss: 0.0780 - acc: 0.9976 - val_loss: 0.1693 - val_acc: 0.9808ve
Epoch 137/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9980Epoch 00136: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.0762 - acc: 0.9980 - val_loss: 0.1753 - val_acc: 0.9815
Epoch 138/200
2279/2279 [==============================] - 197s - loss: 0.0756 - acc: 0.9981 - val_loss: 0.1735 - val_acc: 0.9826ve
Epoch 139/200
2279/2279 [==============================] - 197s - loss: 0.0748 - acc: 0.9983 - val_loss: 0.1795 - val_acc: 0.9810ve
Epoch 140/200
2279/2279 [==============================] - 198s - loss: 0.0737 - acc: 0.9986 - val_loss: 0.1745 - val_acc: 0.9818ve
Epoch 141/200
2279/2279 [==============================] - 197s - loss: 0.0735 - acc: 0.9983 - val_loss: 0.1724 - val_acc: 0.9820ve
Epoch 142/200
2279/2279 [==============================] - 197s - loss: 0.0731 - acc: 0.9985 - val_loss: 0.1740 - val_acc: 0.9829ve
Epoch 143/200
2279/2279 [==============================] - 197s - loss: 0.0724 - acc: 0.9983 - val_loss: 0.1752 - val_acc: 0.9819ve
Epoch 144/200
2279/2279 [==============================] - 198s - loss: 0.0720 - acc: 0.9985 - val_loss: 0.1759 - val_acc: 0.9808ve
Epoch 145/200
2278/2279 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9985Epoch 00144: val_acc did not improve

2279/2279 [==============================] - 197s - loss: 0.0717 - acc: 0.9985 - val_loss: 0.1732 - val_acc: 0.9827
Epoch 146/200
2279/2279 [==============================] - 197s - loss: 0.0712 - acc: 0.9984 - val_loss: 0.1702 - val_acc: 0.9832ve
Epoch 147/200
2279/2279 [==============================] - 197s - loss: 0.0711 - acc: 0.9986 - val_loss: 0.1748 - val_acc: 0.9824ve
Epoch 148/200
2279/2279 [==============================] - 197s - loss: 0.0698 - acc: 0.9988 - val_loss: 0.1659 - val_acc: 0.9837ve
Epoch 149/200
2279/2279 [==============================] - 198s - loss: 0.0705 - acc: 0.9985 - val_loss: 0.1687 - val_acc: 0.9830ve
Epoch 00148: early stopping
Loading best model from check-point and testing...
C:\Programming\Anaconda3\lib\site-packages\sklearn\metrics\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
                          precision    recall  f1-score   support

               12-8-Time       1.00      1.00      1.00        40
                2-2-Time       1.00      0.97      0.99        39
                2-4-Time       1.00      0.98      0.99        44
                2-8-Time       1.00      1.00      1.00         1
                3-4-Time       0.96      1.00      0.98        48
                3-8-Time       0.95      1.00      0.98        42
                4-4-Time       1.00      1.00      1.00        42
                5-4-Time       1.00      1.00      1.00         1
                5-8-Time       1.00      1.00      1.00         1
                6-8-Time       1.00      0.95      0.98        42
                9-8-Time       1.00      0.97      0.99        40
                  Accent       0.93      1.00      0.96        62
                Arpeggio       1.00      1.00      1.00         2
                 Barline       0.99      0.99      0.99       679
                    Beam       0.97      0.97      0.97       816
                   Brace       0.97      0.97      0.97        35
             Breath-Mark       1.00      1.00      1.00         1
                   Breve       1.00      1.00      1.00         2
                  C-Clef       1.00      1.00      1.00       168
                   Chord       0.85      1.00      0.92        11
             Common-Time       0.97      0.99      0.98        68
                Cut-Time       0.98      0.98      0.98        63
                     Dot       0.99      0.99      0.99       477
            Double-Sharp       1.00      0.99      0.99        93
       Double-Whole-Rest       1.00      0.93      0.97        15
       Eighth-Grace-Note       1.00      1.00      1.00         1
             Eighth-Note       0.99      0.97      0.98       313
             Eighth-Rest       0.97      0.99      0.98       225
                  F-Clef       0.99      1.00      1.00       157
                 Fermata       1.00      1.00      1.00        13
                    Flat       0.98      1.00      0.99       311
                  G-Clef       1.00      1.00      1.00       247
               Glissando       0.91      0.83      0.87        12
       Hairpin-Crescendo       1.00      1.00      1.00        22
     Hairpin-Decrescendo       1.00      0.85      0.92        26
               Half-Note       0.97      0.99      0.98       148
      Horizontal-Spanner       0.50      1.00      0.67         1
                 Marcato       0.88      1.00      0.93         7
                 Mordent       1.00      1.00      1.00         8
   Multiple-Eighth-Notes       1.00      1.00      1.00         9
     Multiple-Half-Notes       1.00      1.00      1.00         3
  Multiple-Quarter-Notes       1.00      0.92      0.96        24
Multiple-Sixteenth-Notes       1.00      1.00      1.00         8
                 Natural       0.99      0.99      0.99       313
                   Other       0.96      0.95      0.96       664
            Quarter-Note       1.00      1.00      1.00      1794
            Quarter-Rest       0.99      0.97      0.98       177
          Repeat-Measure       1.00      1.00      1.00         1
                   Sharp       0.99      0.99      0.99       428
          Sixteenth-Note       0.94      0.97      0.96       124
          Sixteenth-Rest       0.98      0.96      0.97       117
         Sixty-Four-Note       0.95      0.92      0.94        88
         Sixty-Four-Rest       0.96      0.98      0.97        54
           Staccatissimo       0.91      0.91      0.91        11
                 Stopped       1.00      1.00      1.00         8
                  Tenuto       0.71      0.71      0.71        24
         Thirty-Two-Note       0.92      0.93      0.92        95
         Thirty-Two-Rest       0.94      0.98      0.96        63
                Tie-Slur       0.97      0.97      0.97       377
                   Trill       0.94      1.00      0.97        17
            Trill-Wobble       1.00      1.00      1.00         2
                  Tuplet       1.00      1.00      1.00         7
                    Turn       1.00      1.00      1.00         8
                   Volta       0.00      0.00      0.00         1
         Whole-Half-Rest       0.91      0.99      0.95        97
              Whole-Note       0.98      0.95      0.97       234

             avg / total       0.98      0.98      0.98      9071

Misclassified files:
        2-2-Time\45-44_3.png is incorrectly classified as Natural
        2-4-Time\symbol200.png is incorrectly classified as 3-4-Time
        6-8-Time\45-48_3.png is incorrectly classified as 3-8-Time
        6-8-Time\symbol223.png is incorrectly classified as Chord
        9-8-Time\symbol146.png is incorrectly classified as 3-8-Time
        Barline\28-152_3.png is incorrectly classified as Tie-Slur
        Barline\32-152_3.png is incorrectly classified as Quarter-Note
        Barline\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-13_N-16_D-ideal___121.png is incorrectly classified as Beam
        Barline\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___771.png is incorrectly classified as Brace
        Barline\symbol7196.png is incorrectly classified as Sharp
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-04_N-20_D-ideal___668.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-06_N-02_D-ideal___287.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-06_N-02_D-ideal___512.png is incorrectly classified as Dot
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-07_N-05_D-ideal___150.png is incorrectly classified as Whole-Half-Rest
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-08_N-10_D-ideal___400.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-15_N-15_D-ideal___306.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-21_N-05_D-ideal___430.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-21_N-05_D-ideal___444.png is incorrectly classified as Dot
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-21_N-08_D-ideal___518.png is incorrectly classified as Dot
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-22_N-10_D-ideal___453.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-25_N-09_D-ideal___290.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-27_N-02_D-ideal___612.png is incorrectly classified as Whole-Half-Rest
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-08_D-ideal___308.png is incorrectly classified as Whole-Half-Rest
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-09_D-ideal___381.png is incorrectly classified as Glissando
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-29_N-10_D-ideal___606.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___423.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___440.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-37_N-17_D-ideal___409.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-44_N-17_D-ideal___387.png is incorrectly classified as Tie-Slur
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-49_N-03_D-ideal___91.png is incorrectly classified as Other
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-49_N-05_D-ideal___68.png is incorrectly classified as Tenuto
        Beam\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-49_N-11_D-ideal___480.png is incorrectly classified as Whole-Half-Rest
        Beam\symbol1283.png is incorrectly classified as Tenuto
        Beam\symbol559.png is incorrectly classified as Whole-Half-Rest
        Beam\symbol927.png is incorrectly classified as Other
        Brace\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-44_N-17_D-ideal___717.png is incorrectly classified as Barline
        Common-Time\25-44_3.png is incorrectly classified as Other
        Cut-Time\symbol360.png is incorrectly classified as Chord
        Dot\42-146_3.png is incorrectly classified as Tie-Slur
        Dot\63-145_3.png is incorrectly classified as Other
        Dot\92-147_3.png is incorrectly classified as Whole-Half-Rest
        Dot\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-21_N-05_D-ideal___450.png is incorrectly classified as Other
        Dot\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-36_N-10_D-ideal___442.png is incorrectly classified as Other
        Dot\symbol200.png is incorrectly classified as Staccatissimo
        Double-Sharp\92-61_3.png is incorrectly classified as Other
        Double-Whole-Rest\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-48_N-02_D-ideal___159.png is incorrectly classified as Whole-Half-Rest
        Eighth-Note\25-58_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\36-97_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\61-99_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\67-101_3.png is incorrectly classified as Common-Time
        Eighth-Note\7-61_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\85-101_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-15_N-10_D-ideal___136.png is incorrectly classified as Quarter-Note
        Eighth-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-26_N-04_D-ideal___152.png is incorrectly classified as Quarter-Note
        Eighth-Rest\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-47_N-04_D-ideal___545.png is incorrectly classified as Accent
        Eighth-Rest\symbol8030.png is incorrectly classified as Sixteenth-Rest
        Flat\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-31_N-01_D-ideal___46.png is incorrectly classified as Natural
        G-Clef\3-78_3.png is incorrectly classified as Thirty-Two-Rest
        Glissando\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-26_N-19_D-ideal___458.png is incorrectly classified as Beam
        Glissando\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-33_N-19_D-ideal___450.png is incorrectly classified as Beam
        Hairpin-Decrescendo\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-02_N-06_D-ideal___630.png is incorrectly classified as Accent
        Hairpin-Decrescendo\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-21_N-08_D-ideal___535.png is incorrectly classified as Accent
        Hairpin-Decrescendo\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-32_N-20_D-ideal___587.png is incorrectly classified as Accent
        Hairpin-Decrescendo\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-44_N-06_D-ideal___525.png is incorrectly classified as Accent
        Half-Note\33-78_3.png is incorrectly classified as Flat
        Half-Note\64-78_3.png is incorrectly classified as Quarter-Rest
        Multiple-Quarter-Notes\symbol4681.png is incorrectly classified as Quarter-Note
        Multiple-Quarter-Notes\symbol5192.png is incorrectly classified as Quarter-Note
        Natural\65-67_3.png is incorrectly classified as Sixteenth-Rest
        Natural\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-02_N-06_D-ideal___389.png is incorrectly classified as Sharp
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-01_N-14_D-ideal___516.png is incorrectly classified as Whole-Half-Rest
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-01_N-14_D-ideal___524.png is incorrectly classified as Sharp
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-08_N-14_D-ideal___497.png is incorrectly classified as Quarter-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-12_N-04_D-ideal___556.png is incorrectly classified as Whole-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-14_N-05_D-ideal___422.png is incorrectly classified as Eighth-Rest
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-15_N-14_D-ideal___299.png is incorrectly classified as Flat
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-16_N-06_D-ideal___572.png is incorrectly classified as Eighth-Rest
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-17_N-01_D-ideal___731.png is incorrectly classified as Eighth-Rest
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-19_N-04_D-ideal___790.png is incorrectly classified as Natural
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-20_N-03_D-ideal___425.png is incorrectly classified as Eighth-Rest
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-22_N-14_D-ideal___463.png is incorrectly classified as Cut-Time
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-22_N-14_D-ideal___477.png is incorrectly classified as Whole-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-22_N-14_D-ideal___527.png is incorrectly classified as Whole-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-24_N-07_D-ideal___323.png is incorrectly classified as Beam
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-05_D-ideal___777.png is incorrectly classified as Dot
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-31_N-01_D-ideal___272.png is incorrectly classified as Flat
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-31_N-01_D-ideal___639.png is incorrectly classified as Dot
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-32_N-20_D-ideal___181.png is incorrectly classified as Beam
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-36_N-10_D-ideal___178.png is incorrectly classified as Tie-Slur
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-36_N-14_D-ideal___530.png is incorrectly classified as Half-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-36_N-14_D-ideal___544.png is incorrectly classified as Half-Note
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-38_N-07_D-ideal___507.png is incorrectly classified as Trill
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-43_N-14_D-ideal___573.png is incorrectly classified as Tie-Slur
        Other\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-44_N-13_D-ideal___533.png is incorrectly classified as Common-Time
        Other\symbol102902.png is incorrectly classified as Quarter-Note
        Other\symbol103862.png is incorrectly classified as Barline
        Other\symbol110870.png is incorrectly classified as Marcato
        Other\symbol15226.png is incorrectly classified as Half-Note
        Other\symbol2181.png is incorrectly classified as 3-4-Time
        Other\symbol3486.png is incorrectly classified as Sharp
        Other\symbol3490.png is incorrectly classified as Whole-Note
        Other\symbol386.png is incorrectly classified as Whole-Half-Rest
        Other\symbol394.png is incorrectly classified as Quarter-Note
        Other\symbol471.png is incorrectly classified as Beam
        Quarter-Note\6.png is incorrectly classified as Eighth-Note
        Quarter-Note\76-91_3.png is incorrectly classified as Half-Note
        Quarter-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-05_N-19_D-ideal___340.png is incorrectly classified as Whole-Half-Rest
        Quarter-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-11_N-12_D-ideal___56.png is incorrectly classified as Beam
        Quarter-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-39_N-20_D-ideal___603.png is incorrectly classified as Other
        Quarter-Rest\1-101_3.png is incorrectly classified as Barline
        Quarter-Rest\2-101_3.png is incorrectly classified as Flat
        Quarter-Rest\76-94_3.png is incorrectly classified as Other
        Quarter-Rest\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-42_N-08_D-ideal___523.png is incorrectly classified as Eighth-Rest
        Quarter-Rest\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-43_N-15_D-ideal___73.png is incorrectly classified as Barline
        Sharp\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___723.png is incorrectly classified as Other
        Sharp\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___725.png is incorrectly classified as Flat
        Sharp\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-30_N-17_D-ideal___731.png is incorrectly classified as Flat
        Sharp\symbol5863.png is incorrectly classified as Natural
        Sixteenth-Note\2-113_3.png is incorrectly classified as Eighth-Note
        Sixteenth-Note\32-116_3.png is incorrectly classified as Eighth-Note
        Sixteenth-Note\58-110_3.png is incorrectly classified as Thirty-Two-Note
        Sixteenth-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-09_N-17_D-ideal___109.png is incorrectly classified as Eighth-Note
        Sixteenth-Rest\33-118_3.png is incorrectly classified as Eighth-Rest
        Sixteenth-Rest\50-117_3.png is incorrectly classified as F-Clef
        Sixteenth-Rest\76-117_3.png is incorrectly classified as Thirty-Two-Rest
        Sixteenth-Rest\91-117_3.png is incorrectly classified as Thirty-Two-Rest
        Sixteenth-Rest\symbol2735.png is incorrectly classified as Eighth-Rest
        Sixty-Four-Note\34-132_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\75-139_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\78-136_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\86-133_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\86-136_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\98-135_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\symbols50.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Rest\49-143_3.png is incorrectly classified as Thirty-Two-Rest
        Staccatissimo\symbols57.png is incorrectly classified as Dot
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-12_N-04_D-ideal___696.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-19_N-04_D-ideal___607.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-05_D-ideal___592.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-33_N-04_D-ideal___364.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-40_N-04_D-ideal___624.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-47_N-04_D-ideal___575.png is incorrectly classified as Beam
        Tenuto\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-50_N-04_D-ideal___812.png is incorrectly classified as Beam
        Thirty-Two-Note\12-140_3.png is incorrectly classified as Sixty-Four-Rest
        Thirty-Two-Note\36-123_3.png is incorrectly classified as Sixteenth-Note
        Thirty-Two-Note\39-125_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\39-126_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\45-124_3.png is incorrectly classified as Sixteenth-Note
        Thirty-Two-Note\49-122_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\symbols95.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Rest\70-129_3.png is incorrectly classified as Sixty-Four-Rest
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-10_N-18_D-ideal___645.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-11_N-12_D-ideal___538.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-15_N-10_D-ideal___652.png is incorrectly classified as Other
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-17_N-18_D-ideal___431.png is incorrectly classified as Other
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-24_N-18_D-ideal___659.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-29_N-10_D-ideal___503.png is incorrectly classified as Other
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-31_N-01_D-ideal___557.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-32_N-20_D-ideal___583.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-34_N-03_D-ideal___86.png is incorrectly classified as Beam
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-49_N-09_D-ideal___340.png is incorrectly classified as Other
        Tie-Slur\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-49_N-11_D-ideal___575.png is incorrectly classified as Beam
        Tie-Slur\symbol102856.png is incorrectly classified as Beam
        Tie-Slur\symbol113104.png is incorrectly classified as Beam
        Volta\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-25_N-12_D-ideal___608.png is incorrectly classified as Horizontal-Spanner
        Whole-Half-Rest\41-76_3.png is incorrectly classified as Beam
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-05_N-19_D-ideal___471.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-14_N-08_D-ideal___59.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-25_N-20_D-ideal___250.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-27_N-16_D-ideal___69.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-08_D-ideal___255.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-28_N-08_D-ideal___348.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-33_N-19_D-ideal___132.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-37_N-06_D-ideal___255.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-42_N-05_D-ideal___146.png is incorrectly classified as Other
        Whole-Note\MUSCIMA-pp_0.9___CVC-MUSCIMA_W-47_N-12_D-ideal___0.png is incorrectly classified as Other
        Whole-Note\symbol6881.png is incorrectly classified as Other
loss: 0.18441
acc: 0.98049
Total Accuracy: 98.04873%
Total Error: 1.95127%
Execution time: 29689.9s
WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0
Traceback (most recent call last):
  File "C:\Programming\Anaconda3\lib\site-packages\googleapiclient\discovery_cache\__init__.py", line 36, in autodetect
    from google.appengine.api import memcache
ModuleNotFoundError: No module named 'google.appengine'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Programming\Anaconda3\lib\site-packages\googleapiclient\discovery_cache\file_cache.py", line 33, in <module>
    from oauth2client.contrib.locked_file import LockedFile
ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Programming\Anaconda3\lib\site-packages\googleapiclient\discovery_cache\file_cache.py", line 37, in <module>
    from oauth2client.locked_file import LockedFile
ModuleNotFoundError: No module named 'oauth2client.locked_file'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Programming\Anaconda3\lib\site-packages\googleapiclient\discovery_cache\__init__.py", line 41, in autodetect
    from . import file_cache
  File "C:\Programming\Anaconda3\lib\site-packages\googleapiclient\discovery_cache\file_cache.py", line 41, in <module>
    'file_cache is unavailable when using oauth2client >= 4.0.0')
ImportError: file_cache is unavailable when using oauth2client >= 4.0.0
Uploading results to Google Spreadsheet and appending at first empty line 217
**********************
Windows PowerShell transcript end
End time: 20170807191801
**********************
