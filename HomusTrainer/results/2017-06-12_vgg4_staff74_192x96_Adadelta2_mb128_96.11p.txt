**********************
Windows PowerShell transcript start
Start time: 20170612164643
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Machine: DONKEY (Microsoft Windows NT 10.0.14393.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\TrainModel.ps1'
Process ID: 9632
PSVersion: 5.1.14393.1198
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.14393.1198
BuildVersion: 10.0.14393.1198
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\2017-06-12_vgg4_staff74_192x96_Adadelta2_mb128.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([74])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\data\images
15200/15200Deleting split directories...
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 192, 96, 32)       896
_________________________________________________________________
batch_normalization_1 (Batch (None, 192, 96, 32)       128
_________________________________________________________________
activation_1 (Activation)    (None, 192, 96, 32)       0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 192, 96, 32)       9248
_________________________________________________________________
batch_normalization_2 (Batch (None, 192, 96, 32)       128
_________________________________________________________________
activation_2 (Activation)    (None, 192, 96, 32)       0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 96, 48, 32)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 96, 48, 64)        18496
_________________________________________________________________
batch_normalization_3 (Batch (None, 96, 48, 64)        256
_________________________________________________________________
activation_3 (Activation)    (None, 96, 48, 64)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 96, 48, 64)        36928
_________________________________________________________________
batch_normalization_4 (Batch (None, 96, 48, 64)        256
_________________________________________________________________
activation_4 (Activation)    (None, 96, 48, 64)        0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 48, 24, 64)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 48, 24, 128)       73856
_________________________________________________________________
batch_normalization_5 (Batch (None, 48, 24, 128)       512
_________________________________________________________________
activation_5 (Activation)    (None, 48, 24, 128)       0
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 48, 24, 128)       147584
_________________________________________________________________
batch_normalization_6 (Batch (None, 48, 24, 128)       512
_________________________________________________________________
activation_6 (Activation)    (None, 48, 24, 128)       0
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 48, 24, 128)       147584
_________________________________________________________________
batch_normalization_7 (Batch (None, 48, 24, 128)       512
_________________________________________________________________
activation_7 (Activation)    (None, 48, 24, 128)       0
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 24, 12, 128)       0
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 24, 12, 256)       295168
_________________________________________________________________
batch_normalization_8 (Batch (None, 24, 12, 256)       1024
_________________________________________________________________
activation_8 (Activation)    (None, 24, 12, 256)       0
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 24, 12, 256)       590080
_________________________________________________________________
batch_normalization_9 (Batch (None, 24, 12, 256)       1024
_________________________________________________________________
activation_9 (Activation)    (None, 24, 12, 256)       0
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 24, 12, 256)       590080
_________________________________________________________________
batch_normalization_10 (Batc (None, 24, 12, 256)       1024
_________________________________________________________________
activation_10 (Activation)   (None, 24, 12, 256)       0
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 12, 6, 256)        0
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 12, 6, 512)        1180160
_________________________________________________________________
batch_normalization_11 (Batc (None, 12, 6, 512)        2048
_________________________________________________________________
activation_11 (Activation)   (None, 12, 6, 512)        0
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 12, 6, 512)        2359808
_________________________________________________________________
batch_normalization_12 (Batc (None, 12, 6, 512)        2048
_________________________________________________________________
activation_12 (Activation)   (None, 12, 6, 512)        0
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 12, 6, 512)        2359808
_________________________________________________________________
batch_normalization_13 (Batc (None, 12, 6, 512)        2048
_________________________________________________________________
activation_13 (Activation)   (None, 12, 6, 512)        0
_________________________________________________________________
average_pooling2d_1 (Average (None, 6, 3, 512)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                294944
_________________________________________________________________
output_node (Activation)     (None, 32)                0
=================================================================
Total params: 8,116,160
Trainable params: 8,110,400
Non-trainable params: 5,760
_________________________________________________________________
Model vgg4 loaded.
2017-06-12 16:50:52.933020: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.933181: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.933769: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.935001: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.935499: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.936493: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.937121: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:52.938310: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-12 16:50:53.268484: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-12 16:50:53.268619: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0
2017-06-12 16:50:53.269876: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y
2017-06-12 16:50:53.270465: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 128, Early stopping after 20 epochs without improvement
Data-Shape: (192, 96, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'decay': 0.0, 'rho': 0.95, 'epsilon': 1e-08}
Epoch 1/200
10/96 [==>...........................] - ETA: 83s - loss: 6.1825 - acc: 0.07032017-06-12 16:51:10.515718: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3729 get requests, put_count=3726 evicted_count=1000 eviction_rate=0.268384 and unsatisfied allocation rate=0.29579
2017-06-12 16:51:10.515838: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
95/96 [============================>.] - ETA: 0s - loss: 2.7371 - acc: 0.3653Epoch 00000: val_acc improved from -inf to 0.10957, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 50s - loss: 2.7250 - acc: 0.3687 - val_loss: 3.4479 - val_acc: 0.1096
Epoch 2/200
95/96 [============================>.] - ETA: 0s - loss: 1.1505 - acc: 0.7022Epoch 00001: val_acc improved from 0.10957 to 0.18020, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 1.1453 - acc: 0.7032 - val_loss: 4.2750 - val_acc: 0.1802
Epoch 3/200
95/96 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7878Epoch 00002: val_acc improved from 0.18020 to 0.42310, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.8679 - acc: 0.7890 - val_loss: 3.4073 - val_acc: 0.4231
Epoch 4/200
95/96 [============================>.] - ETA: 0s - loss: 0.7347 - acc: 0.8296Epoch 00003: val_acc improved from 0.42310 to 0.43630, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.7376 - acc: 0.8293 - val_loss: 4.7412 - val_acc: 0.4363
Epoch 5/200
95/96 [============================>.] - ETA: 0s - loss: 0.6540 - acc: 0.8545Epoch 00004: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.6518 - acc: 0.8550 - val_loss: 9.3966 - val_acc: 0.1571
Epoch 6/200
95/96 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.8721Epoch 00005: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.6029 - acc: 0.8693 - val_loss: 6.3288 - val_acc: 0.2244
Epoch 7/200
95/96 [============================>.] - ETA: 0s - loss: 0.5905 - acc: 0.8831Epoch 00006: val_acc improved from 0.43630 to 0.85347, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.5905 - acc: 0.8833 - val_loss: 0.6491 - val_acc: 0.8535
Epoch 8/200
95/96 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.8909Epoch 00007: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.5312 - acc: 0.8910 - val_loss: 2.0521 - val_acc: 0.6416
Epoch 9/200
95/96 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.9060Epoch 00008: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.5078 - acc: 0.9059 - val_loss: 4.0814 - val_acc: 0.5116
Epoch 10/200
95/96 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.9198Epoch 00009: val_acc improved from 0.85347 to 0.90429, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.4704 - acc: 0.9206 - val_loss: 0.5304 - val_acc: 0.9043
Epoch 11/200
95/96 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.9262Epoch 00010: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.4434 - acc: 0.9270 - val_loss: 1.7258 - val_acc: 0.7221
Epoch 12/200
95/96 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.9275Epoch 00011: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.4448 - acc: 0.9262 - val_loss: 5.2908 - val_acc: 0.3459
Epoch 13/200
95/96 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.9340Epoch 00012: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.4113 - acc: 0.9327 - val_loss: 5.1505 - val_acc: 0.4528
Epoch 14/200
95/96 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.9398Epoch 00013: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.4039 - acc: 0.9394 - val_loss: 9.4015 - val_acc: 0.2370
Epoch 15/200
95/96 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.9456Epoch 00014: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.3846 - acc: 0.9461 - val_loss: 1.1369 - val_acc: 0.7710
Epoch 16/200
95/96 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.9485Epoch 00015: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.3713 - acc: 0.9491 - val_loss: 0.5245 - val_acc: 0.9023
Epoch 17/200
95/96 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.9544Epoch 00016: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.3545 - acc: 0.9538 - val_loss: 2.7705 - val_acc: 0.5875
Epoch 18/200
95/96 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.9511Epoch 00017: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.3605 - acc: 0.9516 - val_loss: 0.7238 - val_acc: 0.8601
Epoch 19/200
95/96 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9586Epoch 00018: val_acc did not improve

Epoch 00018: reducing learning rate to 0.5.
96/96 [==============================] - 42s - loss: 0.3365 - acc: 0.9590 - val_loss: 0.6710 - val_acc: 0.8878
Epoch 20/200
95/96 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9761Epoch 00019: val_acc improved from 0.90429 to 0.93399, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.2894 - acc: 0.9763 - val_loss: 0.4211 - val_acc: 0.9340
Epoch 21/200
95/96 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9810Epoch 00020: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2738 - acc: 0.9812 - val_loss: 0.4395 - val_acc: 0.9215
Epoch 22/200
95/96 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9810Epoch 00021: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2726 - acc: 0.9802 - val_loss: 1.3052 - val_acc: 0.7861
Epoch 23/200
95/96 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9832Epoch 00022: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2631 - acc: 0.9834 - val_loss: 0.5744 - val_acc: 0.8911
Epoch 24/200
95/96 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9831Epoch 00023: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2588 - acc: 0.9813 - val_loss: 0.6429 - val_acc: 0.8845
Epoch 25/200
95/96 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9843Epoch 00024: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2522 - acc: 0.9845 - val_loss: 0.4713 - val_acc: 0.9182
Epoch 26/200
95/96 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9887Epoch 00025: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2423 - acc: 0.9888 - val_loss: 0.7572 - val_acc: 0.8502
Epoch 27/200
95/96 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9873Epoch 00026: val_acc improved from 0.93399 to 0.95182, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.2411 - acc: 0.9875 - val_loss: 0.3546 - val_acc: 0.9518
Epoch 28/200
95/96 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9873Epoch 00027: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2383 - acc: 0.9874 - val_loss: 0.3749 - val_acc: 0.9426
Epoch 29/200
95/96 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9859Epoch 00028: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2395 - acc: 0.9860 - val_loss: 0.5160 - val_acc: 0.9175
Epoch 30/200
95/96 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9877Epoch 00029: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2348 - acc: 0.9879 - val_loss: 0.3653 - val_acc: 0.9492
Epoch 31/200
95/96 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9897Epoch 00030: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2258 - acc: 0.9888 - val_loss: 0.6585 - val_acc: 0.8871
Epoch 32/200
95/96 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9887Epoch 00031: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2238 - acc: 0.9888 - val_loss: 0.3466 - val_acc: 0.9472
Epoch 33/200
95/96 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9882Epoch 00032: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2270 - acc: 0.9873 - val_loss: 0.5612 - val_acc: 0.8977
Epoch 34/200
95/96 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9916Epoch 00033: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2172 - acc: 0.9907 - val_loss: 0.5565 - val_acc: 0.9175
Epoch 35/200
95/96 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9924Epoch 00034: val_acc improved from 0.95182 to 0.95710, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.2136 - acc: 0.9924 - val_loss: 0.3165 - val_acc: 0.9571
Epoch 36/200
95/96 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9915Epoch 00035: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2149 - acc: 0.9906 - val_loss: 0.6999 - val_acc: 0.8983
Epoch 37/200
95/96 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9914Epoch 00036: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2118 - acc: 0.9904 - val_loss: 0.5766 - val_acc: 0.8937
Epoch 38/200
95/96 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9913Epoch 00037: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2088 - acc: 0.9914 - val_loss: 0.3890 - val_acc: 0.9446
Epoch 39/200
95/96 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9935Epoch 00038: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2007 - acc: 0.9936 - val_loss: 0.3239 - val_acc: 0.9531
Epoch 40/200
95/96 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9903Epoch 00039: val_acc improved from 0.95710 to 0.96106, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.2068 - acc: 0.9904 - val_loss: 0.3069 - val_acc: 0.9611
Epoch 41/200
95/96 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9929Epoch 00040: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1998 - acc: 0.9930 - val_loss: 0.6338 - val_acc: 0.8845
Epoch 42/200
95/96 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9925Epoch 00041: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1999 - acc: 0.9926 - val_loss: 0.6909 - val_acc: 0.8713
Epoch 43/200
95/96 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9910Epoch 00042: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.2005 - acc: 0.9911 - val_loss: 0.3307 - val_acc: 0.9485
Epoch 44/200
95/96 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9932Epoch 00043: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1940 - acc: 0.9932 - val_loss: 0.3081 - val_acc: 0.9545
Epoch 45/200
95/96 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9940Epoch 00044: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1909 - acc: 0.9941 - val_loss: 0.4066 - val_acc: 0.9333
Epoch 46/200
95/96 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9934Epoch 00045: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1885 - acc: 0.9935 - val_loss: 0.3939 - val_acc: 0.9300
Epoch 47/200
95/96 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9929Epoch 00046: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1886 - acc: 0.9930 - val_loss: 0.3135 - val_acc: 0.9545
Epoch 48/200
95/96 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9929Epoch 00047: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1861 - acc: 0.9930 - val_loss: 0.4074 - val_acc: 0.9307
Epoch 49/200
95/96 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9933Epoch 00048: val_acc did not improve

Epoch 00048: reducing learning rate to 0.25.
96/96 [==============================] - 42s - loss: 0.1846 - acc: 0.9924 - val_loss: 1.2479 - val_acc: 0.7789
Epoch 50/200
95/96 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9957Epoch 00049: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1819 - acc: 0.9958 - val_loss: 0.3107 - val_acc: 0.9604
Epoch 51/200
95/96 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9982Epoch 00050: val_acc improved from 0.96106 to 0.96766, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.1698 - acc: 0.9982 - val_loss: 0.2784 - val_acc: 0.9677
Epoch 52/200
95/96 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9977Epoch 00051: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1694 - acc: 0.9977 - val_loss: 0.3083 - val_acc: 0.9624
Epoch 53/200
95/96 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9979Epoch 00052: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1678 - acc: 0.9980 - val_loss: 0.2727 - val_acc: 0.9637
Epoch 54/200
95/96 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9981Epoch 00053: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1662 - acc: 0.9981 - val_loss: 0.2598 - val_acc: 0.9663
Epoch 55/200
95/96 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9979Epoch 00054: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1648 - acc: 0.9980 - val_loss: 0.2851 - val_acc: 0.9630
Epoch 56/200
95/96 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9981Epoch 00055: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1630 - acc: 0.9981 - val_loss: 0.2979 - val_acc: 0.9630
Epoch 57/200
95/96 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9983Epoch 00056: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1617 - acc: 0.9983 - val_loss: 0.2861 - val_acc: 0.9637
Epoch 58/200
95/96 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9984Epoch 00057: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1628 - acc: 0.9974 - val_loss: 0.3044 - val_acc: 0.9604
Epoch 59/200
95/96 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9985Epoch 00058: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1595 - acc: 0.9985 - val_loss: 0.2995 - val_acc: 0.9630
Epoch 60/200
95/96 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9990Epoch 00059: val_acc did not improve

Epoch 00059: reducing learning rate to 0.125.
96/96 [==============================] - 42s - loss: 0.1577 - acc: 0.9990 - val_loss: 0.2883 - val_acc: 0.9644
Epoch 61/200
95/96 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9992Epoch 00060: val_acc improved from 0.96766 to 0.96832, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.1556 - acc: 0.9992 - val_loss: 0.2712 - val_acc: 0.9683
Epoch 62/200
95/96 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9995Epoch 00061: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1552 - acc: 0.9995 - val_loss: 0.2782 - val_acc: 0.9611
Epoch 63/200
95/96 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9994Epoch 00062: val_acc improved from 0.96832 to 0.97162, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.1539 - acc: 0.9994 - val_loss: 0.2534 - val_acc: 0.9716
Epoch 64/200
95/96 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9995Epoch 00063: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1535 - acc: 0.9995 - val_loss: 0.2674 - val_acc: 0.9677
Epoch 65/200
95/96 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9993Epoch 00064: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1531 - acc: 0.9993 - val_loss: 0.2808 - val_acc: 0.9611
Epoch 66/200
95/96 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9992Epoch 00065: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1533 - acc: 0.9992 - val_loss: 0.2500 - val_acc: 0.9683
Epoch 67/200
95/96 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9997Epoch 00066: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1508 - acc: 0.9997 - val_loss: 0.2691 - val_acc: 0.9690
Epoch 68/200
95/96 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9993Epoch 00067: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1515 - acc: 0.9993 - val_loss: 0.2778 - val_acc: 0.9683
Epoch 69/200
95/96 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9990Epoch 00068: val_acc improved from 0.97162 to 0.97756, saving model to 2017-06-12_vgg4.h5
96/96 [==============================] - 42s - loss: 0.1517 - acc: 0.9990 - val_loss: 0.2354 - val_acc: 0.9776
Epoch 70/200
95/96 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9994Epoch 00069: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1504 - acc: 0.9994 - val_loss: 0.2680 - val_acc: 0.9677
Epoch 71/200
95/96 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9993Epoch 00070: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1492 - acc: 0.9993 - val_loss: 0.2629 - val_acc: 0.9657
Epoch 72/200
95/96 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9993Epoch 00071: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1490 - acc: 0.9993 - val_loss: 0.2807 - val_acc: 0.9630
Epoch 73/200
95/96 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9991Epoch 00072: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1487 - acc: 0.9991 - val_loss: 0.2671 - val_acc: 0.9644
Epoch 74/200
95/96 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9994Epoch 00073: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1480 - acc: 0.9994 - val_loss: 0.2657 - val_acc: 0.9663
Epoch 75/200
95/96 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9998Epoch 00074: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1471 - acc: 0.9998 - val_loss: 0.2667 - val_acc: 0.9716
Epoch 76/200
95/96 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9996Epoch 00075: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1464 - acc: 0.9996 - val_loss: 0.2406 - val_acc: 0.9729
Epoch 77/200
95/96 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9994Epoch 00076: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1457 - acc: 0.9994 - val_loss: 0.2780 - val_acc: 0.9670
Epoch 78/200
95/96 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9993Epoch 00077: val_acc did not improve

Epoch 00077: reducing learning rate to 0.0625.
96/96 [==============================] - 42s - loss: 0.1456 - acc: 0.9993 - val_loss: 0.2750 - val_acc: 0.9677
Epoch 79/200
95/96 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9997Epoch 00078: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1448 - acc: 0.9997 - val_loss: 0.2565 - val_acc: 0.9710
Epoch 80/200
95/96 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9993Epoch 00079: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1452 - acc: 0.9993 - val_loss: 0.2562 - val_acc: 0.9690
Epoch 81/200
95/96 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9998Epoch 00080: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1439 - acc: 0.9998 - val_loss: 0.2589 - val_acc: 0.9736
Epoch 82/200
95/96 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9995Epoch 00081: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1435 - acc: 0.9995 - val_loss: 0.2727 - val_acc: 0.9617
Epoch 83/200
95/96 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9998Epoch 00082: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1434 - acc: 0.9998 - val_loss: 0.2378 - val_acc: 0.9716
Epoch 84/200
95/96 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9998Epoch 00083: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1430 - acc: 0.9998 - val_loss: 0.2455 - val_acc: 0.9690
Epoch 85/200
95/96 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9999Epoch 00084: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1423 - acc: 0.9999 - val_loss: 0.2519 - val_acc: 0.9677
Epoch 86/200
95/96 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9995Epoch 00085: val_acc did not improve

Epoch 00085: reducing learning rate to 0.03125.
96/96 [==============================] - 42s - loss: 0.1424 - acc: 0.9995 - val_loss: 0.2673 - val_acc: 0.9663
Epoch 87/200
95/96 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9996Epoch 00086: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1425 - acc: 0.9996 - val_loss: 0.2459 - val_acc: 0.9736
Epoch 88/200
95/96 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9996Epoch 00087: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1420 - acc: 0.9996 - val_loss: 0.2469 - val_acc: 0.9723
Epoch 89/200
95/96 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9998Epoch 00088: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1416 - acc: 0.9998 - val_loss: 0.2477 - val_acc: 0.9729
Epoch 90/200
95/96 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9997Epoch 00089: val_acc did not improve
96/96 [==============================] - 42s - loss: 0.1416 - acc: 0.9997 - val_loss: 0.2464 - val_acc: 0.9723
Epoch 00089: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

       6-8-Time       1.00      1.00      1.00        40
      12-8-Time       0.97      1.00      0.99        39
    Eighth-Note       0.95      0.97      0.96        40
       2-2-Time       1.00      0.95      0.97        40
         G-Clef       1.00      1.00      1.00        40
         C-Clef       1.00      1.00      1.00        40
Sixty-Four-Note       0.98      1.00      0.99        40
       4-4-Time       1.00      0.97      0.99        40
       Cut-Time       1.00      0.97      0.99        40
Thirty-Two-Rest       1.00      1.00      1.00        40
 Sixteenth-Note       1.00      1.00      1.00        40
       2-4-Time       1.00      0.97      0.99        40
           Flat       0.95      0.97      0.96        40
Whole-Half-Rest       0.97      0.97      0.97        40
      Half-Note       0.95      0.94      0.94        80
 Sixteenth-Rest       1.00      0.95      0.97        40
       3-4-Time       0.98      1.00      0.99        40
    Common-Time       0.97      0.97      0.97        39
   Double-Sharp       0.98      1.00      0.99        40
        Natural       0.99      1.00      0.99        79
        Barline       0.98      1.00      0.99        40
         F-Clef       0.99      0.99      0.99        80
    Eighth-Rest       0.87      0.85      0.86        40
            Dot       1.00      0.97      0.99        40
Sixty-Four-Rest       0.89      0.91      0.90        80
       9-8-Time       0.90      0.90      0.90        40
   Quarter-Note       0.96      0.87      0.91        79
   Quarter-Rest       1.00      0.95      0.97        40
     Whole-Note       0.84      0.90      0.87        79
Thirty-Two-Note       0.90      0.93      0.91        40
       3-8-Time       0.95      0.97      0.96        40
          Sharp       0.98      1.00      0.99        40

    avg / total       0.96      0.96      0.96      1515

Total Loss: 0.30731
Total Accuracy: 96.10561%
Total Error: 3.89439%
Execution time: 3854.8s
**********************
Windows PowerShell transcript end
End time: 20170612175508
**********************
