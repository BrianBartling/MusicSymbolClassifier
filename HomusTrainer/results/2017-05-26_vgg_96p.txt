C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/ModelGenerator/TrainModel.py --delete_and_recreate_dataset_directory True
Using TensorFlow backend.
Deleting split directories... 
Splitting data into training, validation and test sets...
Copying 320 training images of 12-8-Time...
Copying 40 validation images of 12-8-Time...
Copying 40 test images of 12-8-Time...
Copying 318 training images of 2-2-Time...
Copying 39 validation images of 2-2-Time...
Copying 39 test images of 2-2-Time...
Copying 320 training images of 2-4-Time...
Copying 40 validation images of 2-4-Time...
Copying 40 test images of 2-4-Time...
Copying 320 training images of 3-4-Time...
Copying 40 validation images of 3-4-Time...
Copying 40 test images of 3-4-Time...
Copying 320 training images of 3-8-Time...
Copying 40 validation images of 3-8-Time...
Copying 40 test images of 3-8-Time...
Copying 320 training images of 4-4-Time...
Copying 40 validation images of 4-4-Time...
Copying 40 test images of 4-4-Time...
Copying 320 training images of 6-8-Time...
Copying 40 validation images of 6-8-Time...
Copying 40 test images of 6-8-Time...
Copying 320 training images of 9-8-Time...
Copying 40 validation images of 9-8-Time...
Copying 40 test images of 9-8-Time...
Copying 322 training images of Barline...
Copying 40 validation images of Barline...
Copying 40 test images of Barline...
Copying 320 training images of C-Clef...
Copying 40 validation images of C-Clef...
Copying 40 test images of C-Clef...
Copying 320 training images of Common-Time...
Copying 40 validation images of Common-Time...
Copying 40 test images of Common-Time...
Copying 324 training images of Cut-Time...
Copying 40 validation images of Cut-Time...
Copying 40 test images of Cut-Time...
Copying 320 training images of Dot...
Copying 40 validation images of Dot...
Copying 40 test images of Dot...
Copying 320 training images of Double-Sharp...
Copying 40 validation images of Double-Sharp...
Copying 40 test images of Double-Sharp...
Copying 640 training images of Eighth-Note...
Copying 80 validation images of Eighth-Note...
Copying 80 test images of Eighth-Note...
Copying 320 training images of Eighth-Rest...
Copying 40 validation images of Eighth-Rest...
Copying 40 test images of Eighth-Rest...
Copying 320 training images of F-Clef...
Copying 40 validation images of F-Clef...
Copying 40 test images of F-Clef...
Copying 321 training images of Flat...
Copying 39 validation images of Flat...
Copying 39 test images of Flat...
Copying 320 training images of G-Clef...
Copying 40 validation images of G-Clef...
Copying 40 test images of G-Clef...
Copying 641 training images of Half-Note...
Copying 79 validation images of Half-Note...
Copying 79 test images of Half-Note...
Copying 320 training images of Natural...
Copying 40 validation images of Natural...
Copying 40 test images of Natural...
Copying 641 training images of Quarter-Note...
Copying 80 validation images of Quarter-Note...
Copying 80 test images of Quarter-Note...
Copying 320 training images of Quarter-Rest...
Copying 40 validation images of Quarter-Rest...
Copying 40 test images of Quarter-Rest...
Copying 320 training images of Sharp...
Copying 40 validation images of Sharp...
Copying 40 test images of Sharp...
Copying 641 training images of Sixteenth-Note...
Copying 80 validation images of Sixteenth-Note...
Copying 80 test images of Sixteenth-Note...
Copying 320 training images of Sixteenth-Rest...
Copying 40 validation images of Sixteenth-Rest...
Copying 40 test images of Sixteenth-Rest...
Copying 641 training images of Sixty-Four-Note...
Copying 79 validation images of Sixty-Four-Note...
Copying 79 test images of Sixty-Four-Note...
Copying 320 training images of Sixty-Four-Rest...
Copying 40 validation images of Sixty-Four-Rest...
Copying 40 test images of Sixty-Four-Rest...
Copying 641 training images of Thirty-Two-Note...
Copying 79 validation images of Thirty-Two-Note...
Copying 79 test images of Thirty-Two-Note...
Copying 320 training images of Thirty-Two-Rest...
Copying 40 validation images of Thirty-Two-Rest...
Copying 40 test images of Thirty-Two-Rest...
Copying 320 training images of Whole-Half-Rest...
Copying 40 validation images of Whole-Half-Rest...
Copying 40 test images of Whole-Half-Rest...
Copying 320 training images of Whole-Note...
Copying 40 validation images of Whole-Note...
Copying 40 test images of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 244, 128, 16)      448       
_________________________________________________________________
batch_normalization_1 (Batch (None, 244, 128, 16)      64        
_________________________________________________________________
activation_1 (Activation)    (None, 244, 128, 16)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 244, 128, 16)      2320      
_________________________________________________________________
batch_normalization_2 (Batch (None, 244, 128, 16)      64        
_________________________________________________________________
activation_2 (Activation)    (None, 244, 128, 16)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 122, 64, 16)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 122, 64, 32)       4640      
_________________________________________________________________
batch_normalization_3 (Batch (None, 122, 64, 32)       128       
_________________________________________________________________
activation_3 (Activation)    (None, 122, 64, 32)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 122, 64, 32)       9248      
_________________________________________________________________
batch_normalization_4 (Batch (None, 122, 64, 32)       128       
_________________________________________________________________
activation_4 (Activation)    (None, 122, 64, 32)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 61, 32, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 61, 32, 64)        18496     
_________________________________________________________________
batch_normalization_5 (Batch (None, 61, 32, 64)        256       
_________________________________________________________________
activation_5 (Activation)    (None, 61, 32, 64)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 61, 32, 64)        36928     
_________________________________________________________________
batch_normalization_6 (Batch (None, 61, 32, 64)        256       
_________________________________________________________________
activation_6 (Activation)    (None, 61, 32, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 61, 32, 64)        36928     
_________________________________________________________________
batch_normalization_7 (Batch (None, 61, 32, 64)        256       
_________________________________________________________________
activation_7 (Activation)    (None, 61, 32, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 30, 16, 64)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 30, 16, 128)       73856     
_________________________________________________________________
batch_normalization_8 (Batch (None, 30, 16, 128)       512       
_________________________________________________________________
activation_8 (Activation)    (None, 30, 16, 128)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 30, 16, 128)       147584    
_________________________________________________________________
batch_normalization_9 (Batch (None, 30, 16, 128)       512       
_________________________________________________________________
activation_9 (Activation)    (None, 30, 16, 128)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 30, 16, 128)       147584    
_________________________________________________________________
batch_normalization_10 (Batc (None, 30, 16, 128)       512       
_________________________________________________________________
activation_10 (Activation)   (None, 30, 16, 128)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 15, 8, 128)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 15, 8, 192)        221376    
_________________________________________________________________
batch_normalization_11 (Batc (None, 15, 8, 192)        768       
_________________________________________________________________
activation_11 (Activation)   (None, 15, 8, 192)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 15, 8, 192)        331968    
_________________________________________________________________
batch_normalization_12 (Batc (None, 15, 8, 192)        768       
_________________________________________________________________
activation_12 (Activation)   (None, 15, 8, 192)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 15, 8, 192)        331968    
_________________________________________________________________
batch_normalization_13 (Batc (None, 15, 8, 192)        768       
_________________________________________________________________
activation_13 (Activation)   (None, 15, 8, 192)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 15, 8, 192)        331968    
_________________________________________________________________
batch_normalization_14 (Batc (None, 15, 8, 192)        768       
_________________________________________________________________
activation_14 (Activation)   (None, 15, 8, 192)        0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 4, 192)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 5376)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
Training for 200 epochs with initial learning rate of 0.001, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: he_normal, Minibatch-size: 16, Early stopping after 10 epochs without improvement
Data-Shape: (244, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 5 epochs
Epoch 1/200
2017-05-26 13:48:28.329836: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.330099: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.330360: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.330604: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.330852: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.331090: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.331334: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.331583: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 13:48:28.671135: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-05-26 13:48:28.671404: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-05-26 13:48:28.671527: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-05-26 13:48:28.671668: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
  9/761 [..............................] - ETA: 541s - loss: 5.6020 - acc: 0.09032017-05-26 13:48:34.182736: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3723 get requests, put_count=3646 evicted_count=1000 eviction_rate=0.274273 and unsatisfied allocation rate=0.316143
2017-05-26 13:48:34.182998: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
760/761 [============================>.] - ETA: 0s - loss: 2.0711 - acc: 0.4748Epoch 00000: val_acc improved from -inf to 0.55776, saving model to vgg.h5
761/761 [==============================] - 84s - loss: 2.0690 - acc: 0.4754 - val_loss: 2.0718 - val_acc: 0.5578
Epoch 2/200
760/761 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7490Epoch 00001: val_acc improved from 0.55776 to 0.82706, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.9338 - acc: 0.7491 - val_loss: 0.6671 - val_acc: 0.8271
Epoch 3/200
760/761 [============================>.] - ETA: 0s - loss: 0.6975 - acc: 0.8165Epoch 00002: val_acc improved from 0.82706 to 0.84950, saving model to vgg.h5
761/761 [==============================] - 77s - loss: 0.6980 - acc: 0.8165 - val_loss: 0.6453 - val_acc: 0.8495
Epoch 4/200
760/761 [============================>.] - ETA: 0s - loss: 0.5785 - acc: 0.8506Epoch 00003: val_acc improved from 0.84950 to 0.89967, saving model to vgg.h5
761/761 [==============================] - 77s - loss: 0.5783 - acc: 0.8505 - val_loss: 0.4473 - val_acc: 0.8997
Epoch 5/200
760/761 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8720Epoch 00004: val_acc did not improve
761/761 [==============================] - 80s - loss: 0.5184 - acc: 0.8721 - val_loss: 0.5054 - val_acc: 0.8746
Epoch 6/200
760/761 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8884Epoch 00005: val_acc improved from 0.89967 to 0.91815, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.4633 - acc: 0.8886 - val_loss: 0.4285 - val_acc: 0.9182
Epoch 7/200
760/761 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.9025Epoch 00006: val_acc improved from 0.91815 to 0.92277, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.4270 - acc: 0.9024 - val_loss: 0.3475 - val_acc: 0.9228
Epoch 8/200
760/761 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.9118Epoch 00007: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.3941 - acc: 0.9117 - val_loss: 0.4301 - val_acc: 0.9109
Epoch 9/200
760/761 [============================>.] - ETA: 0s - loss: 0.3829 - acc: 0.9154Epoch 00008: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.3827 - acc: 0.9155 - val_loss: 0.3996 - val_acc: 0.9030
Epoch 10/200
760/761 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.9264Epoch 00009: val_acc improved from 0.92277 to 0.93663, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.3525 - acc: 0.9265 - val_loss: 0.3480 - val_acc: 0.9366
Epoch 11/200
760/761 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.9345Epoch 00010: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.3325 - acc: 0.9345 - val_loss: 0.3954 - val_acc: 0.9122
Epoch 12/200
760/761 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9373Epoch 00011: val_acc improved from 0.93663 to 0.94653, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.3190 - acc: 0.9372 - val_loss: 0.3070 - val_acc: 0.9465
Epoch 13/200
760/761 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.9386Epoch 00012: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.3167 - acc: 0.9385 - val_loss: 0.3397 - val_acc: 0.9307
Epoch 14/200
760/761 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9460Epoch 00013: val_acc improved from 0.94653 to 0.95116, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.2979 - acc: 0.9460 - val_loss: 0.3085 - val_acc: 0.9512
Epoch 15/200
760/761 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9498Epoch 00014: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2874 - acc: 0.9499 - val_loss: 0.3138 - val_acc: 0.9399
Epoch 16/200
760/761 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9498Epoch 00015: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2929 - acc: 0.9498 - val_loss: 0.3758 - val_acc: 0.9267
Epoch 17/200
760/761 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9550Epoch 00016: val_acc improved from 0.95116 to 0.96898, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.2707 - acc: 0.9551 - val_loss: 0.2429 - val_acc: 0.9690
Epoch 18/200
760/761 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9585Epoch 00017: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2649 - acc: 0.9585 - val_loss: 0.2902 - val_acc: 0.9525
Epoch 19/200
760/761 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9590Epoch 00018: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2588 - acc: 0.9591 - val_loss: 0.3328 - val_acc: 0.9406
Epoch 20/200
760/761 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9643Epoch 00019: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2478 - acc: 0.9642 - val_loss: 0.3188 - val_acc: 0.9512
Epoch 21/200
760/761 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9620Epoch 00020: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2511 - acc: 0.9621 - val_loss: 0.3064 - val_acc: 0.9472
Epoch 22/200
760/761 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9658Epoch 00021: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2390 - acc: 0.9657 - val_loss: 0.3279 - val_acc: 0.9393
Epoch 23/200
760/761 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9696Epoch 00022: val_acc did not improve

Epoch 00022: reducing learning rate to 0.0005000000237487257.
761/761 [==============================] - 75s - loss: 0.2342 - acc: 0.9695 - val_loss: 0.2390 - val_acc: 0.9670
Epoch 24/200
760/761 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9799Epoch 00023: val_acc improved from 0.96898 to 0.96964, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.2018 - acc: 0.9799 - val_loss: 0.2407 - val_acc: 0.9696
Epoch 25/200
760/761 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9797Epoch 00024: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.2009 - acc: 0.9797 - val_loss: 0.2717 - val_acc: 0.9644
Epoch 26/200
760/761 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9804Epoch 00025: val_acc improved from 0.96964 to 0.97162, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.1995 - acc: 0.9805 - val_loss: 0.2348 - val_acc: 0.9716
Epoch 27/200
760/761 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9818Epoch 00026: val_acc improved from 0.97162 to 0.97294, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.1949 - acc: 0.9818 - val_loss: 0.2211 - val_acc: 0.9729
Epoch 28/200
760/761 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9832Epoch 00027: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1887 - acc: 0.9832 - val_loss: 0.2446 - val_acc: 0.9716
Epoch 29/200
760/761 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9821Epoch 00028: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1947 - acc: 0.9821 - val_loss: 0.2276 - val_acc: 0.9723
Epoch 30/200
760/761 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9841Epoch 00029: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1881 - acc: 0.9841 - val_loss: 0.2740 - val_acc: 0.9617
Epoch 31/200
760/761 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9866Epoch 00030: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1827 - acc: 0.9866 - val_loss: 0.2587 - val_acc: 0.9630
Epoch 32/200
760/761 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9850Epoch 00031: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1851 - acc: 0.9848 - val_loss: 0.2566 - val_acc: 0.9644
Epoch 33/200
760/761 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9868Epoch 00032: val_acc did not improve

Epoch 00032: reducing learning rate to 0.0002500000118743628.
761/761 [==============================] - 76s - loss: 0.1847 - acc: 0.9869 - val_loss: 0.2383 - val_acc: 0.9696
Epoch 34/200
760/761 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9875Epoch 00033: val_acc improved from 0.97294 to 0.97558, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.1758 - acc: 0.9875 - val_loss: 0.2177 - val_acc: 0.9756
Epoch 35/200
760/761 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9907Epoch 00034: val_acc improved from 0.97558 to 0.97558, saving model to vgg.h5
761/761 [==============================] - 76s - loss: 0.1712 - acc: 0.9907 - val_loss: 0.2295 - val_acc: 0.9756
Epoch 36/200
760/761 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9910Epoch 00035: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1706 - acc: 0.9910 - val_loss: 0.2457 - val_acc: 0.9710
Epoch 37/200
760/761 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9910Epoch 00036: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1708 - acc: 0.9910 - val_loss: 0.2552 - val_acc: 0.9650
Epoch 38/200
760/761 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9905Epoch 00037: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1688 - acc: 0.9906 - val_loss: 0.2364 - val_acc: 0.9743
Epoch 39/200
760/761 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9932Epoch 00038: val_acc improved from 0.97558 to 0.97690, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.1647 - acc: 0.9932 - val_loss: 0.2253 - val_acc: 0.9769
Epoch 40/200
760/761 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9907Epoch 00039: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1680 - acc: 0.9907 - val_loss: 0.2409 - val_acc: 0.9736
Epoch 41/200
760/761 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9919Epoch 00040: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1641 - acc: 0.9920 - val_loss: 0.2446 - val_acc: 0.9736
Epoch 42/200
760/761 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9933Epoch 00041: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1636 - acc: 0.9933 - val_loss: 0.2540 - val_acc: 0.9630
Epoch 43/200
760/761 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9911Epoch 00042: val_acc did not improve
761/761 [==============================] - 76s - loss: 0.1667 - acc: 0.9911 - val_loss: 0.2302 - val_acc: 0.9769
Epoch 44/200
760/761 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9920Epoch 00043: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1641 - acc: 0.9920 - val_loss: 0.2349 - val_acc: 0.9696
Epoch 45/200
760/761 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9929Epoch 00044: val_acc did not improve

Epoch 00044: reducing learning rate to 0.0001250000059371814.
761/761 [==============================] - 75s - loss: 0.1623 - acc: 0.9929 - val_loss: 0.2533 - val_acc: 0.9736
Epoch 46/200
760/761 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9942Epoch 00045: val_acc improved from 0.97690 to 0.97822, saving model to vgg.h5
761/761 [==============================] - 75s - loss: 0.1590 - acc: 0.9943 - val_loss: 0.2175 - val_acc: 0.9782
Epoch 47/200
760/761 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9952Epoch 00046: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1574 - acc: 0.9952 - val_loss: 0.2245 - val_acc: 0.9769
Epoch 48/200
760/761 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9946Epoch 00047: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1570 - acc: 0.9946 - val_loss: 0.2446 - val_acc: 0.9703
Epoch 49/200
760/761 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9936Epoch 00048: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1589 - acc: 0.9936 - val_loss: 0.2325 - val_acc: 0.9729
Epoch 50/200
760/761 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9947Epoch 00049: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1579 - acc: 0.9947 - val_loss: 0.2344 - val_acc: 0.9729
Epoch 51/200
760/761 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9950Epoch 00050: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1570 - acc: 0.9949 - val_loss: 0.2422 - val_acc: 0.9723
Epoch 52/200
760/761 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9946Epoch 00051: val_acc did not improve

Epoch 00051: reducing learning rate to 0.0001.
761/761 [==============================] - 76s - loss: 0.1559 - acc: 0.9946 - val_loss: 0.2337 - val_acc: 0.9756
Epoch 53/200
760/761 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9955Epoch 00052: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1553 - acc: 0.9955 - val_loss: 0.2502 - val_acc: 0.9769
Epoch 54/200
760/761 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9946Epoch 00053: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1562 - acc: 0.9946 - val_loss: 0.2157 - val_acc: 0.9762
Epoch 55/200
760/761 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9956Epoch 00054: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1551 - acc: 0.9956 - val_loss: 0.2231 - val_acc: 0.9762
Epoch 56/200
760/761 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9959Epoch 00055: val_acc did not improve
761/761 [==============================] - 75s - loss: 0.1547 - acc: 0.9959 - val_loss: 0.2444 - val_acc: 0.9710
Epoch 57/200
760/761 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9952Epoch 00056: val_acc did not improve
761/761 [==============================] - 74s - loss: 0.1569 - acc: 0.9952 - val_loss: 0.2349 - val_acc: 0.9749
Epoch 00056: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

   Quarter-Note       1.00      1.00      1.00        40
          Sharp       1.00      1.00      1.00        39
       3-8-Time       0.97      0.97      0.97        40
    Eighth-Rest       1.00      0.97      0.99        40
       9-8-Time       1.00      0.97      0.99        40
         C-Clef       1.00      1.00      1.00        40
   Double-Sharp       1.00      1.00      1.00        40
       2-4-Time       0.98      1.00      0.99        40
 Sixteenth-Rest       0.97      0.97      0.97        40
     Whole-Note       1.00      1.00      1.00        40
Sixty-Four-Rest       1.00      0.97      0.99        40
      12-8-Time       1.00      1.00      1.00        40
         G-Clef       0.97      0.97      0.97        40
Whole-Half-Rest       0.95      1.00      0.98        40
    Eighth-Note       0.97      0.97      0.97        80
       Cut-Time       1.00      0.95      0.97        40
   Quarter-Rest       0.98      1.00      0.99        40
           Flat       0.95      0.97      0.96        39
       6-8-Time       1.00      1.00      1.00        40
Thirty-Two-Rest       1.00      0.99      0.99        79
            Dot       1.00      0.95      0.97        40
    Common-Time       0.98      1.00      0.99        80
Sixty-Four-Note       0.92      0.90      0.91        40
       2-2-Time       1.00      1.00      1.00        40
      Half-Note       0.92      0.95      0.93        80
Thirty-Two-Note       0.93      0.95      0.94        40
        Barline       0.96      0.90      0.93        79
       3-4-Time       1.00      0.93      0.96        40
       4-4-Time       0.87      0.90      0.88        79
 Sixteenth-Note       0.93      0.95      0.94        40
         F-Clef       0.97      0.97      0.97        40
        Natural       0.95      1.00      0.98        40

    avg / total       0.97      0.97      0.97      1515

Total Loss: 0.25626
Total Accuracy: 96.96370%
Total Error: 3.03630%
Execution time: 4354.9s
