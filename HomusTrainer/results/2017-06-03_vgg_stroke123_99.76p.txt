C:\Programming\Anaconda3\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py --delete_and_recreate_dataset_directory False --model_name vgg
Using TensorFlow backend.
Training on dataset...
Found 36490 images belonging to 32 classes.
Found 4555 images belonging to 32 classes.
Found 4555 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 16)      448       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 16)      2320      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 16)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 32)       4640      
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 32)       9248      
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 64)        18496     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 64)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 128)       73856     
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 128)       512       
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 128)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 128)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 192)        221376    
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_14 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_14 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 4, 192)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-06-02 11:22:17.195157: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.195377: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.195589: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.195809: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.196035: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.196676: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.196871: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.197188: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 11:22:17.462532: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2017-06-02 11:22:17.462834: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-06-02 11:22:17.463006: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-06-02 11:22:17.463215: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 32, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'rho': 0.95, 'lr': 1.0, 'decay': 0.0, 'epsilon': 1e-08}
Epoch 1/200
   9/1141 [..............................] - ETA: 1098s - loss: 5.9158 - acc: 0.07292017-06-02 11:22:33.341435: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3616 get requests, put_count=3568 evicted_count=1000 eviction_rate=0.280269 and unsatisfied allocation rate=0.317478
2017-06-02 11:22:33.341693: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
1140/1141 [============================>.] - ETA: 0s - loss: 0.9655 - acc: 0.7468Epoch 00000: val_acc improved from -inf to 0.38793, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 462s - loss: 0.9652 - acc: 0.7469 - val_loss: 3.6128 - val_acc: 0.3879
Epoch 2/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8995Epoch 00001: val_acc improved from 0.38793 to 0.91636, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 455s - loss: 0.4309 - acc: 0.8995 - val_loss: 0.3898 - val_acc: 0.9164
Epoch 3/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.9336Epoch 00002: val_acc did not improve
1141/1141 [==============================] - 459s - loss: 0.3322 - acc: 0.9335 - val_loss: 7.5305 - val_acc: 0.2843
Epoch 4/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9482Epoch 00003: val_acc improved from 0.91636 to 0.92250, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 461s - loss: 0.2886 - acc: 0.9482 - val_loss: 0.3413 - val_acc: 0.9225
Epoch 5/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9605Epoch 00004: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.2509 - acc: 0.9604 - val_loss: 12.0378 - val_acc: 0.1473
Epoch 6/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9664Epoch 00005: val_acc improved from 0.92250 to 0.92689, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.2316 - acc: 0.9665 - val_loss: 0.3875 - val_acc: 0.9269
Epoch 7/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9707Epoch 00006: val_acc improved from 0.92689 to 0.93875, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.2137 - acc: 0.9707 - val_loss: 0.3523 - val_acc: 0.9387
Epoch 8/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9746Epoch 00007: val_acc improved from 0.93875 to 0.98046, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.2038 - acc: 0.9747 - val_loss: 0.1826 - val_acc: 0.9805
Epoch 9/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9782Epoch 00008: val_acc improved from 0.98046 to 0.98375, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.1854 - acc: 0.9782 - val_loss: 0.1767 - val_acc: 0.9838
Epoch 10/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9796Epoch 00009: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1815 - acc: 0.9796 - val_loss: 0.6824 - val_acc: 0.8538
Epoch 11/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9826Epoch 00010: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1688 - acc: 0.9826 - val_loss: 0.1883 - val_acc: 0.9807
Epoch 12/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9831Epoch 00011: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1635 - acc: 0.9831 - val_loss: 0.1913 - val_acc: 0.9780
Epoch 13/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9844Epoch 00012: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1559 - acc: 0.9844 - val_loss: 1.1836 - val_acc: 0.7216
Epoch 14/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9846Epoch 00013: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1514 - acc: 0.9846 - val_loss: 0.5045 - val_acc: 0.9289
Epoch 15/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9869Epoch 00014: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1451 - acc: 0.9869 - val_loss: 0.1726 - val_acc: 0.9737
Epoch 16/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9873Epoch 00015: val_acc improved from 0.98375 to 0.98968, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.1389 - acc: 0.9873 - val_loss: 0.1338 - val_acc: 0.9897
Epoch 17/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9865Epoch 00016: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1379 - acc: 0.9865 - val_loss: 0.1662 - val_acc: 0.9811
Epoch 18/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9878Epoch 00017: val_acc improved from 0.98968 to 0.99232, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.1319 - acc: 0.9878 - val_loss: 0.1202 - val_acc: 0.9923
Epoch 19/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9886Epoch 00018: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1261 - acc: 0.9886 - val_loss: 0.1567 - val_acc: 0.9820
Epoch 20/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9875Epoch 00019: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1272 - acc: 0.9875 - val_loss: 0.1410 - val_acc: 0.9866
Epoch 21/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9885Epoch 00020: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.1222 - acc: 0.9886 - val_loss: 0.1205 - val_acc: 0.9908
Epoch 22/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9883Epoch 00021: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.1223 - acc: 0.9883 - val_loss: 0.1367 - val_acc: 0.9827
Epoch 23/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9890Epoch 00022: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.1189 - acc: 0.9890 - val_loss: 0.1558 - val_acc: 0.9791
Epoch 24/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9890Epoch 00023: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.1180 - acc: 0.9890 - val_loss: 0.1357 - val_acc: 0.9827
Epoch 25/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9897Epoch 00024: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.1133 - acc: 0.9896 - val_loss: 0.1256 - val_acc: 0.9859
Epoch 26/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9903Epoch 00025: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.1118 - acc: 0.9903 - val_loss: 0.1160 - val_acc: 0.9901
Epoch 27/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9907Epoch 00026: val_acc did not improve

Epoch 00026: reducing learning rate to 0.5.
1141/1141 [==============================] - 461s - loss: 0.1100 - acc: 0.9907 - val_loss: 0.2822 - val_acc: 0.9377
Epoch 28/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9962Epoch 00027: val_acc improved from 0.99232 to 0.99517, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 461s - loss: 0.0896 - acc: 0.9961 - val_loss: 0.0996 - val_acc: 0.9952
Epoch 29/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9970Epoch 00028: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.0824 - acc: 0.9970 - val_loss: 0.0939 - val_acc: 0.9934
Epoch 30/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9970Epoch 00029: val_acc improved from 0.99517 to 0.99627, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 461s - loss: 0.0789 - acc: 0.9970 - val_loss: 0.0871 - val_acc: 0.9963
Epoch 31/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9973Epoch 00030: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.0747 - acc: 0.9973 - val_loss: 0.0831 - val_acc: 0.9956
Epoch 32/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9973Epoch 00031: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.0718 - acc: 0.9973 - val_loss: 0.0780 - val_acc: 0.9963
Epoch 33/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9973Epoch 00032: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.0696 - acc: 0.9973 - val_loss: 0.0783 - val_acc: 0.9956
Epoch 34/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9971Epoch 00033: val_acc improved from 0.99627 to 0.99737, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 461s - loss: 0.0671 - acc: 0.9971 - val_loss: 0.0693 - val_acc: 0.9974
Epoch 35/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9968Epoch 00034: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.0661 - acc: 0.9968 - val_loss: 0.0790 - val_acc: 0.9952
Epoch 36/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9971Epoch 00035: val_acc did not improve
1141/1141 [==============================] - 459s - loss: 0.0637 - acc: 0.9972 - val_loss: 0.0789 - val_acc: 0.9952
Epoch 37/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9972Epoch 00036: val_acc did not improve
1141/1141 [==============================] - 456s - loss: 0.0621 - acc: 0.9972 - val_loss: 0.0741 - val_acc: 0.9954
Epoch 38/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9968Epoch 00037: val_acc did not improve
1141/1141 [==============================] - 456s - loss: 0.0623 - acc: 0.9968 - val_loss: 0.0684 - val_acc: 0.9958
Epoch 39/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9971Epoch 00038: val_acc did not improve
1141/1141 [==============================] - 456s - loss: 0.0605 - acc: 0.9971 - val_loss: 0.0696 - val_acc: 0.9958
Epoch 40/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9971Epoch 00039: val_acc did not improve
1141/1141 [==============================] - 463s - loss: 0.0581 - acc: 0.9971 - val_loss: 0.0698 - val_acc: 0.9958
Epoch 41/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9970Epoch 00040: val_acc did not improve
1141/1141 [==============================] - 467s - loss: 0.0581 - acc: 0.9970 - val_loss: 0.0696 - val_acc: 0.9943
Epoch 42/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9973Epoch 00041: val_acc did not improve
1141/1141 [==============================] - 466s - loss: 0.0560 - acc: 0.9973 - val_loss: 0.0712 - val_acc: 0.9952
Epoch 43/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9969Epoch 00042: val_acc did not improve

Epoch 00042: reducing learning rate to 0.25.
1141/1141 [==============================] - 466s - loss: 0.0562 - acc: 0.9969 - val_loss: 0.0686 - val_acc: 0.9965
Epoch 44/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9983Epoch 00043: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.0503 - acc: 0.9983 - val_loss: 0.0596 - val_acc: 0.9971
Epoch 45/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9987Epoch 00044: val_acc did not improve
1141/1141 [==============================] - 461s - loss: 0.0478 - acc: 0.9987 - val_loss: 0.0569 - val_acc: 0.9971
Epoch 46/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9987Epoch 00045: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.0473 - acc: 0.9987 - val_loss: 0.0635 - val_acc: 0.9971
Epoch 47/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9989Epoch 00046: val_acc did not improve
1141/1141 [==============================] - 460s - loss: 0.0451 - acc: 0.9988 - val_loss: 0.0621 - val_acc: 0.9965
Epoch 48/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9989Epoch 00047: val_acc improved from 0.99737 to 0.99759, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 460s - loss: 0.0439 - acc: 0.9989 - val_loss: 0.0567 - val_acc: 0.9976
Epoch 49/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9987Epoch 00048: val_acc did not improve
1141/1141 [==============================] - 465s - loss: 0.0446 - acc: 0.9987 - val_loss: 0.0633 - val_acc: 0.9963
Epoch 50/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9987Epoch 00049: val_acc did not improve
1141/1141 [==============================] - 463s - loss: 0.0433 - acc: 0.9987 - val_loss: 0.0517 - val_acc: 0.9971
Epoch 51/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9992Epoch 00050: val_acc improved from 0.99759 to 0.99780, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 463s - loss: 0.0407 - acc: 0.9992 - val_loss: 0.0483 - val_acc: 0.9978
Epoch 52/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9990Epoch 00051: val_acc did not improve
1141/1141 [==============================] - 463s - loss: 0.0400 - acc: 0.9990 - val_loss: 0.0605 - val_acc: 0.9967
Epoch 53/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9991Epoch 00052: val_acc did not improve
1141/1141 [==============================] - 622s - loss: 0.0385 - acc: 0.9991 - val_loss: 0.0460 - val_acc: 0.9967
Epoch 54/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9988Epoch 00053: val_acc did not improve
1141/1141 [==============================] - 765s - loss: 0.0386 - acc: 0.9988 - val_loss: 0.0581 - val_acc: 0.9954
Epoch 55/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9988Epoch 00054: val_acc did not improve
1141/1141 [==============================] - 778s - loss: 0.0385 - acc: 0.9988 - val_loss: 0.0453 - val_acc: 0.9974
Epoch 56/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9991Epoch 00055: val_acc did not improve
1141/1141 [==============================] - 770s - loss: 0.0365 - acc: 0.9991 - val_loss: 0.0488 - val_acc: 0.9974
Epoch 57/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9991Epoch 00056: val_acc did not improve
1141/1141 [==============================] - 772s - loss: 0.0355 - acc: 0.9991 - val_loss: 0.0488 - val_acc: 0.9965
Epoch 58/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9992Epoch 00057: val_acc did not improve
1141/1141 [==============================] - 761s - loss: 0.0352 - acc: 0.9992 - val_loss: 0.0483 - val_acc: 0.9967
Epoch 59/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9989Epoch 00058: val_acc did not improve
1141/1141 [==============================] - 788s - loss: 0.0352 - acc: 0.9989 - val_loss: 0.0457 - val_acc: 0.9978
Epoch 60/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9989Epoch 00059: val_acc did not improve

Epoch 00059: reducing learning rate to 0.125.
1141/1141 [==============================] - 773s - loss: 0.0343 - acc: 0.9989 - val_loss: 0.0538 - val_acc: 0.9960
Epoch 61/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9991Epoch 00060: val_acc did not improve
1141/1141 [==============================] - 776s - loss: 0.0327 - acc: 0.9991 - val_loss: 0.0429 - val_acc: 0.9978
Epoch 62/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9993Epoch 00061: val_acc did not improve
1141/1141 [==============================] - 760s - loss: 0.0316 - acc: 0.9993 - val_loss: 0.0453 - val_acc: 0.9967
Epoch 63/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9993Epoch 00062: val_acc did not improve
1141/1141 [==============================] - 765s - loss: 0.0317 - acc: 0.9993 - val_loss: 0.0436 - val_acc: 0.9976
Epoch 64/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9996Epoch 00063: val_acc did not improve
1141/1141 [==============================] - 778s - loss: 0.0312 - acc: 0.9996 - val_loss: 0.0427 - val_acc: 0.9976
Epoch 65/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9995Epoch 00064: val_acc did not improve
1141/1141 [==============================] - 767s - loss: 0.0299 - acc: 0.9995 - val_loss: 0.0459 - val_acc: 0.9971
Epoch 66/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9993Epoch 00065: val_acc improved from 0.99780 to 0.99824, saving model to 2017-06-02_vgg.h5
1141/1141 [==============================] - 765s - loss: 0.0298 - acc: 0.9993 - val_loss: 0.0418 - val_acc: 0.9982
Epoch 67/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9992Epoch 00066: val_acc did not improve
1141/1141 [==============================] - 771s - loss: 0.0302 - acc: 0.9992 - val_loss: 0.0416 - val_acc: 0.9976
Epoch 68/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9995Epoch 00067: val_acc did not improve
1141/1141 [==============================] - 776s - loss: 0.0288 - acc: 0.9995 - val_loss: 0.0450 - val_acc: 0.9971
Epoch 69/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9995Epoch 00068: val_acc did not improve
1141/1141 [==============================] - 772s - loss: 0.0282 - acc: 0.9995 - val_loss: 0.0373 - val_acc: 0.9976
Epoch 70/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9993Epoch 00069: val_acc did not improve
1141/1141 [==============================] - 735s - loss: 0.0285 - acc: 0.9993 - val_loss: 0.0405 - val_acc: 0.9980
Epoch 71/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9995Epoch 00070: val_acc did not improve
1141/1141 [==============================] - 769s - loss: 0.0281 - acc: 0.9995 - val_loss: 0.0423 - val_acc: 0.9974
Epoch 72/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9995Epoch 00071: val_acc did not improve
1141/1141 [==============================] - 762s - loss: 0.0274 - acc: 0.9995 - val_loss: 0.0403 - val_acc: 0.9971
Epoch 73/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9996Epoch 00072: val_acc did not improve
1141/1141 [==============================] - 779s - loss: 0.0269 - acc: 0.9996 - val_loss: 0.0383 - val_acc: 0.9963
Epoch 74/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9993Epoch 00073: val_acc did not improve
1141/1141 [==============================] - 764s - loss: 0.0270 - acc: 0.9993 - val_loss: 0.0366 - val_acc: 0.9974
Epoch 75/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9995Epoch 00074: val_acc did not improve

Epoch 00074: reducing learning rate to 0.0625.
1141/1141 [==============================] - 780s - loss: 0.0263 - acc: 0.9995 - val_loss: 0.0421 - val_acc: 0.9960
Epoch 76/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9996Epoch 00075: val_acc did not improve
1141/1141 [==============================] - 774s - loss: 0.0254 - acc: 0.9996 - val_loss: 0.0405 - val_acc: 0.9971
Epoch 77/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9996Epoch 00076: val_acc did not improve
1141/1141 [==============================] - 765s - loss: 0.0256 - acc: 0.9996 - val_loss: 0.0356 - val_acc: 0.9974
Epoch 78/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9994Epoch 00077: val_acc did not improve
1141/1141 [==============================] - 782s - loss: 0.0255 - acc: 0.9994 - val_loss: 0.0411 - val_acc: 0.9974
Epoch 79/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9996Epoch 00078: val_acc did not improve
1141/1141 [==============================] - 778s - loss: 0.0244 - acc: 0.9996 - val_loss: 0.0379 - val_acc: 0.9982
Epoch 80/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9995Epoch 00079: val_acc did not improve
1141/1141 [==============================] - 768s - loss: 0.0248 - acc: 0.9995 - val_loss: 0.0359 - val_acc: 0.9976
Epoch 81/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9995Epoch 00080: val_acc did not improve
1141/1141 [==============================] - 776s - loss: 0.0245 - acc: 0.9995 - val_loss: 0.0404 - val_acc: 0.9974
Epoch 82/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9996Epoch 00081: val_acc did not improve
1141/1141 [==============================] - 788s - loss: 0.0242 - acc: 0.9996 - val_loss: 0.0408 - val_acc: 0.9971
Epoch 83/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9997Epoch 00082: val_acc did not improve

Epoch 00082: reducing learning rate to 0.03125.
1141/1141 [==============================] - 764s - loss: 0.0240 - acc: 0.9997 - val_loss: 0.0342 - val_acc: 0.9978
Epoch 84/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9996Epoch 00083: val_acc did not improve
1141/1141 [==============================] - 767s - loss: 0.0241 - acc: 0.9996 - val_loss: 0.0378 - val_acc: 0.9974
Epoch 85/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9995Epoch 00084: val_acc did not improve
1141/1141 [==============================] - 775s - loss: 0.0240 - acc: 0.9995 - val_loss: 0.0377 - val_acc: 0.9974
Epoch 86/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9996Epoch 00085: val_acc did not improve
1141/1141 [==============================] - 765s - loss: 0.0234 - acc: 0.9996 - val_loss: 0.0368 - val_acc: 0.9971
Epoch 87/200
1140/1141 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9996Epoch 00086: val_acc did not improve
1141/1141 [==============================] - 800s - loss: 0.0232 - acc: 0.9996 - val_loss: 0.0384 - val_acc: 0.9969
Epoch 00086: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

      12-8-Time       1.00      1.00      1.00       120
       2-2-Time       1.00      1.00      1.00       118
       2-4-Time       1.00      1.00      1.00       120
       3-4-Time       1.00      1.00      1.00       120
       3-8-Time       1.00      1.00      1.00       120
       4-4-Time       1.00      1.00      1.00       120
       6-8-Time       1.00      1.00      1.00       120
       9-8-Time       1.00      1.00      1.00       120
        Barline       1.00      1.00      1.00       120
         C-Clef       1.00      1.00      1.00       120
    Common-Time       1.00      1.00      1.00       120
       Cut-Time       1.00      1.00      1.00       121
            Dot       1.00      0.99      1.00       120
   Double-Sharp       1.00      1.00      1.00       120
    Eighth-Note       1.00      1.00      1.00       240
    Eighth-Rest       1.00      1.00      1.00       120
         F-Clef       1.00      1.00      1.00       120
           Flat       1.00      1.00      1.00       119
         G-Clef       1.00      1.00      1.00       120
      Half-Note       1.00      1.00      1.00       239
        Natural       1.00      1.00      1.00       120
   Quarter-Note       0.99      1.00      0.99       240
   Quarter-Rest       1.00      0.99      1.00       120
          Sharp       1.00      1.00      1.00       120
 Sixteenth-Note       1.00      0.99      0.99       240
 Sixteenth-Rest       1.00      1.00      1.00       120
Sixty-Four-Note       0.99      0.99      0.99       239
Sixty-Four-Rest       1.00      1.00      1.00       120
Thirty-Two-Note       0.98      0.99      0.99       239
Thirty-Two-Rest       1.00      1.00      1.00       120
Whole-Half-Rest       0.99      1.00      1.00       120
     Whole-Note       1.00      1.00      1.00       120

    avg / total       1.00      1.00      1.00      4555

Total Loss: 0.03671
Total Accuracy: 99.75851%
Total Error: 0.24149%
Execution time: 50900.6s
