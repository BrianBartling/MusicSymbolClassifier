C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py --model_name vgg4
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses
15200/15200Deleting split directories... 
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 32)      9248      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 64)       18496     
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 64)       36928     
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 128)       73856     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 256)       295168    
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 256)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 256)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 512)        1180160   
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
average_pooling2d_1 (Average (None, 7, 4, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 14336)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                458784    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 8,280,000
Trainable params: 8,274,240
Non-trainable params: 5,760
_________________________________________________________________
Model vgg4 loaded.
2017-06-01 17:47:22.334780: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.335053: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.335294: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.335575: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.335868: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.336734: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.336904: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.337069: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-01 17:47:22.668647: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-01 17:47:22.668910: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-06-01 17:47:22.669035: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-06-01 17:47:22.669169: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 64, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'rho': 0.95, 'epsilon': 1e-08, 'decay': 0.0, 'lr': 1.0}
Epoch 1/200
 10/191 [>.............................] - ETA: 156s - loss: 6.2183 - acc: 0.06722017-06-01 17:47:39.397732: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3729 get requests, put_count=3726 evicted_count=1000 eviction_rate=0.268384 and unsatisfied allocation rate=0.29579
2017-06-01 17:47:39.397984: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
190/191 [============================>.] - ETA: 0s - loss: 2.5812 - acc: 0.3761Epoch 00000: val_acc improved from -inf to 0.18416, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 80s - loss: 2.5782 - acc: 0.3762 - val_loss: 5.7868 - val_acc: 0.1842
Epoch 2/200
190/191 [============================>.] - ETA: 0s - loss: 1.1132 - acc: 0.7105Epoch 00001: val_acc improved from 0.18416 to 0.27327, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 70s - loss: 1.1126 - acc: 0.7100 - val_loss: 5.4929 - val_acc: 0.2733
Epoch 3/200
190/191 [============================>.] - ETA: 0s - loss: 0.8750 - acc: 0.7813Epoch 00002: val_acc improved from 0.27327 to 0.56436, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.8740 - acc: 0.7814 - val_loss: 3.0157 - val_acc: 0.5644
Epoch 4/200
190/191 [============================>.] - ETA: 0s - loss: 0.7232 - acc: 0.8322Epoch 00003: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.7223 - acc: 0.8325 - val_loss: 3.1021 - val_acc: 0.4799
Epoch 5/200
190/191 [============================>.] - ETA: 0s - loss: 0.6363 - acc: 0.8569Epoch 00004: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.6355 - acc: 0.8571 - val_loss: 3.3406 - val_acc: 0.5446
Epoch 6/200
190/191 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.8727Epoch 00005: val_acc improved from 0.56436 to 0.73003, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.5937 - acc: 0.8723 - val_loss: 1.4996 - val_acc: 0.7300
Epoch 7/200
190/191 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8887Epoch 00006: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.5488 - acc: 0.8877 - val_loss: 3.1408 - val_acc: 0.5208
Epoch 8/200
190/191 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.9010Epoch 00007: val_acc improved from 0.73003 to 0.85083, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.5054 - acc: 0.9010 - val_loss: 0.8037 - val_acc: 0.8508
Epoch 9/200
190/191 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.9118Epoch 00008: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.4821 - acc: 0.9113 - val_loss: 2.6636 - val_acc: 0.5564
Epoch 10/200
190/191 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.9163Epoch 00009: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.4552 - acc: 0.9162 - val_loss: 1.9695 - val_acc: 0.6561
Epoch 11/200
190/191 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.9194Epoch 00010: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.4400 - acc: 0.9188 - val_loss: 0.9386 - val_acc: 0.8396
Epoch 12/200
190/191 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.9289Epoch 00011: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.4084 - acc: 0.9293 - val_loss: 0.6943 - val_acc: 0.8442
Epoch 13/200
190/191 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.9349Epoch 00012: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.3936 - acc: 0.9347 - val_loss: 1.5668 - val_acc: 0.6865
Epoch 14/200
190/191 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.9382Epoch 00013: val_acc improved from 0.85083 to 0.90231, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.3819 - acc: 0.9380 - val_loss: 0.5484 - val_acc: 0.9023
Epoch 15/200
190/191 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.9412Epoch 00014: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.3645 - acc: 0.9415 - val_loss: 0.4963 - val_acc: 0.8944
Epoch 16/200
190/191 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.9437Epoch 00015: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.3510 - acc: 0.9435 - val_loss: 0.5767 - val_acc: 0.9010
Epoch 17/200
190/191 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.9443Epoch 00016: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.3534 - acc: 0.9436 - val_loss: 1.8946 - val_acc: 0.7063
Epoch 18/200
190/191 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.9544Epoch 00017: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.3179 - acc: 0.9547 - val_loss: 0.9979 - val_acc: 0.7941
Epoch 19/200
190/191 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.9494Epoch 00018: val_acc did not improve
191/191 [==============================] - 66s - loss: 0.3243 - acc: 0.9497 - val_loss: 1.5560 - val_acc: 0.7617
Epoch 20/200
190/191 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9574Epoch 00019: val_acc improved from 0.90231 to 0.91815, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 67s - loss: 0.3019 - acc: 0.9576 - val_loss: 0.4239 - val_acc: 0.9182
Epoch 21/200
190/191 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9554Epoch 00020: val_acc improved from 0.91815 to 0.93201, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.3004 - acc: 0.9557 - val_loss: 0.3718 - val_acc: 0.9320
Epoch 22/200
190/191 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9567Epoch 00021: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2909 - acc: 0.9570 - val_loss: 0.5005 - val_acc: 0.9116
Epoch 23/200
190/191 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9618Epoch 00022: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2780 - acc: 0.9620 - val_loss: 0.5036 - val_acc: 0.8997
Epoch 24/200
190/191 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9640Epoch 00023: val_acc improved from 0.93201 to 0.93597, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.2694 - acc: 0.9642 - val_loss: 0.3680 - val_acc: 0.9360
Epoch 25/200
190/191 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9618Epoch 00024: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2735 - acc: 0.9620 - val_loss: 0.4216 - val_acc: 0.9300
Epoch 26/200
190/191 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9651Epoch 00025: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2643 - acc: 0.9643 - val_loss: 1.1615 - val_acc: 0.7802
Epoch 27/200
190/191 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9669Epoch 00026: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2525 - acc: 0.9665 - val_loss: 0.7054 - val_acc: 0.8601
Epoch 28/200
190/191 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9697Epoch 00027: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2480 - acc: 0.9693 - val_loss: 0.8346 - val_acc: 0.8686
Epoch 29/200
190/191 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9675Epoch 00028: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2407 - acc: 0.9672 - val_loss: 0.4530 - val_acc: 0.9168
Epoch 30/200
190/191 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9719Epoch 00029: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2329 - acc: 0.9715 - val_loss: 0.9373 - val_acc: 0.8389
Epoch 31/200
190/191 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9689Epoch 00030: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2370 - acc: 0.9691 - val_loss: 0.6405 - val_acc: 0.8792
Epoch 32/200
190/191 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9703Epoch 00031: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.2264 - acc: 0.9705 - val_loss: 0.7762 - val_acc: 0.8832
Epoch 33/200
190/191 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9724Epoch 00032: val_acc did not improve

Epoch 00032: reducing learning rate to 0.5.
191/191 [==============================] - 65s - loss: 0.2203 - acc: 0.9720 - val_loss: 0.4892 - val_acc: 0.9155
Epoch 34/200
190/191 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9853Epoch 00033: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1854 - acc: 0.9854 - val_loss: 0.3997 - val_acc: 0.9228
Epoch 35/200
190/191 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9882Epoch 00034: val_acc improved from 0.93597 to 0.95776, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.1749 - acc: 0.9882 - val_loss: 0.2949 - val_acc: 0.9578
Epoch 36/200
190/191 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9914Epoch 00035: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1667 - acc: 0.9909 - val_loss: 0.3604 - val_acc: 0.9314
Epoch 37/200
190/191 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9920Epoch 00036: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1617 - acc: 0.9921 - val_loss: 0.2704 - val_acc: 0.9578
Epoch 38/200
190/191 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9921Epoch 00037: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1576 - acc: 0.9921 - val_loss: 0.3507 - val_acc: 0.9320
Epoch 39/200
190/191 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9912Epoch 00038: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1565 - acc: 0.9912 - val_loss: 0.3382 - val_acc: 0.9564
Epoch 40/200
190/191 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9922Epoch 00039: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1536 - acc: 0.9922 - val_loss: 0.3157 - val_acc: 0.9538
Epoch 41/200
190/191 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9914Epoch 00040: val_acc improved from 0.95776 to 0.95908, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.1501 - acc: 0.9915 - val_loss: 0.3120 - val_acc: 0.9591
Epoch 42/200
190/191 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9919Epoch 00041: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1504 - acc: 0.9915 - val_loss: 0.5470 - val_acc: 0.9076
Epoch 43/200
190/191 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9939Epoch 00042: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1460 - acc: 0.9929 - val_loss: 0.2991 - val_acc: 0.9498
Epoch 44/200
190/191 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9950Epoch 00043: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1388 - acc: 0.9950 - val_loss: 0.3287 - val_acc: 0.9538
Epoch 45/200
190/191 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9925Epoch 00044: val_acc improved from 0.95908 to 0.95974, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 66s - loss: 0.1390 - acc: 0.9926 - val_loss: 0.2625 - val_acc: 0.9597
Epoch 46/200
190/191 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9938Epoch 00045: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1370 - acc: 0.9938 - val_loss: 0.3340 - val_acc: 0.9505
Epoch 47/200
190/191 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9938Epoch 00046: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1342 - acc: 0.9938 - val_loss: 0.3430 - val_acc: 0.9498
Epoch 48/200
190/191 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9938Epoch 00047: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1344 - acc: 0.9933 - val_loss: 0.5366 - val_acc: 0.9234
Epoch 49/200
190/191 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9937Epoch 00048: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1318 - acc: 0.9937 - val_loss: 0.2912 - val_acc: 0.9545
Epoch 50/200
190/191 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9929Epoch 00049: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1315 - acc: 0.9930 - val_loss: 0.2995 - val_acc: 0.9531
Epoch 51/200
190/191 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9927Epoch 00050: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1322 - acc: 0.9922 - val_loss: 0.2995 - val_acc: 0.9564
Epoch 52/200
190/191 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9947Epoch 00051: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1233 - acc: 0.9948 - val_loss: 0.2896 - val_acc: 0.9578
Epoch 53/200
190/191 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9950Epoch 00052: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1247 - acc: 0.9945 - val_loss: 0.5050 - val_acc: 0.9162
Epoch 54/200
190/191 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9935Epoch 00053: val_acc improved from 0.95974 to 0.96238, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.1238 - acc: 0.9935 - val_loss: 0.2531 - val_acc: 0.9624
Epoch 55/200
190/191 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9929Epoch 00054: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1242 - acc: 0.9930 - val_loss: 0.4444 - val_acc: 0.9234
Epoch 56/200
190/191 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9941Epoch 00055: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1216 - acc: 0.9941 - val_loss: 0.2760 - val_acc: 0.9591
Epoch 57/200
190/191 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9944Epoch 00056: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1188 - acc: 0.9944 - val_loss: 0.3777 - val_acc: 0.9459
Epoch 58/200
190/191 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9947Epoch 00057: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1155 - acc: 0.9947 - val_loss: 0.3198 - val_acc: 0.9505
Epoch 59/200
190/191 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9925Epoch 00058: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1185 - acc: 0.9926 - val_loss: 0.3371 - val_acc: 0.9505
Epoch 60/200
190/191 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9936Epoch 00059: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1182 - acc: 0.9936 - val_loss: 0.3271 - val_acc: 0.9525
Epoch 61/200
190/191 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9952Epoch 00060: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1121 - acc: 0.9953 - val_loss: 0.2938 - val_acc: 0.9505
Epoch 62/200
190/191 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9942Epoch 00061: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1132 - acc: 0.9942 - val_loss: 0.2773 - val_acc: 0.9578
Epoch 63/200
190/191 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9932Epoch 00062: val_acc did not improve

Epoch 00062: reducing learning rate to 0.25.
191/191 [==============================] - 65s - loss: 0.1142 - acc: 0.9932 - val_loss: 0.2417 - val_acc: 0.9611
Epoch 64/200
190/191 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9973Epoch 00063: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1030 - acc: 0.9973 - val_loss: 0.2821 - val_acc: 0.9545
Epoch 65/200
190/191 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9979Epoch 00064: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.1001 - acc: 0.9974 - val_loss: 0.3247 - val_acc: 0.9426
Epoch 66/200
190/191 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9984Epoch 00065: val_acc improved from 0.96238 to 0.96766, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.0984 - acc: 0.9984 - val_loss: 0.2282 - val_acc: 0.9677
Epoch 67/200
190/191 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9979Epoch 00066: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0973 - acc: 0.9980 - val_loss: 0.2839 - val_acc: 0.9617
Epoch 68/200
190/191 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9983Epoch 00067: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0971 - acc: 0.9983 - val_loss: 0.2460 - val_acc: 0.9644
Epoch 69/200
190/191 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9981Epoch 00068: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0955 - acc: 0.9981 - val_loss: 0.2477 - val_acc: 0.9558
Epoch 70/200
190/191 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9985Epoch 00069: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0935 - acc: 0.9985 - val_loss: 0.2539 - val_acc: 0.9591
Epoch 71/200
190/191 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9988Epoch 00070: val_acc improved from 0.96766 to 0.97096, saving model to 2017-06-01_vgg4.h5
191/191 [==============================] - 65s - loss: 0.0924 - acc: 0.9988 - val_loss: 0.2340 - val_acc: 0.9710
Epoch 72/200
190/191 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9979Epoch 00071: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0926 - acc: 0.9974 - val_loss: 0.3020 - val_acc: 0.9512
Epoch 73/200
190/191 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9982Epoch 00072: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0919 - acc: 0.9982 - val_loss: 0.2566 - val_acc: 0.9611
Epoch 74/200
190/191 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9979Epoch 00073: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0917 - acc: 0.9979 - val_loss: 0.2184 - val_acc: 0.9644
Epoch 75/200
190/191 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9987Epoch 00074: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0888 - acc: 0.9987 - val_loss: 0.2604 - val_acc: 0.9611
Epoch 76/200
190/191 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9988Epoch 00075: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0872 - acc: 0.9989 - val_loss: 0.2416 - val_acc: 0.9644
Epoch 77/200
190/191 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9982Epoch 00076: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0889 - acc: 0.9982 - val_loss: 0.2461 - val_acc: 0.9637
Epoch 78/200
190/191 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9986Epoch 00077: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0870 - acc: 0.9986 - val_loss: 0.2669 - val_acc: 0.9604
Epoch 79/200
190/191 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9984Epoch 00078: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0860 - acc: 0.9984 - val_loss: 0.2535 - val_acc: 0.9604
Epoch 80/200
190/191 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9987Epoch 00079: val_acc did not improve

Epoch 00079: reducing learning rate to 0.125.
191/191 [==============================] - 65s - loss: 0.0848 - acc: 0.9987 - val_loss: 0.2338 - val_acc: 0.9604
Epoch 81/200
190/191 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9990Epoch 00080: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0833 - acc: 0.9990 - val_loss: 0.2613 - val_acc: 0.9591
Epoch 82/200
190/191 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9993Epoch 00081: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0819 - acc: 0.9993 - val_loss: 0.2099 - val_acc: 0.9624
Epoch 83/200
190/191 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9993Epoch 00082: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0819 - acc: 0.9993 - val_loss: 0.2400 - val_acc: 0.9611
Epoch 84/200
190/191 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9995Epoch 00083: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0811 - acc: 0.9995 - val_loss: 0.2306 - val_acc: 0.9650
Epoch 85/200
190/191 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9995Epoch 00084: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0800 - acc: 0.9995 - val_loss: 0.2180 - val_acc: 0.9657
Epoch 86/200
190/191 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9994Epoch 00085: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0799 - acc: 0.9994 - val_loss: 0.2134 - val_acc: 0.9690
Epoch 87/200
190/191 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9992Epoch 00086: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0794 - acc: 0.9992 - val_loss: 0.2396 - val_acc: 0.9604
Epoch 88/200
190/191 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9993Epoch 00087: val_acc did not improve

Epoch 00087: reducing learning rate to 0.0625.
191/191 [==============================] - 65s - loss: 0.0799 - acc: 0.9993 - val_loss: 0.2347 - val_acc: 0.9637
Epoch 89/200
190/191 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9994Epoch 00088: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0785 - acc: 0.9994 - val_loss: 0.2249 - val_acc: 0.9670
Epoch 90/200
190/191 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9995Epoch 00089: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0778 - acc: 0.9995 - val_loss: 0.2410 - val_acc: 0.9630
Epoch 91/200
190/191 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9997Epoch 00090: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0773 - acc: 0.9997 - val_loss: 0.2258 - val_acc: 0.9617
Epoch 92/200
190/191 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9997Epoch 00091: val_acc did not improve
191/191 [==============================] - 65s - loss: 0.0769 - acc: 0.9997 - val_loss: 0.2182 - val_acc: 0.9657
Epoch 00091: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

       2-4-Time       1.00      1.00      1.00        40
      Half-Note       1.00      1.00      1.00        39
Whole-Half-Rest       0.97      0.97      0.97        40
        Barline       1.00      0.97      0.99        40
          Sharp       1.00      1.00      1.00        40
 Sixteenth-Note       0.95      1.00      0.98        40
         F-Clef       0.98      1.00      0.99        40
       3-8-Time       1.00      0.97      0.99        40
    Eighth-Rest       1.00      0.97      0.99        40
           Flat       1.00      0.97      0.99        40
         G-Clef       1.00      1.00      1.00        40
       2-2-Time       1.00      1.00      1.00        40
       6-8-Time       1.00      0.97      0.99        40
         C-Clef       0.95      0.97      0.96        40
   Double-Sharp       0.95      0.90      0.92        80
       9-8-Time       0.93      0.95      0.94        40
       3-4-Time       0.98      1.00      0.99        40
Thirty-Two-Rest       1.00      0.97      0.99        39
        Natural       1.00      1.00      1.00        40
    Common-Time       1.00      1.00      1.00        79
     Whole-Note       1.00      0.90      0.95        40
Sixty-Four-Note       0.99      1.00      0.99        80
       4-4-Time       0.93      0.93      0.93        40
   Quarter-Rest       1.00      0.97      0.99        40
            Dot       0.87      0.93      0.90        80
Thirty-Two-Note       0.93      0.93      0.93        40
 Sixteenth-Rest       0.90      0.91      0.91        79
    Eighth-Note       0.95      0.93      0.94        40
   Quarter-Note       0.88      0.87      0.88        79
Sixty-Four-Rest       0.88      0.88      0.88        40
       Cut-Time       0.93      0.97      0.95        40
      12-8-Time       0.95      1.00      0.98        40

    avg / total       0.96      0.96      0.96      1515

Total Loss: 0.26508
Total Accuracy: 95.97360%
Total Error: 4.02640%
Execution time: 6089.5s
