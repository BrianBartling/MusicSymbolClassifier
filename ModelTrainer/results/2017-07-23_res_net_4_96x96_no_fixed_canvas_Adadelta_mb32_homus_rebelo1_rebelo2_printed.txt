**********************
Windows PowerShell transcript start
Start time: 20170723222123
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Machine: DONKEY (Microsoft Windows NT 10.0.15063.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\TrainModel.ps1'
Process ID: 8444
PSVersion: 5.1.15063.483
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.15063.483
BuildVersion: 10.0.15063.483
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\2017-07-23_res_net_4_96x96_no_fixed_canvas_Adadelta_mb32_homus_rebelo1_rebelo2_printed.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\data\images
15200/15200Extracting Rebelo Symbol Dataset 1...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset1
Extracting Rebelo Symbol Dataset 2...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\Rebelo-Music-Symbol-Dataset2
Extracting Printed Music Symbol dataset...
Deleting temporary directory C:\Users\Alex\Repositories\MusicSymbolClassifier\ModelTrainer\PrintedMusicSymbolsDataset
Resizing all images with the LANCZOS interpolation to 96x96px (width x height).
Deleting split directories...
Splitting data into training, validation and test sets...
Copying 2 training files of 1-8-Time...
Copying 0 validation files of 1-8-Time...
Copying 0 test files of 1-8-Time...
Copying 323 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 319 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 353 training files of 2-4-Time...
Copying 44 validation files of 2-4-Time...
Copying 44 test files of 2-4-Time...
Copying 10 training files of 2-8-Time...
Copying 1 validation files of 2-8-Time...
Copying 1 test files of 2-8-Time...
Copying 387 training files of 3-4-Time...
Copying 48 validation files of 3-4-Time...
Copying 48 test files of 3-4-Time...
Copying 344 training files of 3-8-Time...
Copying 42 validation files of 3-8-Time...
Copying 42 test files of 3-8-Time...
Copying 6 training files of 4-2-Time...
Copying 0 validation files of 4-2-Time...
Copying 0 test files of 4-2-Time...
Copying 337 training files of 4-4-Time...
Copying 42 validation files of 4-4-Time...
Copying 42 test files of 4-4-Time...
Copying 1 training files of 4-8-Time...
Copying 0 validation files of 4-8-Time...
Copying 0 test files of 4-8-Time...
Copying 12 training files of 5-4-Time...
Copying 1 validation files of 5-4-Time...
Copying 1 test files of 5-4-Time...
Copying 11 training files of 5-8-Time...
Copying 1 validation files of 5-8-Time...
Copying 1 test files of 5-8-Time...
Copying 4 training files of 6-4-Time...
Copying 0 validation files of 6-4-Time...
Copying 0 test files of 6-4-Time...
Copying 337 training files of 6-8-Time...
Copying 42 validation files of 6-8-Time...
Copying 42 test files of 6-8-Time...
Copying 9 training files of 7-4-Time...
Copying 0 validation files of 7-4-Time...
Copying 0 test files of 7-4-Time...
Copying 6 training files of 8-8-Time...
Copying 0 validation files of 8-8-Time...
Copying 0 test files of 8-8-Time...
Copying 326 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 336 training files of Accent...
Copying 42 validation files of Accent...
Copying 42 test files of Accent...
Copying 362 training files of Barline...
Copying 45 validation files of Barline...
Copying 45 test files of Barline...
Copying 719 training files of Beam...
Copying 89 validation files of Beam...
Copying 89 test files of Beam...
Copying 408 training files of Beams...
Copying 50 validation files of Beams...
Copying 50 test files of Beams...
Copying 22 training files of Breve...
Copying 2 validation files of Breve...
Copying 2 test files of Breve...
Copying 593 training files of C-Clef...
Copying 74 validation files of C-Clef...
Copying 74 test files of C-Clef...
Copying 96 training files of Chord...
Copying 11 validation files of Chord...
Copying 11 test files of Chord...
Copying 526 training files of Common-Time...
Copying 65 validation files of Common-Time...
Copying 65 test files of Common-Time...
Copying 507 training files of Cut-Time...
Copying 63 validation files of Cut-Time...
Copying 63 test files of Cut-Time...
Copying 674 training files of Dot...
Copying 84 validation files of Dot...
Copying 84 test files of Dot...
Copying 1 training files of Double-Flat...
Copying 0 validation files of Double-Flat...
Copying 0 test files of Double-Flat...
Copying 321 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 86 training files of Double-Whole-Rest...
Copying 10 validation files of Double-Whole-Rest...
Copying 10 test files of Double-Whole-Rest...
Copying 12 training files of Eighth-Grace-Note...
Copying 1 validation files of Eighth-Grace-Note...
Copying 1 test files of Eighth-Grace-Note...
Copying 1019 training files of Eighth-Note...
Copying 127 validation files of Eighth-Note...
Copying 127 test files of Eighth-Note...
Copying 886 training files of Eighth-Rest...
Copying 110 validation files of Eighth-Rest...
Copying 110 test files of Eighth-Rest...
Copying 561 training files of F-Clef...
Copying 70 validation files of F-Clef...
Copying 70 test files of F-Clef...
Copying 82 training files of Fermata...
Copying 10 validation files of Fermata...
Copying 10 test files of Fermata...
Copying 1098 training files of Flat...
Copying 137 validation files of Flat...
Copying 137 test files of Flat...
Copying 969 training files of G-Clef...
Copying 121 validation files of G-Clef...
Copying 121 test files of G-Clef...
Copying 77 training files of Glissando...
Copying 9 validation files of Glissando...
Copying 9 test files of Glissando...
Copying 1182 training files of Half-Note...
Copying 147 validation files of Half-Note...
Copying 147 test files of Half-Note...
Copying 64 training files of Marcato...
Copying 7 validation files of Marcato...
Copying 7 test files of Marcato...
Copying 70 training files of Mordent...
Copying 8 validation files of Mordent...
Copying 8 test files of Mordent...
Copying 78 training files of Multiple-Eighth-Notes...
Copying 9 validation files of Multiple-Eighth-Notes...
Copying 9 test files of Multiple-Eighth-Notes...
Copying 21 training files of Multiple-Half-Notes...
Copying 2 validation files of Multiple-Half-Notes...
Copying 2 test files of Multiple-Half-Notes...
Copying 195 training files of Multiple-Quarter-Notes...
Copying 24 validation files of Multiple-Quarter-Notes...
Copying 24 test files of Multiple-Quarter-Notes...
Copying 69 training files of Multiple-Sixteenth-Notes...
Copying 8 validation files of Multiple-Sixteenth-Notes...
Copying 8 test files of Multiple-Sixteenth-Notes...
Copying 1198 training files of Natural...
Copying 149 validation files of Natural...
Copying 149 test files of Natural...
Copying 642 training files of Other...
Copying 80 validation files of Other...
Copying 80 test files of Other...
Copying 1374 training files of Quarter-Note...
Copying 171 validation files of Quarter-Note...
Copying 171 test files of Quarter-Note...
Copying 792 training files of Quarter-Rest...
Copying 99 validation files of Quarter-Rest...
Copying 99 test files of Quarter-Rest...
Copying 929 training files of Sharp...
Copying 116 validation files of Sharp...
Copying 116 test files of Sharp...
Copying 354 training files of Sharps...
Copying 44 validation files of Sharps...
Copying 44 test files of Sharps...
Copying 687 training files of Sixteenth-Note...
Copying 85 validation files of Sixteenth-Note...
Copying 85 test files of Sixteenth-Note...
Copying 581 training files of Sixteenth-Rest...
Copying 72 validation files of Sixteenth-Rest...
Copying 72 test files of Sixteenth-Rest...
Copying 706 training files of Sixty-Four-Note...
Copying 88 validation files of Sixty-Four-Note...
Copying 88 test files of Sixty-Four-Note...
Copying 432 training files of Sixty-Four-Rest...
Copying 53 validation files of Sixty-Four-Rest...
Copying 53 test files of Sixty-Four-Rest...
Copying 94 training files of Staccatissimo...
Copying 11 validation files of Staccatissimo...
Copying 11 test files of Staccatissimo...
Copying 69 training files of Stopped...
Copying 8 validation files of Stopped...
Copying 8 test files of Stopped...
Copying 74 training files of Tenuto...
Copying 9 validation files of Tenuto...
Copying 9 test files of Tenuto...
Copying 761 training files of Thirty-Two-Note...
Copying 94 validation files of Thirty-Two-Note...
Copying 94 test files of Thirty-Two-Note...
Copying 504 training files of Thirty-Two-Rest...
Copying 62 validation files of Thirty-Two-Rest...
Copying 62 test files of Thirty-Two-Rest...
Copying 374 training files of Tie-Slur...
Copying 46 validation files of Tie-Slur...
Copying 46 test files of Tie-Slur...
Copying 59 training files of Tuplet...
Copying 7 validation files of Tuplet...
Copying 7 test files of Tuplet...
Copying 65 training files of Turn...
Copying 8 validation files of Turn...
Copying 8 test files of Turn...
Copying 420 training files of Whole-Half-Rest...
Copying 52 validation files of Whole-Half-Rest...
Copying 52 test files of Whole-Half-Rest...
Copying 543 training files of Whole-Note...
Copying 67 validation files of Whole-Note...
Copying 67 test files of Whole-Note...
Loading configuration and data-readers...
Found 24479 images belonging to 65 classes.
Found 3026 images belonging to 65 classes.
Found 3026 images belonging to 65 classes.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 96, 96, 3)     0
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 48, 48, 64)    9472        input_1[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 48, 48, 64)    256         conv2d_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 48, 48, 64)    0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 48, 48, 32)    18464       activation_1[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 48, 48, 32)    128         conv2d_2[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 48, 48, 32)    0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 48, 48, 32)    9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 48, 48, 32)    128         conv2d_3[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 48, 48, 32)    0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 48, 48, 32)    9248        activation_3[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 48, 48, 32)    128         conv2d_4[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 48, 48, 32)    0           batch_normalization_4[0][0]
                                                                   activation_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 48, 48, 32)    0           add_1[0][0]
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 24, 24, 32)    0           activation_4[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 24, 24, 64)    18496       max_pooling2d_1[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 24, 24, 64)    256         conv2d_5[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 24, 24, 64)    0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 24, 24, 64)    36928       activation_5[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 24, 24, 64)    256         conv2d_6[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 24, 24, 64)    18496       max_pooling2d_1[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 24, 24, 64)    0           batch_normalization_6[0][0]
                                                                   conv2d_7[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 24, 24, 64)    0           add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 24, 24, 64)    36928       activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 24, 24, 64)    256         conv2d_8[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 24, 24, 64)    0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 24, 24, 64)    36928       activation_7[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 24, 24, 64)    256         conv2d_9[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 24, 24, 64)    0           batch_normalization_8[0][0]
                                                                   activation_6[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 24, 24, 64)    0           add_3[0][0]
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 64)    0           activation_8[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 12, 12, 128)   73856       max_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 12, 12, 128)   512         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 12, 12, 128)   0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 12, 12, 128)   147584      activation_9[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 12, 12, 128)   512         conv2d_11[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 12, 12, 128)   73856       max_pooling2d_2[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 12, 12, 128)   0           batch_normalization_10[0][0]
                                                                   conv2d_12[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 12, 12, 128)   0           add_4[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 12, 12, 128)   147584      activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 12, 12, 128)   512         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 12, 12, 128)   0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 12, 12, 128)   147584      activation_11[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 12, 12, 128)   512         conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 12, 12, 128)   0           batch_normalization_12[0][0]
                                                                   activation_10[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 12, 12, 128)   0           add_5[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 12, 12, 128)   147584      activation_12[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 12, 12, 128)   512         conv2d_15[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 12, 12, 128)   0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 12, 12, 128)   147584      activation_13[0][0]
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 12, 12, 128)   512         conv2d_16[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 12, 12, 128)   0           batch_normalization_14[0][0]
                                                                   activation_12[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 12, 12, 128)   0           add_6[0][0]
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 6, 6, 128)     0           activation_14[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 6, 6, 256)     295168      max_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 6, 6, 256)     1024        conv2d_17[0][0]
____________________________________________________________________________________________________
activation_15 (Activation)       (None, 6, 6, 256)     0           batch_normalization_15[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 6, 6, 256)     590080      activation_15[0][0]
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 6, 6, 256)     1024        conv2d_18[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 6, 6, 256)     295168      max_pooling2d_3[0][0]
____________________________________________________________________________________________________
add_7 (Add)                      (None, 6, 6, 256)     0           batch_normalization_16[0][0]
                                                                   conv2d_19[0][0]
____________________________________________________________________________________________________
activation_16 (Activation)       (None, 6, 6, 256)     0           add_7[0][0]
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 6, 6, 256)     590080      activation_16[0][0]
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 6, 6, 256)     1024        conv2d_20[0][0]
____________________________________________________________________________________________________
activation_17 (Activation)       (None, 6, 6, 256)     0           batch_normalization_17[0][0]
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 6, 6, 256)     590080      activation_17[0][0]
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 6, 6, 256)     1024        conv2d_21[0][0]
____________________________________________________________________________________________________
add_8 (Add)                      (None, 6, 6, 256)     0           batch_normalization_18[0][0]
                                                                   activation_16[0][0]
____________________________________________________________________________________________________
activation_18 (Activation)       (None, 6, 6, 256)     0           add_8[0][0]
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 6, 6, 256)     590080      activation_18[0][0]
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 6, 6, 256)     1024        conv2d_22[0][0]
____________________________________________________________________________________________________
activation_19 (Activation)       (None, 6, 6, 256)     0           batch_normalization_19[0][0]
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 6, 6, 256)     590080      activation_19[0][0]
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 6, 6, 256)     1024        conv2d_23[0][0]
____________________________________________________________________________________________________
add_9 (Add)                      (None, 6, 6, 256)     0           batch_normalization_20[0][0]
                                                                   activation_18[0][0]
____________________________________________________________________________________________________
activation_20 (Activation)       (None, 6, 6, 256)     0           add_9[0][0]
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 3, 3, 256)     0           activation_20[0][0]
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 3, 3, 512)     1180160     max_pooling2d_4[0][0]
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 3, 3, 512)     2048        conv2d_24[0][0]
____________________________________________________________________________________________________
activation_21 (Activation)       (None, 3, 3, 512)     0           batch_normalization_21[0][0]
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 3, 3, 512)     2359808     activation_21[0][0]
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 3, 3, 512)     2048        conv2d_25[0][0]
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 3, 3, 512)     1180160     max_pooling2d_4[0][0]
____________________________________________________________________________________________________
add_10 (Add)                     (None, 3, 3, 512)     0           batch_normalization_22[0][0]
                                                                   conv2d_26[0][0]
____________________________________________________________________________________________________
activation_22 (Activation)       (None, 3, 3, 512)     0           add_10[0][0]
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 3, 3, 512)     2359808     activation_22[0][0]
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 3, 3, 512)     2048        conv2d_27[0][0]
____________________________________________________________________________________________________
activation_23 (Activation)       (None, 3, 3, 512)     0           batch_normalization_23[0][0]
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 3, 3, 512)     2359808     activation_23[0][0]
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 3, 3, 512)     2048        conv2d_28[0][0]
____________________________________________________________________________________________________
add_11 (Add)                     (None, 3, 3, 512)     0           batch_normalization_24[0][0]
                                                                   activation_22[0][0]
____________________________________________________________________________________________________
activation_24 (Activation)       (None, 3, 3, 512)     0           add_11[0][0]
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 3, 3, 512)     2359808     activation_24[0][0]
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 3, 3, 512)     2048        conv2d_29[0][0]
____________________________________________________________________________________________________
activation_25 (Activation)       (None, 3, 3, 512)     0           batch_normalization_25[0][0]
____________________________________________________________________________________________________
conv2d_30 (Conv2D)               (None, 3, 3, 512)     2359808     activation_25[0][0]
____________________________________________________________________________________________________
batch_normalization_26 (BatchNor (None, 3, 3, 512)     2048        conv2d_30[0][0]
____________________________________________________________________________________________________
add_12 (Add)                     (None, 3, 3, 512)     0           batch_normalization_26[0][0]
                                                                   activation_24[0][0]
____________________________________________________________________________________________________
activation_26 (Activation)       (None, 3, 3, 512)     0           add_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_26[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]
____________________________________________________________________________________________________
output_class (Dense)             (None, 65)            33345       flatten_1[0][0]
====================================================================================================
Total params: 18,836,449
Trainable params: 18,824,865
Non-trainable params: 11,584
____________________________________________________________________________________________________
Model res_net_4 loaded.
2017-07-23 22:28:54.496626: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but thes
e are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.496769: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but the
se are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.497101: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but the
se are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.498839: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but t
hese are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.499237: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but t
hese are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.499539: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but thes
e are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.499806: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but the
se are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.501714: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but thes
e are available on your machine and could speed up CPU computations.
2017-07-23 22:28:54.815658: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-07-23 22:28:54.815766: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0
2017-07-23 22:28:54.817119: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y
2017-07-23 22:28:54.817423: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT
X 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 32, Early stopping after 20 epochs without improvement
Data-Shape: (96, 96, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'rho': 0.95, 'lr': 1.0, 'epsilon': 1e-08, 'decay': 0.0}
Performing object localization: False
Training on dataset...
Epoch 1/200
  3/765 [..............................] - ETA: 2152s - loss: 8.9260 - acc: 0.07292017-07-23 22:29:19.944597: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_
allocator.cc:247] PoolAllocator: After 2505 get requests, put_count=2259 evicted_count=1000 eviction_rate=0.442674 and unsatisfied allocation rate=0.537325
2017-07-23 22:29:19.944699: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
 46/765 [>.............................] - ETA: 197s - loss: 5.6776 - acc: 0.09582017-07-23 22:29:24.123114: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_a
llocator.cc:247] PoolAllocator: After 3795 get requests, put_count=3893 evicted_count=1000 eviction_rate=0.256871 and unsatisfied allocation rate=0.243742
2017-07-23 22:29:24.123259: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
764/765 [============================>.] - ETA: 0s - loss: 2.1349 - acc: 0.6074Epoch 00000: val_acc improved from -inf to 0.61170, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 87s - loss: 2.1332 - acc: 0.6078 - val_loss: 2.0953 - val_acc: 0.6117
Epoch 2/200
764/765 [============================>.] - ETA: 0s - loss: 1.0284 - acc: 0.8411Epoch 00001: val_acc improved from 0.61170 to 0.89656, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 1.0279 - acc: 0.8413 - val_loss: 0.8408 - val_acc: 0.8966
Epoch 3/200
764/765 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.8915Epoch 00002: val_acc improved from 0.89656 to 0.92069, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.8143 - acc: 0.8916 - val_loss: 0.6907 - val_acc: 0.9207
Epoch 4/200
764/765 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.9178Epoch 00003: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.6786 - acc: 0.9178 - val_loss: 0.7932 - val_acc: 0.8807
Epoch 5/200
764/765 [============================>.] - ETA: 0s - loss: 0.5897 - acc: 0.9300Epoch 00004: val_acc improved from 0.92069 to 0.94613, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.5895 - acc: 0.9301 - val_loss: 0.5240 - val_acc: 0.9461
Epoch 6/200
764/765 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.9424Epoch 00005: val_acc improved from 0.94613 to 0.94712, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 78s - loss: 0.5145 - acc: 0.9424 - val_loss: 0.5048 - val_acc: 0.9471
Epoch 7/200
764/765 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.9504Epoch 00006: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.4548 - acc: 0.9504 - val_loss: 0.5018 - val_acc: 0.9309
Epoch 8/200
764/765 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.9544Epoch 00007: val_acc improved from 0.94712 to 0.95968, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.4137 - acc: 0.9543 - val_loss: 0.4049 - val_acc: 0.9597
Epoch 9/200
764/765 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.9603Epoch 00008: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.3725 - acc: 0.9602 - val_loss: 0.6403 - val_acc: 0.8843
Epoch 10/200
764/765 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.9624Epoch 00009: val_acc improved from 0.95968 to 0.96100, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.3454 - acc: 0.9624 - val_loss: 0.3575 - val_acc: 0.9610
Epoch 11/200
764/765 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.9643Epoch 00010: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.3198 - acc: 0.9643 - val_loss: 0.3660 - val_acc: 0.9587
Epoch 12/200
764/765 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9670Epoch 00011: val_acc improved from 0.96100 to 0.96761, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 76s - loss: 0.3028 - acc: 0.9670 - val_loss: 0.3086 - val_acc: 0.9676
Epoch 13/200
764/765 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9708Epoch 00012: val_acc improved from 0.96761 to 0.97125, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.2785 - acc: 0.9708 - val_loss: 0.2915 - val_acc: 0.9712
Epoch 14/200
764/765 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9728Epoch 00013: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.2622 - acc: 0.9728 - val_loss: 0.2862 - val_acc: 0.9683
Epoch 15/200
764/765 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9747Epoch 00014: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.2465 - acc: 0.9747 - val_loss: 0.3044 - val_acc: 0.9597
Epoch 16/200
764/765 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9726Epoch 00015: val_acc improved from 0.97125 to 0.97191, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.2431 - acc: 0.9727 - val_loss: 0.2577 - val_acc: 0.9719
Epoch 17/200
764/765 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9765Epoch 00016: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.2283 - acc: 0.9765 - val_loss: 0.3472 - val_acc: 0.9544
Epoch 18/200
764/765 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9782Epoch 00017: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.2185 - acc: 0.9781 - val_loss: 0.2581 - val_acc: 0.9679
Epoch 19/200
764/765 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9774Epoch 00018: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.2131 - acc: 0.9774 - val_loss: 0.3329 - val_acc: 0.9521
Epoch 20/200
764/765 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9802Epoch 00019: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.2021 - acc: 0.9802 - val_loss: 0.2689 - val_acc: 0.9660
Epoch 21/200
764/765 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9804Epoch 00020: val_acc improved from 0.97191 to 0.97389, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.1986 - acc: 0.9804 - val_loss: 0.2480 - val_acc: 0.9739
Epoch 22/200
764/765 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9796Epoch 00021: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1933 - acc: 0.9796 - val_loss: 0.3048 - val_acc: 0.9537
Epoch 23/200
764/765 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9790Epoch 00022: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.1900 - acc: 0.9789 - val_loss: 0.2396 - val_acc: 0.9699
Epoch 24/200
764/765 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9806Epoch 00023: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.1830 - acc: 0.9806 - val_loss: 0.2412 - val_acc: 0.9732
Epoch 25/200
764/765 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9818Epoch 00024: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1800 - acc: 0.9818 - val_loss: 0.2670 - val_acc: 0.9617
Epoch 26/200
764/765 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9815Epoch 00025: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1737 - acc: 0.9815 - val_loss: 0.2497 - val_acc: 0.9709
Epoch 27/200
764/765 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9835Epoch 00026: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1662 - acc: 0.9835 - val_loss: 0.2654 - val_acc: 0.9603
Epoch 28/200
764/765 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9828Epoch 00027: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.1656 - acc: 0.9828 - val_loss: 0.2350 - val_acc: 0.9650
Epoch 29/200
764/765 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9840Epoch 00028: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.1614 - acc: 0.9839 - val_loss: 0.2218 - val_acc: 0.9689
Epoch 30/200
764/765 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9844Epoch 00029: val_acc did not improve

Epoch 00029: reducing learning rate to 0.5.
765/765 [==============================] - 77s - loss: 0.1565 - acc: 0.9844 - val_loss: 0.2001 - val_acc: 0.9732
Epoch 31/200
764/765 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9929Epoch 00030: val_acc improved from 0.97389 to 0.97621, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.1283 - acc: 0.9929 - val_loss: 0.2013 - val_acc: 0.9762
Epoch 32/200
764/765 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9937Epoch 00031: val_acc improved from 0.97621 to 0.97885, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.1195 - acc: 0.9937 - val_loss: 0.1981 - val_acc: 0.9788
Epoch 33/200
764/765 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9946Epoch 00032: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.1128 - acc: 0.9946 - val_loss: 0.1917 - val_acc: 0.9759
Epoch 34/200
764/765 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9944Epoch 00033: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1099 - acc: 0.9944 - val_loss: 0.1939 - val_acc: 0.9762
Epoch 35/200
764/765 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9952Epoch 00034: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1052 - acc: 0.9952 - val_loss: 0.1855 - val_acc: 0.9785
Epoch 36/200
764/765 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9948Epoch 00035: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.1034 - acc: 0.9948 - val_loss: 0.2052 - val_acc: 0.9729
Epoch 37/200
764/765 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9954Epoch 00036: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0991 - acc: 0.9954 - val_loss: 0.1968 - val_acc: 0.9732
Epoch 38/200
764/765 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9942Epoch 00037: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0997 - acc: 0.9942 - val_loss: 0.1945 - val_acc: 0.9759
Epoch 39/200
764/765 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9950Epoch 00038: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0958 - acc: 0.9950 - val_loss: 0.1961 - val_acc: 0.9739
Epoch 40/200
764/765 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9942Epoch 00039: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0958 - acc: 0.9942 - val_loss: 0.1702 - val_acc: 0.9752
Epoch 41/200
764/765 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9936Epoch 00040: val_acc did not improve

Epoch 00040: reducing learning rate to 0.25.
765/765 [==============================] - 77s - loss: 0.0956 - acc: 0.9935 - val_loss: 0.1798 - val_acc: 0.9772
Epoch 42/200
764/765 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9974Epoch 00041: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0829 - acc: 0.9974 - val_loss: 0.1829 - val_acc: 0.9772
Epoch 43/200
764/765 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9973Epoch 00042: val_acc improved from 0.97885 to 0.98182, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0808 - acc: 0.9973 - val_loss: 0.1524 - val_acc: 0.9818
Epoch 44/200
764/765 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9981Epoch 00043: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0773 - acc: 0.9981 - val_loss: 0.1661 - val_acc: 0.9782
Epoch 45/200
764/765 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9978Epoch 00044: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0769 - acc: 0.9978 - val_loss: 0.1762 - val_acc: 0.9812
Epoch 46/200
764/765 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9976Epoch 00045: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0741 - acc: 0.9976 - val_loss: 0.1479 - val_acc: 0.9805
Epoch 47/200
764/765 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9980Epoch 00046: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0726 - acc: 0.9980 - val_loss: 0.1636 - val_acc: 0.9795
Epoch 48/200
764/765 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9977Epoch 00047: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0721 - acc: 0.9977 - val_loss: 0.1459 - val_acc: 0.9815
Epoch 49/200
764/765 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9985Epoch 00048: val_acc improved from 0.98182 to 0.98381, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0688 - acc: 0.9985 - val_loss: 0.1411 - val_acc: 0.9838
Epoch 50/200
764/765 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9978Epoch 00049: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0698 - acc: 0.9978 - val_loss: 0.1453 - val_acc: 0.9838
Epoch 51/200
764/765 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9979Epoch 00050: val_acc improved from 0.98381 to 0.98414, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0678 - acc: 0.9979 - val_loss: 0.1444 - val_acc: 0.9841
Epoch 52/200
764/765 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9978Epoch 00051: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0668 - acc: 0.9978 - val_loss: 0.1706 - val_acc: 0.9755
Epoch 53/200
764/765 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9978Epoch 00052: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0658 - acc: 0.9978 - val_loss: 0.1481 - val_acc: 0.9831
Epoch 54/200
764/765 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9979Epoch 00053: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0643 - acc: 0.9979 - val_loss: 0.1562 - val_acc: 0.9805
Epoch 55/200
764/765 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9975Epoch 00054: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0654 - acc: 0.9975 - val_loss: 0.1395 - val_acc: 0.9812
Epoch 56/200
764/765 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9980Epoch 00055: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0640 - acc: 0.9980 - val_loss: 0.1616 - val_acc: 0.9792
Epoch 57/200
764/765 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9972Epoch 00056: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0631 - acc: 0.9972 - val_loss: 0.1463 - val_acc: 0.9792
Epoch 58/200
764/765 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9974Epoch 00057: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0627 - acc: 0.9974 - val_loss: 0.1562 - val_acc: 0.9798
Epoch 59/200
764/765 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9974Epoch 00058: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0627 - acc: 0.9974 - val_loss: 0.1601 - val_acc: 0.9792
Epoch 60/200
764/765 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9981Epoch 00059: val_acc did not improve

Epoch 00059: reducing learning rate to 0.125.
765/765 [==============================] - 77s - loss: 0.0603 - acc: 0.9981 - val_loss: 0.1334 - val_acc: 0.9775
Epoch 61/200
764/765 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9985Epoch 00060: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0572 - acc: 0.9985 - val_loss: 0.1557 - val_acc: 0.9822
Epoch 62/200
764/765 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9987Epoch 00061: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0560 - acc: 0.9987 - val_loss: 0.1551 - val_acc: 0.9815
Epoch 63/200
764/765 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9991Epoch 00062: val_acc improved from 0.98414 to 0.98480, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0547 - acc: 0.9991 - val_loss: 0.1271 - val_acc: 0.9848
Epoch 64/200
764/765 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9989Epoch 00063: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0544 - acc: 0.9989 - val_loss: 0.1201 - val_acc: 0.9838
Epoch 65/200
764/765 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9989Epoch 00064: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0535 - acc: 0.9989 - val_loss: 0.1562 - val_acc: 0.9792
Epoch 66/200
764/765 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9989Epoch 00065: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0527 - acc: 0.9989 - val_loss: 0.1405 - val_acc: 0.9815
Epoch 67/200
764/765 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9987Epoch 00066: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0525 - acc: 0.9987 - val_loss: 0.1432 - val_acc: 0.9828
Epoch 68/200
764/765 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9991Epoch 00067: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0511 - acc: 0.9991 - val_loss: 0.1445 - val_acc: 0.9822
Epoch 69/200
764/765 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9987Epoch 00068: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0515 - acc: 0.9987 - val_loss: 0.1375 - val_acc: 0.9818
Epoch 70/200
764/765 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9987Epoch 00069: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0515 - acc: 0.9987 - val_loss: 0.1384 - val_acc: 0.9812
Epoch 71/200
764/765 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9987Epoch 00070: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0502 - acc: 0.9987 - val_loss: 0.1386 - val_acc: 0.9838
Epoch 72/200
764/765 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9991Epoch 00071: val_acc did not improve

Epoch 00071: reducing learning rate to 0.0625.
765/765 [==============================] - 77s - loss: 0.0498 - acc: 0.9991 - val_loss: 0.1330 - val_acc: 0.9782
Epoch 73/200
764/765 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9994Epoch 00072: val_acc improved from 0.98480 to 0.98513, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0479 - acc: 0.9994 - val_loss: 0.1105 - val_acc: 0.9851
Epoch 74/200
764/765 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9993Epoch 00073: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0477 - acc: 0.9993 - val_loss: 0.1385 - val_acc: 0.9815
Epoch 75/200
764/765 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9991Epoch 00074: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0474 - acc: 0.9991 - val_loss: 0.1252 - val_acc: 0.9828
Epoch 76/200
764/765 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9993Epoch 00075: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0468 - acc: 0.9993 - val_loss: 0.1219 - val_acc: 0.9851
Epoch 77/200
764/765 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9992Epoch 00076: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0466 - acc: 0.9992 - val_loss: 0.1277 - val_acc: 0.9835
Epoch 78/200
764/765 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9994Epoch 00077: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0460 - acc: 0.9994 - val_loss: 0.1368 - val_acc: 0.9828
Epoch 79/200
764/765 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9994Epoch 00078: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0457 - acc: 0.9994 - val_loss: 0.1261 - val_acc: 0.9838
Epoch 80/200
764/765 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9994Epoch 00079: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0450 - acc: 0.9994 - val_loss: 0.1227 - val_acc: 0.9825
Epoch 81/200
764/765 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9993Epoch 00080: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0452 - acc: 0.9993 - val_loss: 0.1438 - val_acc: 0.9851
Epoch 82/200
764/765 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9994Epoch 00081: val_acc did not improve

Epoch 00081: reducing learning rate to 0.03125.
765/765 [==============================] - 77s - loss: 0.0446 - acc: 0.9994 - val_loss: 0.1118 - val_acc: 0.9831
Epoch 83/200
764/765 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9995Epoch 00082: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0442 - acc: 0.9995 - val_loss: 0.1227 - val_acc: 0.9831
Epoch 84/200
764/765 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9994Epoch 00083: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0438 - acc: 0.9994 - val_loss: 0.1408 - val_acc: 0.9808
Epoch 85/200
764/765 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9994Epoch 00084: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0435 - acc: 0.9994 - val_loss: 0.1308 - val_acc: 0.9848
Epoch 86/200
764/765 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9996Epoch 00085: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0436 - acc: 0.9996 - val_loss: 0.1237 - val_acc: 0.9838
Epoch 87/200
764/765 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9995Epoch 00086: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0434 - acc: 0.9995 - val_loss: 0.1263 - val_acc: 0.9841
Epoch 88/200
764/765 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9996Epoch 00087: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0425 - acc: 0.9996 - val_loss: 0.1248 - val_acc: 0.9848
Epoch 89/200
764/765 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9994Epoch 00088: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0426 - acc: 0.9994 - val_loss: 0.1308 - val_acc: 0.9848
Epoch 90/200
764/765 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9994Epoch 00089: val_acc did not improve

Epoch 00089: reducing learning rate to 0.015625.
765/765 [==============================] - 76s - loss: 0.0428 - acc: 0.9994 - val_loss: 0.1346 - val_acc: 0.9822
Epoch 91/200
764/765 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9993Epoch 00090: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0426 - acc: 0.9993 - val_loss: 0.1203 - val_acc: 0.9835
Epoch 92/200
764/765 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9998Epoch 00091: val_acc improved from 0.98513 to 0.98645, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0418 - acc: 0.9998 - val_loss: 0.1141 - val_acc: 0.9865
Epoch 93/200
764/765 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9996Epoch 00092: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0419 - acc: 0.9996 - val_loss: 0.1392 - val_acc: 0.9822
Epoch 94/200
764/765 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9994Epoch 00093: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0421 - acc: 0.9994 - val_loss: 0.1300 - val_acc: 0.9835
Epoch 95/200
764/765 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9994Epoch 00094: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0421 - acc: 0.9994 - val_loss: 0.1122 - val_acc: 0.9858
Epoch 96/200
764/765 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9994Epoch 00095: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0418 - acc: 0.9994 - val_loss: 0.1245 - val_acc: 0.9851
Epoch 97/200
764/765 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9995Epoch 00096: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0417 - acc: 0.9995 - val_loss: 0.1148 - val_acc: 0.9835
Epoch 98/200
764/765 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9997Epoch 00097: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0414 - acc: 0.9997 - val_loss: 0.1513 - val_acc: 0.9828
Epoch 99/200
764/765 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9996Epoch 00098: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0414 - acc: 0.9996 - val_loss: 0.1030 - val_acc: 0.9858
Epoch 100/200
764/765 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9996Epoch 00099: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0413 - acc: 0.9996 - val_loss: 0.1275 - val_acc: 0.9835
Epoch 101/200
764/765 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9996Epoch 00100: val_acc did not improve

Epoch 00100: reducing learning rate to 0.0078125.
765/765 [==============================] - 77s - loss: 0.0412 - acc: 0.9996 - val_loss: 0.1262 - val_acc: 0.9845
Epoch 102/200
764/765 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9995Epoch 00101: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0412 - acc: 0.9995 - val_loss: 0.1203 - val_acc: 0.9835
Epoch 103/200
764/765 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9995Epoch 00102: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0415 - acc: 0.9995 - val_loss: 0.1257 - val_acc: 0.9855
Epoch 104/200
764/765 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9996Epoch 00103: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0410 - acc: 0.9996 - val_loss: 0.1255 - val_acc: 0.9835
Epoch 105/200
764/765 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9996Epoch 00104: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0409 - acc: 0.9996 - val_loss: 0.1248 - val_acc: 0.9848
Epoch 106/200
764/765 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9995Epoch 00105: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0411 - acc: 0.9995 - val_loss: 0.1352 - val_acc: 0.9825
Epoch 107/200
764/765 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9996Epoch 00106: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0407 - acc: 0.9996 - val_loss: 0.1200 - val_acc: 0.9835
Epoch 108/200
764/765 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9996Epoch 00107: val_acc improved from 0.98645 to 0.98645, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0407 - acc: 0.9996 - val_loss: 0.1110 - val_acc: 0.9865
Epoch 109/200
764/765 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9996Epoch 00108: val_acc did not improve

Epoch 00108: reducing learning rate to 0.00390625.
765/765 [==============================] - 77s - loss: 0.0407 - acc: 0.9996 - val_loss: 0.1364 - val_acc: 0.9822
Epoch 110/200
764/765 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9997Epoch 00109: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0406 - acc: 0.9997 - val_loss: 0.1299 - val_acc: 0.9828
Epoch 111/200
764/765 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9996Epoch 00110: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0404 - acc: 0.9996 - val_loss: 0.1189 - val_acc: 0.9825
Epoch 112/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9996Epoch 00111: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0405 - acc: 0.9996 - val_loss: 0.1134 - val_acc: 0.9835
Epoch 113/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9996Epoch 00112: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0405 - acc: 0.9996 - val_loss: 0.1253 - val_acc: 0.9841
Epoch 114/200
764/765 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9996Epoch 00113: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0407 - acc: 0.9996 - val_loss: 0.1184 - val_acc: 0.9851
Epoch 115/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9998Epoch 00114: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0403 - acc: 0.9998 - val_loss: 0.1222 - val_acc: 0.9848
Epoch 116/200
764/765 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9997Epoch 00115: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0404 - acc: 0.9997 - val_loss: 0.1327 - val_acc: 0.9822
Epoch 117/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9996Epoch 00116: val_acc did not improve

Epoch 00116: reducing learning rate to 0.001953125.
765/765 [==============================] - 76s - loss: 0.0405 - acc: 0.9996 - val_loss: 0.1238 - val_acc: 0.9861
Epoch 118/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9998Epoch 00117: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0403 - acc: 0.9998 - val_loss: 0.1187 - val_acc: 0.9845
Epoch 119/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9996Epoch 00118: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0405 - acc: 0.9996 - val_loss: 0.1323 - val_acc: 0.9831
Epoch 120/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9996Epoch 00119: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0403 - acc: 0.9996 - val_loss: 0.1175 - val_acc: 0.9858
Epoch 121/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9997Epoch 00120: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0402 - acc: 0.9997 - val_loss: 0.1326 - val_acc: 0.9828
Epoch 122/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00121: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1141 - val_acc: 0.9841
Epoch 123/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9997Epoch 00122: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0403 - acc: 0.9997 - val_loss: 0.1240 - val_acc: 0.9845
Epoch 124/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9996Epoch 00123: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0405 - acc: 0.9996 - val_loss: 0.1180 - val_acc: 0.9845
Epoch 125/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00124: val_acc did not improve

Epoch 00124: reducing learning rate to 0.0009765625.
765/765 [==============================] - 76s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1206 - val_acc: 0.9848
Epoch 126/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9996Epoch 00125: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0403 - acc: 0.9996 - val_loss: 0.1309 - val_acc: 0.9841
Epoch 127/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00126: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1383 - val_acc: 0.9812
Epoch 128/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00127: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1100 - val_acc: 0.9848
Epoch 129/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00128: val_acc improved from 0.98645 to 0.98777, saving model to 2017-07-23_res_net_4.h5
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1153 - val_acc: 0.9878
Epoch 130/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9997Epoch 00129: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0402 - acc: 0.9997 - val_loss: 0.1336 - val_acc: 0.9822
Epoch 131/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9997Epoch 00130: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9997 - val_loss: 0.1204 - val_acc: 0.9841
Epoch 132/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9997Epoch 00131: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9997 - val_loss: 0.1305 - val_acc: 0.9838
Epoch 133/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9997Epoch 00132: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0401 - acc: 0.9997 - val_loss: 0.1280 - val_acc: 0.9845
Epoch 134/200
764/765 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9996Epoch 00133: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0406 - acc: 0.9996 - val_loss: 0.1229 - val_acc: 0.9841
Epoch 135/200
764/765 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9996Epoch 00134: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0403 - acc: 0.9996 - val_loss: 0.1231 - val_acc: 0.9838
Epoch 136/200
764/765 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9996Epoch 00135: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0404 - acc: 0.9996 - val_loss: 0.1230 - val_acc: 0.9828
Epoch 137/200
764/765 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9998Epoch 00136: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0400 - acc: 0.9998 - val_loss: 0.1274 - val_acc: 0.9838
Epoch 138/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9997Epoch 00137: val_acc did not improve

Epoch 00137: reducing learning rate to 0.00048828125.
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9997 - val_loss: 0.1375 - val_acc: 0.9825
Epoch 139/200
764/765 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9998Epoch 00138: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0399 - acc: 0.9998 - val_loss: 0.1055 - val_acc: 0.9861
Epoch 140/200
764/765 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9998Epoch 00139: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0400 - acc: 0.9998 - val_loss: 0.1285 - val_acc: 0.9855
Epoch 141/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00140: val_acc did not improve
765/765 [==============================] - 76s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1163 - val_acc: 0.9851
Epoch 142/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9996Epoch 00141: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9996 - val_loss: 0.1252 - val_acc: 0.9838
Epoch 143/200
764/765 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9997Epoch 00142: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0400 - acc: 0.9997 - val_loss: 0.1238 - val_acc: 0.9845
Epoch 144/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9997Epoch 00143: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9997 - val_loss: 0.1275 - val_acc: 0.9841
Epoch 145/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9997Epoch 00144: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9997 - val_loss: 0.1272 - val_acc: 0.9838
Epoch 146/200
764/765 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9995Epoch 00145: val_acc did not improve

Epoch 00145: reducing learning rate to 0.000244140625.
765/765 [==============================] - 76s - loss: 0.0405 - acc: 0.9995 - val_loss: 0.1223 - val_acc: 0.9855
Epoch 147/200
764/765 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9996Epoch 00146: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0401 - acc: 0.9996 - val_loss: 0.1089 - val_acc: 0.9855
Epoch 148/200
764/765 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9998Epoch 00147: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0399 - acc: 0.9998 - val_loss: 0.1216 - val_acc: 0.9848
Epoch 149/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9997Epoch 00148: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9997 - val_loss: 0.1300 - val_acc: 0.9865
Epoch 150/200
764/765 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9996Epoch 00149: val_acc did not improve
765/765 [==============================] - 77s - loss: 0.0402 - acc: 0.9996 - val_loss: 0.1284 - val_acc: 0.9835
Epoch 00149: early stopping
Loading best model from check-point and testing...
                          precision    recall  f1-score   support

                1-8-Time       1.00      1.00      1.00        40
               12-8-Time       1.00      0.97      0.99        39
                2-2-Time       0.98      1.00      0.99        44
                2-4-Time       1.00      1.00      1.00         1
                2-8-Time       1.00      0.98      0.99        48
                3-4-Time       1.00      1.00      1.00        42
                3-8-Time       1.00      0.98      0.99        42
                4-2-Time       1.00      1.00      1.00         1
                4-4-Time       1.00      1.00      1.00         1
                4-8-Time       1.00      0.98      0.99        42
                5-4-Time       1.00      1.00      1.00        40
                5-8-Time       1.00      0.98      0.99        42
                6-4-Time       0.96      1.00      0.98        45
                6-8-Time       0.97      0.99      0.98        89
                7-4-Time       0.96      0.94      0.95        50
                8-8-Time       1.00      1.00      1.00         2
                9-8-Time       1.00      1.00      1.00        74
                  Accent       0.92      1.00      0.96        11
                 Barline       0.98      0.98      0.98        65
                    Beam       1.00      1.00      1.00        63
                   Beams       0.98      0.99      0.98        84
                   Breve       0.98      1.00      0.99        40
                  C-Clef       1.00      1.00      1.00        10
                   Chord       1.00      1.00      1.00         1
             Common-Time       0.97      0.98      0.98       127
                Cut-Time       1.00      0.97      0.99       110
                     Dot       1.00      1.00      1.00        70
             Double-Flat       1.00      1.00      1.00        10
            Double-Sharp       0.99      1.00      1.00       137
       Double-Whole-Rest       0.99      1.00      1.00       121
       Eighth-Grace-Note       1.00      1.00      1.00         9
             Eighth-Note       0.97      1.00      0.99       147
             Eighth-Rest       1.00      1.00      1.00         7
                  F-Clef       1.00      1.00      1.00         8
                 Fermata       0.90      1.00      0.95         9
                    Flat       1.00      1.00      1.00         2
                  G-Clef       1.00      0.92      0.96        24
               Glissando       1.00      1.00      1.00         8
               Half-Note       0.99      1.00      1.00       149
                 Marcato       0.98      0.81      0.89        80
                 Mordent       0.97      0.97      0.97       171
   Multiple-Eighth-Notes       0.98      0.97      0.97        99
     Multiple-Half-Notes       0.97      1.00      0.99       116
  Multiple-Quarter-Notes       0.98      1.00      0.99        44
Multiple-Sixteenth-Notes       0.95      0.95      0.95        85
                 Natural       0.96      1.00      0.98        72
                   Other       0.90      0.90      0.90        88
            Quarter-Note       0.96      1.00      0.98        53
            Quarter-Rest       1.00      1.00      1.00        11
                   Sharp       1.00      1.00      1.00         8
                  Sharps       1.00      1.00      1.00         9
          Sixteenth-Note       0.90      0.87      0.89        94
          Sixteenth-Rest       1.00      0.97      0.98        62
         Sixty-Four-Note       1.00      1.00      1.00        46
         Sixty-Four-Rest       1.00      1.00      1.00         7
           Staccatissimo       1.00      1.00      1.00         8
                 Stopped       0.95      1.00      0.97        52
                  Tenuto       0.99      1.00      0.99        67

             avg / total       0.98      0.98      0.98      3026

Misclassified files:
        2-2-Time\45-44_3.png is incorrectly classified as Quarter-Rest
        3-4-Time\39-30_3.png is incorrectly classified as 2-4-Time
        4-4-Time\78-21_3.png is incorrectly classified as Natural
        6-8-Time\symbol223.png is incorrectly classified as Chord
        Accent\symbol88.png is incorrectly classified as Dot
        Beam\symbol112466.png is incorrectly classified as Beams
        Beams\symbol2248.png is incorrectly classified as Beam
        Beams\symbol2542.png is incorrectly classified as Beam
        Beams\symbol2920.png is incorrectly classified as Beam
        Common-Time\25-44_3.png is incorrectly classified as Other
        Dot\77-148_3.png is incorrectly classified as Whole-Half-Rest
        Eighth-Note\21-62_3.png is incorrectly classified as Sixteenth-Note
        Eighth-Note\symbol567.png is incorrectly classified as Multiple-Eighth-Notes
        Eighth-Rest\54-108_3.png is incorrectly classified as Double-Sharp
        Eighth-Rest\symbol2737.png is incorrectly classified as Sixteenth-Rest
        Eighth-Rest\symbol8030.png is incorrectly classified as Sixteenth-Rest
        Multiple-Quarter-Notes\symbol4681.png is incorrectly classified as Quarter-Note
        Multiple-Quarter-Notes\symbol5192.png is incorrectly classified as Quarter-Note
        Other\symbol101162.png is incorrectly classified as Whole-Half-Rest
        Other\symbol1641.png is incorrectly classified as G-Clef
        Other\symbol2109.png is incorrectly classified as Sharp
        Other\symbol3945.png is incorrectly classified as Flat
        Other\symbol438.png is incorrectly classified as Whole-Half-Rest
        Other\symbol443.png is incorrectly classified as Barline
        Other\symbol477.png is incorrectly classified as Quarter-Rest
        Other\symbol5837.png is incorrectly classified as Whole-Note
        Other\symbol6115.png is incorrectly classified as Sharps
        Other\symbol6400.png is incorrectly classified as Beams
        Other\symbol7143.png is incorrectly classified as Quarter-Note
        Other\symbol7147.png is incorrectly classified as Eighth-Note
        Other\symbol7171.png is incorrectly classified as Common-Time
        Other\symbol7198.png is incorrectly classified as Dot
        Other\symbol937.png is incorrectly classified as Quarter-Note
        Quarter-Note\49-85_3.png is incorrectly classified as Half-Note
        Quarter-Note\54-87_3.png is incorrectly classified as Half-Note
        Quarter-Note\98-91_3.png is incorrectly classified as Half-Note
        Quarter-Note\symbol1897.png is incorrectly classified as Eighth-Note
        Quarter-Note\symbol2660.png is incorrectly classified as Half-Note
        Quarter-Rest\2-101_3.png is incorrectly classified as Quarter-Note
        Quarter-Rest\55-96_3.png is incorrectly classified as Sharp
        Quarter-Rest\81-94_3.png is incorrectly classified as Barline
        Sixteenth-Note\39-111_3.png is incorrectly classified as Thirty-Two-Note
        Sixteenth-Note\81-109_3.png is incorrectly classified as Thirty-Two-Note
        Sixteenth-Note\81-112_3.png is incorrectly classified as Eighth-Note
        Sixteenth-Note\88-113_3.png is incorrectly classified as Eighth-Note
        Sixty-Four-Note\34-132_3.png is incorrectly classified as Sixteenth-Note
        Sixty-Four-Note\39-134_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\5.png is incorrectly classified as Sixty-Four-Rest
        Sixty-Four-Note\75-139_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\86-133_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\86-136_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\98-135_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\98-137_3.png is incorrectly classified as Thirty-Two-Note
        Sixty-Four-Note\symbols50.png is incorrectly classified as Thirty-Two-Note
        Thirty-Two-Note\16-136_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\25-136_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\36-124_3.png is incorrectly classified as Sixteenth-Note
        Thirty-Two-Note\36-125_3.png is incorrectly classified as Sixteenth-Note
        Thirty-Two-Note\39-127_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\39-128_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\45-122_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\45-126_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\61-124_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\62-123_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\85-125_3.png is incorrectly classified as Sixty-Four-Note
        Thirty-Two-Note\98-128_3.png is incorrectly classified as Sharp
        Thirty-Two-Rest\23-144_3.png is incorrectly classified as Sixty-Four-Rest
        Thirty-Two-Rest\9-142_3.png is incorrectly classified as Sixteenth-Rest
loss: 0.16565
acc: 0.97753
Total Accuracy: 97.75281%
Total Error: 2.24719%
Execution time: 11602.6s
Uploading results to Google Spreadsheet and appending at first empty line 188
**********************
Windows PowerShell transcript end
End time: 20170724014223
**********************
